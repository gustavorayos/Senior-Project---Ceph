[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3e43f80290>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f3e443e4e60>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  keyrings                      : None
[ceph_deploy][ERROR ] ConfigError: Cannot load config: [Errno 2] No such file or directory: 'ceph.conf'; has `ceph-deploy new` been run in this directory?

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy new ceph1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fa641e83230>
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa641a20290>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[ceph_deploy.cli][INFO  ]  mon                           : ['ceph1']
[ceph_deploy.cli][INFO  ]  public_network                : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  cluster_network               : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  fsid                          : None
[ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] find the location of an executable
[ceph1][INFO  ] Running command: /bin/ip link show
[ceph1][INFO  ] Running command: /bin/ip addr show
[ceph1][DEBUG ] IP addresses found: ['10.0.69.35', '10.0.0.1']
[ceph_deploy.new][DEBUG ] Resolving host ceph1
[ceph_deploy.new][DEBUG ] Monitor ceph1 at 10.0.69.35
[ceph_deploy.new][DEBUG ] Monitor initial members are ['ceph1']
[ceph_deploy.new][DEBUG ] Monitor addrs are ['10.0.69.35']
[ceph_deploy.new][DEBUG ] Creating a random mon key...
[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf6e64d290>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fcf6eab1e60>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  keyrings                      : None
[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph1
[ceph_deploy.mon][DEBUG ] detecting platform for host ceph1 ...
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] find the location of an executable
[ceph_deploy.mon][INFO  ] distro info: Ubuntu 14.04 trusty
[ceph1][DEBUG ] determining if provided host has same hostname in remote
[ceph1][DEBUG ] get remote short hostname
[ceph1][DEBUG ] deploying mon to ceph1
[ceph1][DEBUG ] get remote short hostname
[ceph1][DEBUG ] remote hostname: ceph1
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9173e3d290>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f91742a1e60>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  keyrings                      : None
[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph1
[ceph_deploy.mon][DEBUG ] detecting platform for host ceph1 ...
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] find the location of an executable
[ceph_deploy.mon][INFO  ] distro info: Ubuntu 14.04 trusty
[ceph1][DEBUG ] determining if provided host has same hostname in remote
[ceph1][DEBUG ] get remote short hostname
[ceph1][DEBUG ] deploying mon to ceph1
[ceph1][DEBUG ] get remote short hostname
[ceph1][DEBUG ] remote hostname: ceph1
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph1][DEBUG ] create the mon path if it does not exist
[ceph1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph1/done
[ceph1][DEBUG ] create a done file to avoid re-doing the mon deployment
[ceph1][DEBUG ] create the init path if it does not exist
[ceph1][DEBUG ] locating the `service` executable...
[ceph1][INFO  ] Running command: initctl emit ceph-mon cluster=ceph id=ceph1
[ceph1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph1.asok mon_status
[ceph1][DEBUG ] ********************************************************************************
[ceph1][DEBUG ] status for monitor: mon.ceph1
[ceph1][DEBUG ] {
[ceph1][DEBUG ]   "election_epoch": 1, 
[ceph1][DEBUG ]   "extra_probe_peers": [], 
[ceph1][DEBUG ]   "monmap": {
[ceph1][DEBUG ]     "created": "2015-07-30 12:51:41.426744", 
[ceph1][DEBUG ]     "epoch": 1, 
[ceph1][DEBUG ]     "fsid": "0cb16b38-9df3-4660-9fcb-57a264af6a0b", 
[ceph1][DEBUG ]     "modified": "2015-07-30 12:51:41.426744", 
[ceph1][DEBUG ]     "mons": [
[ceph1][DEBUG ]       {
[ceph1][DEBUG ]         "addr": "10.0.69.35:6789/0", 
[ceph1][DEBUG ]         "name": "ceph1", 
[ceph1][DEBUG ]         "rank": 0
[ceph1][DEBUG ]       }
[ceph1][DEBUG ]     ]
[ceph1][DEBUG ]   }, 
[ceph1][DEBUG ]   "name": "ceph1", 
[ceph1][DEBUG ]   "outside_quorum": [], 
[ceph1][DEBUG ]   "quorum": [
[ceph1][DEBUG ]     0
[ceph1][DEBUG ]   ], 
[ceph1][DEBUG ]   "rank": 0, 
[ceph1][DEBUG ]   "state": "leader", 
[ceph1][DEBUG ]   "sync_provider": []
[ceph1][DEBUG ] }
[ceph1][DEBUG ] ********************************************************************************
[ceph1][INFO  ] monitor: mon.ceph1 is running
[ceph1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph1.asok mon_status
[ceph_deploy.mon][INFO  ] processing monitor mon.ceph1
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] find the location of an executable
[ceph1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph1.asok mon_status
[ceph_deploy.mon][INFO  ] mon.ceph1 monitor has reached quorum!
[ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[ceph_deploy.mon][INFO  ] Running gatherkeys...
[ceph_deploy.gatherkeys][DEBUG ] Checking ceph1 for /etc/ceph/ceph.client.admin.keyring
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.client.admin.keyring key from ceph1.
[ceph_deploy.gatherkeys][DEBUG ] Have ceph.mon.keyring
[ceph_deploy.gatherkeys][DEBUG ] Checking ceph1 for /var/lib/ceph/bootstrap-osd/ceph.keyring
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.bootstrap-osd.keyring key from ceph1.
[ceph_deploy.gatherkeys][DEBUG ] Checking ceph1 for /var/lib/ceph/bootstrap-mds/ceph.keyring
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.bootstrap-mds.keyring key from ceph1.
[ceph_deploy.gatherkeys][DEBUG ] Checking ceph1 for /var/lib/ceph/bootstrap-rgw/ceph.keyring
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.bootstrap-rgw.keyring key from ceph1.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk list ceph2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1942b6d710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f1942fd77d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', None, None)]
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Listing disks on ceph2...
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk list
[ceph2][DEBUG ] /dev/sda :
[ceph2][DEBUG ]  /dev/sda1 ceph data, prepared, cluster ceph, osd.0, journal /dev/sda2
[ceph2][DEBUG ]  /dev/sda2 ceph journal, for /dev/sda1
[ceph2][DEBUG ] /dev/sdb :
[ceph2][DEBUG ]  /dev/sdb1 ceph data, prepared, cluster ceph, osd.1, journal /dev/sdb2
[ceph2][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[ceph2][DEBUG ] /dev/sdc :
[ceph2][DEBUG ]  /dev/sdc1 other, ext4, mounted on /
[ceph2][DEBUG ]  /dev/sdc2 other, 0x5
[ceph2][DEBUG ]  /dev/sdc5 swap, swap
[ceph2][DEBUG ] /dev/sdd :
[ceph2][DEBUG ]  /dev/sdd1 ceph data, prepared, cluster ceph, osd.2, journal /dev/sdd2
[ceph2][DEBUG ]  /dev/sdd2 ceph journal, for /dev/sdd1
[ceph2][DEBUG ] /dev/sde :
[ceph2][DEBUG ]  /dev/sde1 ceph data, prepared, cluster ceph, osd.3, journal /dev/sde2
[ceph2][DEBUG ]  /dev/sde2 ceph journal, for /dev/sde1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph17:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe9f51da710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe9f56447d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph17][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph17:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f602a9de710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f602ae487d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph17][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph17:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f303e4fa710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f303e9647d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph17][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph17:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4add01b710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4add4857d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph17][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph16:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4f5f017710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4f5f4817d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph16][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph16:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa695d8e710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fa6961f87d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph16][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph16:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fae4a97f710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fae4ade97d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph16][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph16:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1446ea0710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f144730a7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph16][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph15:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9b790b8710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f9b795227d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph15][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph15:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb2b6635710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fb2b6a9f7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph15][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph15:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7eff00709710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7eff00b737d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph15][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph15:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6fdcf03710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6fdd36d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph15][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph12:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbe051c8710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fbe056327d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph12][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph12:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0c6b2d9710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0c6b7437d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph12][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph12:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9c590d9710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f9c595437d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph12][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph12:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f87f5312710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f87f577c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph12][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph11:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f837e472710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f837e8dc7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph11][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph11:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7708956710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f7708dc07d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph11][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph11:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3fdb844710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f3fdbcae7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph11][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph11:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f52bc133710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f52bc59d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph11][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph10:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f745f811710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f745fc7b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph10][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph10:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe893a46710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe893eb07d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph10][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph10:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f112b216710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f112b6807d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph10][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph10:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b76801710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f3b76c6b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph10][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph9:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f81c8824710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f81c8c8e7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph9:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4ab1e9f710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4ab23097d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph9:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0845dfe710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f08462687d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph9:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f342a3cf710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f342a8397d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph8:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8221265710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f82216cf7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph8][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph8:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8c2bfe2710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f8c2c44c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph8][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph8:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f02d50f2710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f02d555c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph8][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph8:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7bb5c03710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f7bb606d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph8][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph6:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ea0b4f710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0ea0fb97d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph6][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph6:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4f06732710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4f06b9c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph6][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph6:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0b0d848710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0b0dcb27d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph6][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph6:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9460d46710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f94611b07d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph6][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph5:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9e6cfb3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f9e6d41d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph5][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph5:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9166b4e710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f9166fb87d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph5][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph5:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f40d8a82710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f40d8eec7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph5][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph5:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f66db552710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f66db9bc7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph5][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph4:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6f98728710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6f98b927d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph4
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph4 resulted in errors: HostNotFound ceph4

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph4:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f89b139b710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f89b18057d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph4
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph4 resulted in errors: HostNotFound ceph4

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph4:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff3a8efe710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7ff3a93687d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph4
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph4 resulted in errors: HostNotFound ceph4

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph4:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ffe6a7710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f7ffeb117d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph4
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph4 resulted in errors: HostNotFound ceph4

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph3:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f09fec34710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f09ff09e7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph3', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph3
[ceph3][DEBUG ] connected to host: ceph3 
[ceph3][DEBUG ] detect platform information from remote host
[ceph3][DEBUG ] detect machine type
[ceph3][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph3][DEBUG ] zeroing last few blocks of device
[ceph3][DEBUG ] find the location of an executable
[ceph3][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph3][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph3][WARNING] backup header from main header.
[ceph3][WARNING] 
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph3][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph3][DEBUG ] other utilities.
[ceph3][DEBUG ] Creating new GPT entries.
[ceph3][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph3][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph3:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe849bc3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe84a02d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph3', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph3
[ceph3][DEBUG ] connected to host: ceph3 
[ceph3][DEBUG ] detect platform information from remote host
[ceph3][DEBUG ] detect machine type
[ceph3][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph3][DEBUG ] zeroing last few blocks of device
[ceph3][DEBUG ] find the location of an executable
[ceph3][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph3][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph3][WARNING] backup header from main header.
[ceph3][WARNING] 
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph3][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph3][DEBUG ] other utilities.
[ceph3][DEBUG ] Creating new GPT entries.
[ceph3][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph3][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph3:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4481927710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4481d917d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph3', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph3
[ceph3][DEBUG ] connected to host: ceph3 
[ceph3][DEBUG ] detect platform information from remote host
[ceph3][DEBUG ] detect machine type
[ceph3][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph3][DEBUG ] zeroing last few blocks of device
[ceph3][DEBUG ] find the location of an executable
[ceph3][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph3][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph3][WARNING] backup header from main header.
[ceph3][WARNING] 
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph3][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph3][DEBUG ] other utilities.
[ceph3][DEBUG ] Creating new GPT entries.
[ceph3][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph3][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph3:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f702b284710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f702b6ee7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph3', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph3
[ceph3][DEBUG ] connected to host: ceph3 
[ceph3][DEBUG ] detect platform information from remote host
[ceph3][DEBUG ] detect machine type
[ceph3][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph3][DEBUG ] zeroing last few blocks of device
[ceph3][DEBUG ] find the location of an executable
[ceph3][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph3][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph3][WARNING] backup header from main header.
[ceph3][WARNING] 
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph3][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph3][DEBUG ] ****************************************************************************
[ceph3][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph3][DEBUG ] The new table will be used at the next reboot.
[ceph3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph3][DEBUG ] other utilities.
[ceph3][DEBUG ] Creating new GPT entries.
[ceph3][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph3][DEBUG ] The new table will be used at the next reboot.
[ceph3][DEBUG ] The operation has completed successfully.
[ceph3][WARNING] Error: Partition(s) 1 on /dev/sda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph3][INFO  ] Running command: partprobe /dev/sda
[ceph3][WARNING] Error: Partition(s) 1 on /dev/sda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph3][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sda

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph2:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fac32342710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fac327ac7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph2][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph2:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6aeeecc710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6aef3367d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph2][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph2:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9072afe710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f9072f687d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph2][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph2:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1ef9840710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f1ef9caa7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph2][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy disk zap ceph2:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7e02140710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f7e025aa7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph2][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph2:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf4e4eb128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcf4e949758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sda:/dev/sda
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph2:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcefa06f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcefa4cd758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdb:/dev/sdb
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph2:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f07575d9128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0757a37758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdd:/dev/sdd
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph2:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fecd3cd2128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fecd4130758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sde:/dev/sde
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph5:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f309b8f3128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f309bd51758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sda:/dev/sda
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph5:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa907504128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa907962758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdb:/dev/sdb
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph5:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ed7d55128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0ed81b3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdd:/dev/sdd
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph5:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7820876128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7820cd4758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sde:/dev/sde
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph6:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f60ac904128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f60acd62758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sda:/dev/sda
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph6:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb4ea188128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb4ea5e6758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdb:/dev/sdb
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph6:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7c40167128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7c405c5758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdd:/dev/sdd
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph6:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7a8585b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7a85cb9758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sde:/dev/sde
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph8:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc6049d3128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc604e31758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sda:/dev/sda
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph8:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fac9231b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fac92779758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdc:/dev/sdc
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph8:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbf6bf11128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fbf6c36f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdd:/dev/sdd
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph8:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd8fb745128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd8fbba3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sde:/dev/sde
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph10:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7f09e78128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7f0a2d6758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sda:/dev/sda
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph10:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e65470128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8e658ce758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdb:/dev/sdb
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph10:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f50c761a128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f50c7a78758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdd:/dev/sdd
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph10:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8a3aa4b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8a3aea9758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sde:/dev/sde
[ceph_deploy][ERROR ] KeyboardInterrupt

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph11:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f95bdf8a128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f95be3e8758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sda:/dev/sda
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph11:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7dec0d6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7dec534758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdb:/dev/sdb
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph11:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdc874dd128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdc8793b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdd:/dev/sdd
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph11:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f944b784128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f944bbe2758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sde:/dev/sde
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph12:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2f70aff128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2f70f5d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sda:/dev/sda
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8b0ce3f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8b0d29d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph12:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f231f965128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f231fdc3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdd:/dev/sdd
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph12:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f285847b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f28588d9758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sde:/dev/sde
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f76b61df128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f76b663d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph15:sdc:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdc', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f986ca63128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f986cec1758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdc:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph15:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f81462dd128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f814673b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdd:/dev/sdd
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph15:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f77aef1c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f77af37a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sde:/dev/sde
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph16:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc896465128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc8968c3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sda:/dev/sda
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph16:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3431cb7128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3432115758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdb:/dev/sdb
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph16:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa212f30128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa21338e758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdd:/dev/sdd
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph16:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f77e198c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f77e1dea758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sde:/dev/sde
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph17:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7b79744128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7b79ba2758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sda:/dev/sda
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph17:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f926a057128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f926a4b5758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdb:/dev/sdb
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph17:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1fe4f7a128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1fe53d8758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdd:/dev/sdd
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy osd create ceph17:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1f36a40128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1f36e9e758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sde:/dev/sde
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb2e8e28128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb2e9286758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sda:/dev/sda
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sda journal /dev/sda activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:9b4fb36e-86a8-448b-9868-7a7676ac3052 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9b4fb36e-86a8-448b-9868-7a7676ac3052
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9b4fb36e-86a8-448b-9868-7a7676ac3052
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:b56cc0ac-0513-4568-8b07-473e7351287e --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph2][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.ZM3ZU8 with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.ZM3ZU8
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.ZM3ZU8
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.ZM3ZU8/journal -> /dev/disk/by-partuuid/9b4fb36e-86a8-448b-9868-7a7676ac3052
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.ZM3ZU8
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.ZM3ZU8
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f546c3e6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f546c844758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdb:/dev/sdb
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdb journal /dev/sdb activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:9cd9b42f-27a4-48ca-b8dc-a52d9e01ab86 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9cd9b42f-27a4-48ca-b8dc-a52d9e01ab86
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9cd9b42f-27a4-48ca-b8dc-a52d9e01ab86
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ca6cd8de-0a3d-44e8-b105-d6ffe646848a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph2][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.v5cBM3 with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.v5cBM3
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.v5cBM3
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.v5cBM3/journal -> /dev/disk/by-partuuid/9cd9b42f-27a4-48ca-b8dc-a52d9e01ab86
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.v5cBM3
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.v5cBM3
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5d7886f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5d78ccd758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdd:/dev/sdd
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdd journal /dev/sdd activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:9eed98be-9833-4532-8f12-404b6f89b989 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9eed98be-9833-4532-8f12-404b6f89b989
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9eed98be-9833-4532-8f12-404b6f89b989
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:119164ef-1397-4ed6-a09d-a30d95c5fb3c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph2][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.nj6d1A with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.nj6d1A
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.nj6d1A
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.nj6d1A/journal -> /dev/disk/by-partuuid/9eed98be-9833-4532-8f12-404b6f89b989
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.nj6d1A
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.nj6d1A
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fabc5af8128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fabc5f56758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sde:/dev/sde
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sde journal /dev/sde activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:7dafb41b-addc-4a90-9d10-6bbb067a39e1 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/7dafb41b-addc-4a90-9d10-6bbb067a39e1
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/7dafb41b-addc-4a90-9d10-6bbb067a39e1
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:806d5672-d754-480d-9a1f-106c89934f70 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph2][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.gRytDP with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.gRytDP
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.gRytDP
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.gRytDP/journal -> /dev/disk/by-partuuid/7dafb41b-addc-4a90-9d10-6bbb067a39e1
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.gRytDP
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.gRytDP
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f69ffb41128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f69fff9f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sda:/dev/sda
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sda journal /dev/sda activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:24974805-2c03-4c6b-89ab-aed815a92189 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/24974805-2c03-4c6b-89ab-aed815a92189
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/24974805-2c03-4c6b-89ab-aed815a92189
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e60a8bde-70f8-4b52-b85a-2893e4ba0d35 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph5][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.cw1xHP with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.cw1xHP
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.cw1xHP
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.cw1xHP/journal -> /dev/disk/by-partuuid/24974805-2c03-4c6b-89ab-aed815a92189
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.cw1xHP
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.cw1xHP
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2ee4d63128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2ee51c1758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdb:/dev/sdb
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdb journal /dev/sdb activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:c128516c-2860-497e-aeae-c9603d1da457 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/c128516c-2860-497e-aeae-c9603d1da457
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/c128516c-2860-497e-aeae-c9603d1da457
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f0b686f9-b2a3-4673-8583-a800391f0056 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph5][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.Tv_BuL with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.Tv_BuL
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Tv_BuL
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Tv_BuL/journal -> /dev/disk/by-partuuid/c128516c-2860-497e-aeae-c9603d1da457
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Tv_BuL
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Tv_BuL
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f95ebdc6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f95ec224758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdd:/dev/sdd
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdd journal /dev/sdd activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:06176763-3418-46ca-8773-393fef9d3b11 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/06176763-3418-46ca-8773-393fef9d3b11
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/06176763-3418-46ca-8773-393fef9d3b11
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a9cb00a3-efae-484c-9ce5-d7384127fb99 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph5][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.GGa0a0 with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.GGa0a0
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.GGa0a0
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.GGa0a0/journal -> /dev/disk/by-partuuid/06176763-3418-46ca-8773-393fef9d3b11
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.GGa0a0
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.GGa0a0
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb37e329128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb37e787758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sde:/dev/sde
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sde journal /dev/sde activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ba853b34-4565-49b9-a249-f0770a0720c5 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ba853b34-4565-49b9-a249-f0770a0720c5
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ba853b34-4565-49b9-a249-f0770a0720c5
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:1cd637e2-665e-4f58-8aa8-99bf6790ffee --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph5][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.Kp6My_ with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.Kp6My_
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Kp6My_
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Kp6My_/journal -> /dev/disk/by-partuuid/ba853b34-4565-49b9-a249-f0770a0720c5
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Kp6My_
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Kp6My_
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3db4bf1128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3db504f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sda:/dev/sda
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sda journal /dev/sda activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:0b9d58a7-e33d-4d49-b768-0af0d7ffd172 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/0b9d58a7-e33d-4d49-b768-0af0d7ffd172
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/0b9d58a7-e33d-4d49-b768-0af0d7ffd172
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:51980173-0410-4d34-a578-0c41fc7fd020 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph6][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.ltFwU7 with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.ltFwU7
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.ltFwU7
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.ltFwU7/journal -> /dev/disk/by-partuuid/0b9d58a7-e33d-4d49-b768-0af0d7ffd172
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.ltFwU7
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.ltFwU7
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcdcde20128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcdce27e758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdb:/dev/sdb
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdb journal /dev/sdb activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:951c1535-77cc-4247-9395-f2c500a42426 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/951c1535-77cc-4247-9395-f2c500a42426
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/951c1535-77cc-4247-9395-f2c500a42426
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c130fd5f-8c0f-47fa-98d2-a59ee3d53eb6 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph6][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.RL28aN with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.RL28aN
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.RL28aN
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.RL28aN/journal -> /dev/disk/by-partuuid/951c1535-77cc-4247-9395-f2c500a42426
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.RL28aN
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.RL28aN
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f96e3463128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f96e38c1758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdd:/dev/sdd
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdd journal /dev/sdd activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:3374ce6a-a7bd-4ec5-bcf8-397b975a62ac --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3374ce6a-a7bd-4ec5-bcf8-397b975a62ac
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3374ce6a-a7bd-4ec5-bcf8-397b975a62ac
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ccbfa130-70aa-4cea-9a88-defb21acd5b7 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph6][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.2lyIJy with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.2lyIJy
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.2lyIJy
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.2lyIJy/journal -> /dev/disk/by-partuuid/3374ce6a-a7bd-4ec5-bcf8-397b975a62ac
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.2lyIJy
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.2lyIJy
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1780d85128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f17811e3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sde:/dev/sde
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sde journal /dev/sde activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ae1dbfb2-f80c-43fd-8830-3f8d0100e0bf --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ae1dbfb2-f80c-43fd-8830-3f8d0100e0bf
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ae1dbfb2-f80c-43fd-8830-3f8d0100e0bf
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e1d6e22e-6680-4f76-8bba-b9e30476f017 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph6][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.LZuFRM with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.LZuFRM
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.LZuFRM
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.LZuFRM/journal -> /dev/disk/by-partuuid/ae1dbfb2-f80c-43fd-8830-3f8d0100e0bf
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.LZuFRM
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.LZuFRM
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0325b35128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0325f93758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sda:/dev/sda
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sda journal /dev/sda activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:397aa0ea-aae5-4b92-a96e-b8aea91e92f7 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/397aa0ea-aae5-4b92-a96e-b8aea91e92f7
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/397aa0ea-aae5-4b92-a96e-b8aea91e92f7
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7001d52d-5632-4992-a39a-02f5f15ebfd9 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph8][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.qWuuYK with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.qWuuYK
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.qWuuYK
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.qWuuYK/journal -> /dev/disk/by-partuuid/397aa0ea-aae5-4b92-a96e-b8aea91e92f7
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.qWuuYK
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.qWuuYK
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe3de0fb128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe3de559758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdc:/dev/sdc
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdc journal /dev/sdc activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:e81a75d3-4c67-46ef-9b79-b3f363c0ae6b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/e81a75d3-4c67-46ef-9b79-b3f363c0ae6b
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/e81a75d3-4c67-46ef-9b79-b3f363c0ae6b
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:523955a7-2811-4a40-9677-5df03af403cb --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdc1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[ceph8][DEBUG ] meta-data=/dev/sdc1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.ZehjL8 with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.ZehjL8
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.ZehjL8
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.ZehjL8/journal -> /dev/disk/by-partuuid/e81a75d3-4c67-46ef-9b79-b3f363c0ae6b
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.ZehjL8
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.ZehjL8
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb3a176128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffb3a5d4758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdd:/dev/sdd
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdd journal /dev/sdd activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6bd74365-2fb9-4266-9980-fa685122ba9b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6bd74365-2fb9-4266-9980-fa685122ba9b
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6bd74365-2fb9-4266-9980-fa685122ba9b
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:89d3fce8-11c7-4943-97b7-c9871a183567 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph8][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.266UMU with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.266UMU
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.266UMU
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.266UMU/journal -> /dev/disk/by-partuuid/6bd74365-2fb9-4266-9980-fa685122ba9b
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.266UMU
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.266UMU
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f86b289b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f86b2cf9758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sde:/dev/sde
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sde journal /dev/sde activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:aeecfbcf-d7f3-4879-8378-14300659043e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/aeecfbcf-d7f3-4879-8378-14300659043e
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/aeecfbcf-d7f3-4879-8378-14300659043e
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3161a65b-6091-47d0-a445-48808651c55b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph8][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.XzT5Ya with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.XzT5Ya
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.XzT5Ya
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.XzT5Ya/journal -> /dev/disk/by-partuuid/aeecfbcf-d7f3-4879-8378-14300659043e
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.XzT5Ya
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.XzT5Ya
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd87392c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd873d8a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sda:/dev/sda
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sda journal /dev/sda activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5b03ec0d-c329-49a7-9d87-ad889491c8b1 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5b03ec0d-c329-49a7-9d87-ad889491c8b1
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5b03ec0d-c329-49a7-9d87-ad889491c8b1
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:352b6702-6ff6-4247-8835-d7d017e02d18 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph10][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.2i_wmI with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.2i_wmI
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.2i_wmI
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.2i_wmI/journal -> /dev/disk/by-partuuid/5b03ec0d-c329-49a7-9d87-ad889491c8b1
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.2i_wmI
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.2i_wmI
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9b0f1b9128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9b0f617758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdb:/dev/sdb
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdb journal /dev/sdb activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:96b13d10-f37c-489f-95d0-4227f8ae6cb1 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/96b13d10-f37c-489f-95d0-4227f8ae6cb1
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/96b13d10-f37c-489f-95d0-4227f8ae6cb1
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4d07ae1c-8bad-4f26-8bbf-a08ee34399bb --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph10][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.quaBmz with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.quaBmz
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.quaBmz
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.quaBmz/journal -> /dev/disk/by-partuuid/96b13d10-f37c-489f-95d0-4227f8ae6cb1
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.quaBmz
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.quaBmz
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f77aab02128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f77aaf60758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdd:/dev/sdd
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdd journal /dev/sdd activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:2b0425d8-5dfb-4d2e-b789-412dc3102c0f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/2b0425d8-5dfb-4d2e-b789-412dc3102c0f
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/2b0425d8-5dfb-4d2e-b789-412dc3102c0f
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:caac287f-708a-4216-8df5-7829b9acac46 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph10][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.4N_NKk with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.4N_NKk
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.4N_NKk
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.4N_NKk/journal -> /dev/disk/by-partuuid/2b0425d8-5dfb-4d2e-b789-412dc3102c0f
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.4N_NKk
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.4N_NKk
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff43dd6f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff43e1cd758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sde:/dev/sde
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sde journal /dev/sde activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:faae05b8-74d1-47c3-b99e-c3a949199cbc --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/faae05b8-74d1-47c3-b99e-c3a949199cbc
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/faae05b8-74d1-47c3-b99e-c3a949199cbc
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:5abbd01a-4e1c-45d6-b680-162bd03bfcf2 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph10][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.s56uDa with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.s56uDa
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.s56uDa
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.s56uDa/journal -> /dev/disk/by-partuuid/faae05b8-74d1-47c3-b99e-c3a949199cbc
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.s56uDa
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.s56uDa
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f65540aa128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6554508758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sda:/dev/sda
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sda journal /dev/sda activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ed16b2b0-4e87-40ed-9f87-77d3bc00a0af --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ed16b2b0-4e87-40ed-9f87-77d3bc00a0af
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ed16b2b0-4e87-40ed-9f87-77d3bc00a0af
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ad35068d-3559-49b4-9d7f-15d9ceaf70b9 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph11][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.x1cSE_ with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.x1cSE_
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.x1cSE_
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.x1cSE_/journal -> /dev/disk/by-partuuid/ed16b2b0-4e87-40ed-9f87-77d3bc00a0af
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.x1cSE_
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.x1cSE_
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5415fac128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f541640a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdb:/dev/sdb
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdb journal /dev/sdb activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b1579925-d782-49c6-8491-3245715c8874 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b1579925-d782-49c6-8491-3245715c8874
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b1579925-d782-49c6-8491-3245715c8874
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:bfd65298-a485-4f6e-8f01-6c1c74fd9fbe --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph11][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.pZPxGl with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.pZPxGl
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.pZPxGl
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.pZPxGl/journal -> /dev/disk/by-partuuid/b1579925-d782-49c6-8491-3245715c8874
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.pZPxGl
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.pZPxGl
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7947327128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7947785758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdd:/dev/sdd
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdd journal /dev/sdd activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:795a4ba5-125e-4667-b06a-b99c1b41cd57 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/795a4ba5-125e-4667-b06a-b99c1b41cd57
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/795a4ba5-125e-4667-b06a-b99c1b41cd57
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d6f649bd-c659-423e-9ccc-91d6c91206d5 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph11][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.Qn3xFX with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.Qn3xFX
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Qn3xFX
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Qn3xFX/journal -> /dev/disk/by-partuuid/795a4ba5-125e-4667-b06a-b99c1b41cd57
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Qn3xFX
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Qn3xFX
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f800aa3b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f800ae99758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sde:/dev/sde
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sde journal /dev/sde activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:acd86d54-8de0-4ca5-b7d6-799b18ff9c3e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/acd86d54-8de0-4ca5-b7d6-799b18ff9c3e
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/acd86d54-8de0-4ca5-b7d6-799b18ff9c3e
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f91bf974-f388-4f3a-a162-34bd0da09437 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph11][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.BFWYz3 with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.BFWYz3
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.BFWYz3
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.BFWYz3/journal -> /dev/disk/by-partuuid/acd86d54-8de0-4ca5-b7d6-799b18ff9c3e
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.BFWYz3
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.BFWYz3
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4d3a881128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4d3acdf758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sda:/dev/sda
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sda journal /dev/sda activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:66f219ef-767f-4d5a-ad1a-734e43da1bd4 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/66f219ef-767f-4d5a-ad1a-734e43da1bd4
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/66f219ef-767f-4d5a-ad1a-734e43da1bd4
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a2bc7c69-6d2b-4db6-ab80-6e0a2450e6c3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph12][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.o0hQko with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.o0hQko
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.o0hQko
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.o0hQko/journal -> /dev/disk/by-partuuid/66f219ef-767f-4d5a-ad1a-734e43da1bd4
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.o0hQko
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.o0hQko
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9d05afe128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9d05f5c758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdb journal /dev/sdb activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a364d3ad-2c44-4b18-9b24-ba226f6dd475 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a364d3ad-2c44-4b18-9b24-ba226f6dd475
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a364d3ad-2c44-4b18-9b24-ba226f6dd475
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:df3c24ca-2822-478d-a863-7b5711cca640 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph12][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.ubK2ux with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.ubK2ux
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.ubK2ux
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.ubK2ux/journal -> /dev/disk/by-partuuid/a364d3ad-2c44-4b18-9b24-ba226f6dd475
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.ubK2ux
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.ubK2ux
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6d815d4128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6d81a32758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdd:/dev/sdd
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdd journal /dev/sdd activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:d49d9fa0-bf4c-4e9d-bd5f-fb7ab24511be --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d49d9fa0-bf4c-4e9d-bd5f-fb7ab24511be
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d49d9fa0-bf4c-4e9d-bd5f-fb7ab24511be
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4ec43090-34d0-4563-b448-ca1bad7bbde8 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph12][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.Do65Ir with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.Do65Ir
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Do65Ir
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Do65Ir/journal -> /dev/disk/by-partuuid/d49d9fa0-bf4c-4e9d-bd5f-fb7ab24511be
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Do65Ir
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Do65Ir
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f06c8423128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f06c8881758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sde:/dev/sde
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sde journal /dev/sde activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6e147148-c0ea-4b45-9fc3-8c33d09f54be --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6e147148-c0ea-4b45-9fc3-8c33d09f54be
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6e147148-c0ea-4b45-9fc3-8c33d09f54be
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c1340a73-169f-4c21-b93f-4b6360dfce77 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph12][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.fQnSrt with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.fQnSrt
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.fQnSrt
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.fQnSrt/journal -> /dev/disk/by-partuuid/6e147148-c0ea-4b45-9fc3-8c33d09f54be
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.fQnSrt
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.fQnSrt
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff38eda7128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff38f205758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sda journal /dev/sda activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:080eb38c-fe53-4fb4-ae68-393ba5a9f428 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/080eb38c-fe53-4fb4-ae68-393ba5a9f428
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/080eb38c-fe53-4fb4-ae68-393ba5a9f428
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:fad97ac6-f2ba-4a34-a833-816884ef9f91 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph15][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.IG4MMN with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.IG4MMN
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.IG4MMN
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.IG4MMN/journal -> /dev/disk/by-partuuid/080eb38c-fe53-4fb4-ae68-393ba5a9f428
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.IG4MMN
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.IG4MMN
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdc:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdc', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbe9f2c6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fbe9f724758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdc:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdc journal /dev/sdb activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdb
[ceph15][WARNING] ceph-disk: Error: Device is mounted: /dev/sdc1
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdba023e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdba069c758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdd:/dev/sdd
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdd journal /dev/sdd activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:367cfd71-e812-4193-a361-5f750fcb042a --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/367cfd71-e812-4193-a361-5f750fcb042a
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/367cfd71-e812-4193-a361-5f750fcb042a
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:084d2ec6-3e3e-4e5a-ace0-18342a395282 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph15][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.13rTy2 with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.13rTy2
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.13rTy2
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.13rTy2/journal -> /dev/disk/by-partuuid/367cfd71-e812-4193-a361-5f750fcb042a
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.13rTy2
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.13rTy2
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc3b5ad4128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc3b5f32758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sde:/dev/sde
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sde journal /dev/sde activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:0220a91c-9fbd-4660-9bab-a8de5e30614f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/0220a91c-9fbd-4660-9bab-a8de5e30614f
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/0220a91c-9fbd-4660-9bab-a8de5e30614f
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:6995c5db-fa35-4491-8c85-b6d685b28a16 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph15][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.J1lYl0 with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.J1lYl0
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.J1lYl0
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.J1lYl0/journal -> /dev/disk/by-partuuid/0220a91c-9fbd-4660-9bab-a8de5e30614f
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.J1lYl0
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.J1lYl0
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf14add128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcf14f3b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sda:/dev/sda
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sda journal /dev/sda activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a37a4018-9390-46aa-91a9-f2e1783d3192 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a37a4018-9390-46aa-91a9-f2e1783d3192
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a37a4018-9390-46aa-91a9-f2e1783d3192
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a1e07445-3195-4c75-b291-a7ccca75a00c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph16][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.mjMvk1 with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.mjMvk1
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.mjMvk1
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.mjMvk1/journal -> /dev/disk/by-partuuid/a37a4018-9390-46aa-91a9-f2e1783d3192
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.mjMvk1
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.mjMvk1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdf9cf18128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdf9d376758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdb:/dev/sdb
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdb journal /dev/sdb activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a7a2143c-1b08-454d-a5e9-937a9a3ffe04 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a7a2143c-1b08-454d-a5e9-937a9a3ffe04
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a7a2143c-1b08-454d-a5e9-937a9a3ffe04
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c04aca73-6829-45db-a7b2-271c30285a29 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph16][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.sk_VXr with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.sk_VXr
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.sk_VXr
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.sk_VXr/journal -> /dev/disk/by-partuuid/a7a2143c-1b08-454d-a5e9-937a9a3ffe04
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.sk_VXr
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.sk_VXr
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9347a13128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9347e71758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdd:/dev/sdd
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdd journal /dev/sdd activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:2a675048-fb0f-4ef9-93f2-21835a3cf426 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/2a675048-fb0f-4ef9-93f2-21835a3cf426
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/2a675048-fb0f-4ef9-93f2-21835a3cf426
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d0ed945e-b962-4143-89f1-a39047004e50 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph16][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.gJVRwe with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.gJVRwe
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.gJVRwe
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.gJVRwe/journal -> /dev/disk/by-partuuid/2a675048-fb0f-4ef9-93f2-21835a3cf426
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.gJVRwe
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.gJVRwe
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f00f586c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f00f5cca758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sde:/dev/sde
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sde journal /dev/sde activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:574d7a92-c1fe-478e-9536-a90f4ba49741 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/574d7a92-c1fe-478e-9536-a90f4ba49741
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/574d7a92-c1fe-478e-9536-a90f4ba49741
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:8278cc9d-39d6-48b6-8662-94c95ac78574 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph16][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.z2hjEM with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.z2hjEM
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.z2hjEM
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.z2hjEM/journal -> /dev/disk/by-partuuid/574d7a92-c1fe-478e-9536-a90f4ba49741
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.z2hjEM
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.z2hjEM
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6857934128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6857d92758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sda:/dev/sda
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sda journal /dev/sda activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:aa38584b-0d7e-4486-af64-06372be0deef --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/aa38584b-0d7e-4486-af64-06372be0deef
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/aa38584b-0d7e-4486-af64-06372be0deef
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:8d92a7c8-293b-45ec-96eb-5ccf5bf34d30 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph17][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.rVpMSZ with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.rVpMSZ
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.rVpMSZ
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.rVpMSZ/journal -> /dev/disk/by-partuuid/aa38584b-0d7e-4486-af64-06372be0deef
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.rVpMSZ
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.rVpMSZ
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2828a2e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2828e8c758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdb:/dev/sdb
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdb journal /dev/sdb activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:3d3f7fe7-7f35-43d2-b155-d0fdae0acf6b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3d3f7fe7-7f35-43d2-b155-d0fdae0acf6b
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3d3f7fe7-7f35-43d2-b155-d0fdae0acf6b
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:af810a4f-a5d4-437e-b4fc-a391a2fb00e6 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph17][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt._F6H6V with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt._F6H6V
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt._F6H6V
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt._F6H6V/journal -> /dev/disk/by-partuuid/3d3f7fe7-7f35-43d2-b155-d0fdae0acf6b
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt._F6H6V
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt._F6H6V
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ee043d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0ee089b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdd:/dev/sdd
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdd journal /dev/sdd activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:d5e047d2-757c-4629-af17-941d76b5d525 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d5e047d2-757c-4629-af17-941d76b5d525
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d5e047d2-757c-4629-af17-941d76b5d525
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:58e07274-aecf-4b07-b440-a08166148311 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph17][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.Bygvbh with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.Bygvbh
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Bygvbh
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Bygvbh/journal -> /dev/disk/by-partuuid/d5e047d2-757c-4629-af17-941d76b5d525
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Bygvbh
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Bygvbh
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8104b6e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8104fcc758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sde:/dev/sde
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sde journal /dev/sde activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6821e014-d416-4ce2-b911-caeebe863a99 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6821e014-d416-4ce2-b911-caeebe863a99
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6821e014-d416-4ce2-b911-caeebe863a99
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:218599f0-2770-4b19-af7d-a02370949b8c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph17][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.aWRijb with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.aWRijb
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.aWRijb
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.aWRijb/journal -> /dev/disk/by-partuuid/6821e014-d416-4ce2-b911-caeebe863a99
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.aWRijb
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.aWRijb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy admin ceph1 ceph2 ceph5 ceph6 ceph7 ceph8 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4980322dd0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['ceph1', 'ceph2', 'ceph5', 'ceph6', 'ceph7', 'ceph8', 'ceph10', 'ceph11', 'ceph12', 'ceph15', 'ceph16', 'ceph17']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f4980bb66e0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph7
[ceph_deploy.admin][ERROR ] connecting to host: ceph7 resulted in errors: HostNotFound ceph7
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy admin ceph1 ceph2 ceph5 ceph6 ceph7 ceph8 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f47e07bcdd0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['ceph1', 'ceph2', 'ceph5', 'ceph6', 'ceph7', 'ceph8', 'ceph10', 'ceph11', 'ceph12', 'ceph15', 'ceph16', 'ceph17']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f47e10506e0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph7
[ceph_deploy.admin][ERROR ] connecting to host: ceph7 resulted in errors: HostNotFound ceph7
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to configure 12 admin hosts

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph5 ceph6 ceph7 ceph8 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fefb922add0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['ceph1', 'ceph2', 'ceph5', 'ceph6', 'ceph7', 'ceph8', 'ceph10', 'ceph11', 'ceph12', 'ceph15', 'ceph16', 'ceph17']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fefb9ac06e0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph7
[ceph_deploy.admin][ERROR ] connecting to host: ceph7 resulted in errors: HostNotFound ceph7
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcec53ba128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcec5818758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdb:/dev/sdb
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdb journal /dev/sdb activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:571f2c71-e0ce-4948-ba75-3736e8b7dc1b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph2][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph2][WARNING] Error encountered; not saving changes.
[ceph2][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:571f2c71-e0ce-4948-ba75-3736e8b7dc1b', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdb']' returned non-zero exit status 4
[ceph2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9d2e769128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9d2ebc7758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdd:/dev/sdd
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdd journal /dev/sdd activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph2][WARNING] ceph-disk: Error: Device is mounted: /dev/sdd1
[ceph2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f202d72b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f202db89758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sde:/dev/sde
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sde journal /dev/sde activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:500ada68-fdc9-4353-8550-ea553bc30a7d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph2][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph2][WARNING] Error encountered; not saving changes.
[ceph2][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:500ada68-fdc9-4353-8550-ea553bc30a7d', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9acb621128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9acba7f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sda:/dev/sda
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sda journal /dev/sda activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:98c2747e-e24b-4327-901c-54c8a6a6dc51 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph5][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph5][WARNING] Error encountered; not saving changes.
[ceph5][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:98c2747e-e24b-4327-901c-54c8a6a6dc51', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph5][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd454a2c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd454e8a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdb:/dev/sdb
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdb journal /dev/sdb activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:d14f8f19-8ba2-4846-8614-8342d60a1cfc --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph5][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph5][WARNING] Error encountered; not saving changes.
[ceph5][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:d14f8f19-8ba2-4846-8614-8342d60a1cfc', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdb']' returned non-zero exit status 4
[ceph5][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffa2b0f5128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffa2b553758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdd:/dev/sdd
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdd journal /dev/sdd activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph5][WARNING] ceph-disk: Error: Device is mounted: /dev/sdd1
[ceph5][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc923197128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc9235f5758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sde:/dev/sde
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sde journal /dev/sde activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:bd48ae35-15d6-4836-a9d6-cdb7665e030b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph5][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph5][WARNING] Error encountered; not saving changes.
[ceph5][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:bd48ae35-15d6-4836-a9d6-cdb7665e030b', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph5][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b3bb5a128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3b3bfb8758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sda:/dev/sda
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sda journal /dev/sda activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:c086baa4-251f-49ff-8a9b-8a5aaaf474eb --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph6][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph6][WARNING] Error encountered; not saving changes.
[ceph6][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:c086baa4-251f-49ff-8a9b-8a5aaaf474eb', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f347f3c6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f347f824758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdb:/dev/sdb
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdb journal /dev/sdb activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b9449c0a-e2cf-4407-a71f-7afd4856d618 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph6][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph6][WARNING] Error encountered; not saving changes.
[ceph6][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:b9449c0a-e2cf-4407-a71f-7afd4856d618', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdb']' returned non-zero exit status 4
[ceph6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1cc1e87128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1cc22e5758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdd:/dev/sdd
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdd journal /dev/sdd activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:3a5b0423-a139-4ea4-ba3d-aca394f060c3 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph6][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph6][WARNING] Error encountered; not saving changes.
[ceph6][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:3a5b0423-a139-4ea4-ba3d-aca394f060c3', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fed0ba16128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fed0be74758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sde:/dev/sde
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sde journal /dev/sde activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph6][WARNING] ceph-disk: Error: Device is mounted: /dev/sde1
[ceph6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdacd0a7128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdacd505758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sda:/dev/sda
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sda journal /dev/sda activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b3c9ef90-34b5-496c-8d76-f8d1454f4894 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph8][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph8][WARNING] Error encountered; not saving changes.
[ceph8][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:b3c9ef90-34b5-496c-8d76-f8d1454f4894', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph8][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb5b4f3d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb5b539b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdc:/dev/sdc
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdc journal /dev/sdc activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph8][WARNING] ceph-disk: Error: Device is mounted: /dev/sdc1
[ceph8][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f468e7f2128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f468ec50758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdd:/dev/sdd
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdd journal /dev/sdd activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:97774da5-2311-4e01-8e4f-7e8a0c0dddda --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph8][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph8][WARNING] Error encountered; not saving changes.
[ceph8][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:97774da5-2311-4e01-8e4f-7e8a0c0dddda', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph8][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f497c62d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f497ca8b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sde:/dev/sde
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sde journal /dev/sde activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:1408113c-f594-4207-b98f-8c056e82c922 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph8][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph8][WARNING] Error encountered; not saving changes.
[ceph8][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:1408113c-f594-4207-b98f-8c056e82c922', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph8][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5d24fab128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5d25409758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sda:/dev/sda
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sda journal /dev/sda activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:18b93686-fd6a-497f-82cf-a1c0ad69efb6 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph10][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph10][WARNING] Error encountered; not saving changes.
[ceph10][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:18b93686-fd6a-497f-82cf-a1c0ad69efb6', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph10][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f506d5d5128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f506da33758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdb:/dev/sdb
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdb journal /dev/sdb activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b6b673b2-34d7-4908-bced-1e4a27413a9a --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph10][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph10][WARNING] Error encountered; not saving changes.
[ceph10][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:b6b673b2-34d7-4908-bced-1e4a27413a9a', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdb']' returned non-zero exit status 4
[ceph10][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f79cfdda128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f79d0238758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdd:/dev/sdd
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdd journal /dev/sdd activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph10][WARNING] ceph-disk: Error: Device is mounted: /dev/sdd1
[ceph10][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6fe8c1b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6fe9079758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sde:/dev/sde
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sde journal /dev/sde activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:2e8d3884-70b7-4e86-b90f-490387e9fd3d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph10][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph10][WARNING] Error encountered; not saving changes.
[ceph10][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:2e8d3884-70b7-4e86-b90f-490387e9fd3d', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph10][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f262ba04128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f262be62758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sda:/dev/sda
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sda journal /dev/sda activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:fb37df80-0a25-45ab-8ca0-8605f26fa9ff --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph11][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph11][WARNING] Error encountered; not saving changes.
[ceph11][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:fb37df80-0a25-45ab-8ca0-8605f26fa9ff', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph11][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f012d6b5128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f012db13758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdb:/dev/sdb
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdb journal /dev/sdb activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph11][WARNING] ceph-disk: Error: Device is mounted: /dev/sdb1
[ceph11][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5e6ec2d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5e6f08b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdd:/dev/sdd
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdd journal /dev/sdd activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:041d644c-ee47-4e39-b812-ac2e6c5d5668 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph11][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph11][WARNING] Error encountered; not saving changes.
[ceph11][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:041d644c-ee47-4e39-b812-ac2e6c5d5668', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph11][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f09963bc128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f099681a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sde:/dev/sde
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sde journal /dev/sde activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5fe9323c-a67b-4f14-949b-9ed43a6f6f20 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph11][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph11][WARNING] Error encountered; not saving changes.
[ceph11][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:5fe9323c-a67b-4f14-949b-9ed43a6f6f20', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph11][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5059c29128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f505a087758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sda:/dev/sda
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sda journal /dev/sda activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4f926b2d-45d1-4fd4-b634-537abbabc948 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph12][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph12][WARNING] Error encountered; not saving changes.
[ceph12][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:4f926b2d-45d1-4fd4-b634-537abbabc948', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph12][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f25ed645128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f25edaa3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdb journal /dev/sdb activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph12][WARNING] ceph-disk: Error: Device is mounted: /dev/sdb1
[ceph12][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f573f3f0128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f573f84e758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdd:/dev/sdd
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdd journal /dev/sdd activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:73c660e7-86be-403b-b3fd-0883d863dc8b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph12][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph12][WARNING] Error encountered; not saving changes.
[ceph12][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:73c660e7-86be-403b-b3fd-0883d863dc8b', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph12][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fee2f798128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fee2fbf6758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sde:/dev/sde
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sde journal /dev/sde activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:c952aec9-25a3-45eb-893b-040fbd6e1a4f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph12][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph12][WARNING] Error encountered; not saving changes.
[ceph12][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:c952aec9-25a3-45eb-893b-040fbd6e1a4f', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph12][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6cb0a44128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6cb0ea2758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sda journal /dev/sda activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:d2a5a622-9512-43aa-aa89-26106cd39697 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph15][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph15][WARNING] Error encountered; not saving changes.
[ceph15][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:d2a5a622-9512-43aa-aa89-26106cd39697', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdc:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdc', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9a82197128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9a825f5758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdc:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdc journal /dev/sdb activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdb
[ceph15][WARNING] ceph-disk: Error: Device is mounted: /dev/sdc1
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde810c4128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fde81522758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdd:/dev/sdd
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdd journal /dev/sdd activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:cc366db1-fe56-43f5-a98c-867bb6e642a6 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph15][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph15][WARNING] Error encountered; not saving changes.
[ceph15][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:cc366db1-fe56-43f5-a98c-867bb6e642a6', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff020f6a128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff0213c8758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sde:/dev/sde
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sde journal /dev/sde activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:954a44f2-a7b2-4325-a056-bb9e855d2d33 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph15][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph15][WARNING] Error encountered; not saving changes.
[ceph15][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:954a44f2-a7b2-4325-a056-bb9e855d2d33', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f257c8bd128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f257cd1b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sda:/dev/sda
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sda journal /dev/sda activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:520d241d-0edb-4f9f-beb4-b35dd269cfe4 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph16][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph16][WARNING] Error encountered; not saving changes.
[ceph16][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:520d241d-0edb-4f9f-beb4-b35dd269cfe4', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f50aed49128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f50af1a7758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdb:/dev/sdb
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdb journal /dev/sdb activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph16][WARNING] ceph-disk: Error: Device is mounted: /dev/sdb1
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb2ccad2128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb2ccf30758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdd:/dev/sdd
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdd journal /dev/sdd activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6c4164c8-404b-4986-92e0-7ad6fe4b7340 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph16][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph16][WARNING] Error encountered; not saving changes.
[ceph16][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:6c4164c8-404b-4986-92e0-7ad6fe4b7340', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdee3d83128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdee41e1758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sde:/dev/sde
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sde journal /dev/sde activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5d1df73b-e0e0-4137-84b2-a490d0b4f3b4 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph16][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph16][WARNING] Error encountered; not saving changes.
[ceph16][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:5d1df73b-e0e0-4137-84b2-a490d0b4f3b4', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sde']' returned non-zero exit status 4
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdddaf11128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdddb36f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sda:/dev/sda
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sda journal /dev/sda activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:83e911a3-6681-4881-9b62-d375cb5b1956 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph17][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph17][WARNING] Error encountered; not saving changes.
[ceph17][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:83e911a3-6681-4881-9b62-d375cb5b1956', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sda']' returned non-zero exit status 4
[ceph17][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b26db9128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3b27217758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdb:/dev/sdb
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdb journal /dev/sdb activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:e118fa8d-70b9-4c2e-99f7-26d1c8ba183d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph17][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph17][WARNING] Error encountered; not saving changes.
[ceph17][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:e118fa8d-70b9-4c2e-99f7-26d1c8ba183d', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdb']' returned non-zero exit status 4
[ceph17][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde0d99d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fde0ddfb758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdd:/dev/sdd
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdd journal /dev/sdd activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6932b926-360d-40cd-adaf-cccd01515aee --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph17][WARNING] Could not create partition 2 from 10485761 to 10485760
[ceph17][WARNING] Error encountered; not saving changes.
[ceph17][WARNING] ceph-disk: Error: Command '['/sbin/sgdisk', '--new=2:0:5120M', '--change-name=2:ceph journal', '--partition-guid=2:6932b926-360d-40cd-adaf-cccd01515aee', '--typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdd']' returned non-zero exit status 4
[ceph17][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9f7d701128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9f7db5f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sde:/dev/sde
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sde journal /dev/sde activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph17][WARNING] ceph-disk: Error: Device is mounted: /dev/sde1
[ceph17][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd85f80f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd85fc6d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdb:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdb journal /dev/sdb activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ac008513-24fa-4bed-afb7-64587ccf7b60 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ac008513-24fa-4bed-afb7-64587ccf7b60
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ac008513-24fa-4bed-afb7-64587ccf7b60
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:9eb15410-2c63-44cc-b3b1-977a8c2d92af --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph15][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.qYQT0B with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.qYQT0B
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.qYQT0B
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.qYQT0B/journal -> /dev/disk/by-partuuid/ac008513-24fa-4bed-afb7-64587ccf7b60
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.qYQT0B
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.qYQT0B
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy purgedata ceph2 ceph2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdd805f2cb0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph2', 'ceph2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fdd80efb140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts ceph2 ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.install][ERROR ] Ceph is still installed on: ['ceph2', 'ceph2']
[ceph_deploy][ERROR ] RuntimeError: refusing to purge data while Ceph is still installed

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy purgedata ceph2 ceph2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f41f00d2cb0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph2', 'ceph2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f41f09db140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts ceph2 ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.install][ERROR ] Ceph is still installed on: ['ceph2', 'ceph2']
[ceph_deploy][ERROR ] RuntimeError: refusing to purge data while Ceph is still installed

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy purgedata ceph2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd92c08fcb0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fd92c998140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.install][ERROR ] Ceph is still installed on: ['ceph2']
[ceph_deploy][ERROR ] RuntimeError: refusing to purge data while Ceph is still installed

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy purge ceph2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f193d194680>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph2']
[ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f193da9d0c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Purging from cluster ceph hosts ceph2
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph2 ...
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][INFO  ] purging host ... ceph2
[ceph2][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph2][DEBUG ] Reading package lists...
[ceph2][DEBUG ] Building dependency tree...
[ceph2][DEBUG ] Reading state information...
[ceph2][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph2][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin libaio1 libboost-program-options1.54.0
[ceph2][DEBUG ]   libboost-system1.54.0 libboost-thread1.54.0 libcephfs1 libcryptsetup4
[ceph2][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph2][DEBUG ]   liblttng-ust0 liblzo2-2 libnspr4 libnss3 libnss3-nssdb librados2
[ceph2][DEBUG ]   libradosstriper1 librbd1 libsnappy1 libtcmalloc-minimal4 libunwind8 liburcu1
[ceph2][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph2][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph2][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph2][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph2][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-58-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph2][DEBUG ]   python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
[ceph2][DEBUG ]   python-markupsafe python-pyinotify python-rados python-rbd python-werkzeug
[ceph2][DEBUG ]   xfsprogs
[ceph2][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph2][DEBUG ] The following packages will be REMOVED:
[ceph2][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph2][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 53 not upgraded.
[ceph2][DEBUG ] After this operation, 36.6 MB disk space will be freed.
[ceph2][DEBUG ] (Reading database ... 293636 files and directories currently installed.)
[ceph2][DEBUG ] Removing ceph (0.94.3-1trusty) ...
[ceph2][DEBUG ] Purging configuration files for ceph (0.94.3-1trusty) ...
[ceph2][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph2][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph2][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph2][DEBUG ] Removing radosgw (0.94.3-1trusty) ...
[ceph2][WARNING] stop: Unknown instance: 
[ceph2][DEBUG ] Purging configuration files for radosgw (0.94.3-1trusty) ...
[ceph2][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph2][WARNING] dpkg: warning: while removing radosgw, directory '/var/log/radosgw' not empty so not removed
[ceph2][DEBUG ] Removing ceph-common (0.94.3-1trusty) ...
[ceph2][DEBUG ] Purging configuration files for ceph-common (0.94.3-1trusty) ...
[ceph2][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph2][DEBUG ] Removing ceph-mds (0.94.3-1trusty) ...
[ceph2][DEBUG ] Purging configuration files for ceph-mds (0.94.3-1trusty) ...
[ceph2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy purge ceph4 ceph5 ceph6 ceph8 ceph9 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f79c6ab9638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph4', 'ceph5', 'ceph6', 'ceph8', 'ceph9', 'ceph10', 'ceph11', 'ceph12', 'ceph15', 'ceph16', 'ceph17']
[ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f79c73c20c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Purging from cluster ceph hosts ceph4 ceph5 ceph6 ceph8 ceph9 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph4 ...
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][INFO  ] purging host ... ceph4
[ceph4][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph4][DEBUG ] Reading package lists...
[ceph4][DEBUG ] Building dependency tree...
[ceph4][DEBUG ] Reading state information...
[ceph4][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph4][DEBUG ]   cryptsetup-bin libaio1 libboost-program-options1.54.0 libboost-system1.54.0
[ceph4][DEBUG ]   libboost-thread1.54.0 libcephfs1 libcryptsetup4 libfcgi0ldbl
[ceph4][DEBUG ]   libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph4][DEBUG ]   liblttng-ust0 libnspr4 libnss3 libnss3-nssdb librados2 librbd1 libsnappy1
[ceph4][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-58
[ceph4][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph4][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph4][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph4][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph4][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph4][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph4][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-58-generic
[ceph4][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph4][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph4][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-67-generic python-cephfs python-flask
[ceph4][DEBUG ]   python-itsdangerous python-jinja2 python-markupsafe python-rados python-rbd
[ceph4][DEBUG ]   python-werkzeug xfsprogs
[ceph4][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph4][DEBUG ] The following packages will be REMOVED:
[ceph4][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph4][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 48 not upgraded.
[ceph4][DEBUG ] After this operation, 126 MB disk space will be freed.
[ceph4][DEBUG ] (Reading database ... 293601 files and directories currently installed.)
[ceph4][DEBUG ] Removing ceph-mds (0.94.3-1trusty) ...
[ceph4][DEBUG ] ceph-mds-all stop/waiting
[ceph4][DEBUG ] Purging configuration files for ceph-mds (0.94.3-1trusty) ...
[ceph4][DEBUG ] Removing ceph (0.94.3-1trusty) ...
[ceph4][DEBUG ] ceph-all stop/waiting
[ceph4][DEBUG ] Purging configuration files for ceph (0.94.3-1trusty) ...
[ceph4][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph4][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph4][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph4][DEBUG ] Removing radosgw (0.94.3-1trusty) ...
[ceph4][WARNING] stop: Unknown instance: 
[ceph4][DEBUG ] Purging configuration files for radosgw (0.94.3-1trusty) ...
[ceph4][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph4][DEBUG ] Removing ceph-common (0.94.3-1trusty) ...
[ceph4][DEBUG ] Purging configuration files for ceph-common (0.94.3-1trusty) ...
[ceph4][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph4][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph4][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph5 ...
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][INFO  ] purging host ... ceph5
[ceph5][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph5][DEBUG ] Reading package lists...
[ceph5][DEBUG ] Building dependency tree...
[ceph5][DEBUG ] Reading state information...
[ceph5][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[ceph5][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph5][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin libaio1 libboost-program-options1.54.0
[ceph5][DEBUG ]   libboost-system1.54.0 libboost-thread1.54.0 libcephfs1 libcryptsetup4
[ceph5][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph5][DEBUG ]   liblttng-ust0 liblzo2-2 libnspr4 libnss3 libnss3-nssdb librados2
[ceph5][DEBUG ]   libradosstriper1 librbd1 libsnappy1 libtcmalloc-minimal4 libunwind8 liburcu1
[ceph5][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph5][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph5][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph5][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph5][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph5][DEBUG ]   python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
[ceph5][DEBUG ]   python-markupsafe python-pyinotify python-rados python-rbd python-werkzeug
[ceph5][DEBUG ]   xfsprogs
[ceph5][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph5][DEBUG ] The following packages will be REMOVED:
[ceph5][DEBUG ]   ceph* ceph-common* ceph-mds* radosgw*
[ceph5][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 53 not upgraded.
[ceph5][DEBUG ] After this operation, 126 MB disk space will be freed.
[ceph5][DEBUG ] (Reading database ... 323365 files and directories currently installed.)
[ceph5][DEBUG ] Removing ceph-mds (0.94.3-1trusty) ...
[ceph5][DEBUG ] ceph-mds-all stop/waiting
[ceph5][DEBUG ] Purging configuration files for ceph-mds (0.94.3-1trusty) ...
[ceph5][DEBUG ] Removing ceph (0.94.3-1trusty) ...
[ceph5][DEBUG ] ceph-all stop/waiting
[ceph5][DEBUG ] Purging configuration files for ceph (0.94.3-1trusty) ...
[ceph5][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph5][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph5][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph5][DEBUG ] Removing radosgw (0.94.3-1trusty) ...
[ceph5][WARNING] stop: Unknown instance: 
[ceph5][DEBUG ] Purging configuration files for radosgw (0.94.3-1trusty) ...
[ceph5][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph5][DEBUG ] Removing ceph-common (0.94.3-1trusty) ...
[ceph5][DEBUG ] Purging configuration files for ceph-common (0.94.3-1trusty) ...
[ceph5][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph5][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph6 ...
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][INFO  ] purging host ... ceph6
[ceph6][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph6][DEBUG ] Reading package lists...
[ceph6][DEBUG ] Building dependency tree...
[ceph6][DEBUG ] Reading state information...
[ceph6][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[ceph6][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph6][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin libaio1 libboost-program-options1.54.0
[ceph6][DEBUG ]   libboost-system1.54.0 libboost-thread1.54.0 libcephfs1 libcryptsetup4
[ceph6][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph6][DEBUG ]   liblttng-ust0 liblzo2-2 libnspr4 libnss3 libnss3-nssdb librados2
[ceph6][DEBUG ]   libradosstriper1 librbd1 libsnappy1 libtcmalloc-minimal4 libunwind8 liburcu1
[ceph6][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph6][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph6][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph6][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph6][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph6][DEBUG ]   python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
[ceph6][DEBUG ]   python-markupsafe python-pyinotify python-rados python-rbd python-werkzeug
[ceph6][DEBUG ]   xfsprogs
[ceph6][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph6][DEBUG ] The following packages will be REMOVED:
[ceph6][DEBUG ]   ceph* ceph-common* ceph-mds* radosgw*
[ceph6][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 53 not upgraded.
[ceph6][DEBUG ] After this operation, 126 MB disk space will be freed.
[ceph6][DEBUG ] (Reading database ... 323365 files and directories currently installed.)
[ceph6][DEBUG ] Removing ceph-mds (0.94.3-1trusty) ...
[ceph6][DEBUG ] ceph-mds-all stop/waiting
[ceph6][DEBUG ] Purging configuration files for ceph-mds (0.94.3-1trusty) ...
[ceph6][DEBUG ] Removing ceph (0.94.3-1trusty) ...
[ceph6][DEBUG ] ceph-all stop/waiting
[ceph6][DEBUG ] Purging configuration files for ceph (0.94.3-1trusty) ...
[ceph6][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph6][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph6][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph6][DEBUG ] Removing radosgw (0.94.3-1trusty) ...
[ceph6][WARNING] stop: Unknown instance: 
[ceph6][DEBUG ] Purging configuration files for radosgw (0.94.3-1trusty) ...
[ceph6][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph6][DEBUG ] Removing ceph-common (0.94.3-1trusty) ...
[ceph6][DEBUG ] Purging configuration files for ceph-common (0.94.3-1trusty) ...
[ceph6][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph6][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph8 ...
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][INFO  ] purging host ... ceph8
[ceph8][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph8][DEBUG ] Reading package lists...
[ceph8][DEBUG ] Building dependency tree...
[ceph8][DEBUG ] Reading state information...
[ceph8][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph8][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph8][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph8][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph8][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph8][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph8][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph8][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph8][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph8][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph8][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph8][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph8][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph8][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph8][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph8][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph8][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph8][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph8][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph8][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph8][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph8][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph8][DEBUG ] The following packages will be REMOVED:
[ceph8][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph8][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 53 not upgraded.
[ceph8][DEBUG ] After this operation, 126 MB disk space will be freed.
[ceph8][DEBUG ] (Reading database ... 323373 files and directories currently installed.)
[ceph8][DEBUG ] Removing ceph-mds (0.94.3-1trusty) ...
[ceph8][DEBUG ] ceph-mds-all stop/waiting
[ceph8][DEBUG ] Purging configuration files for ceph-mds (0.94.3-1trusty) ...
[ceph8][DEBUG ] Removing ceph (0.94.3-1trusty) ...
[ceph8][DEBUG ] ceph-all stop/waiting
[ceph8][DEBUG ] Purging configuration files for ceph (0.94.3-1trusty) ...
[ceph8][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph8][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph8][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph8][DEBUG ] Removing radosgw (0.94.3-1trusty) ...
[ceph8][WARNING] stop: Unknown instance: 
[ceph8][DEBUG ] Purging configuration files for radosgw (0.94.3-1trusty) ...
[ceph8][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph8][DEBUG ] Removing ceph-common (0.94.3-1trusty) ...
[ceph8][DEBUG ] Purging configuration files for ceph-common (0.94.3-1trusty) ...
[ceph8][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph8][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph8][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph9 ...
[ceph9][DEBUG ] connected to host: ceph9 
[ceph9][DEBUG ] detect platform information from remote host
[ceph9][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph9][INFO  ] purging host ... ceph9
[ceph9][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph9][DEBUG ] Reading package lists...
[ceph9][DEBUG ] Building dependency tree...
[ceph9][DEBUG ] Reading state information...
[ceph9][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph9][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph9][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph9][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph9][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph9][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph9][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph9][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph9][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph9][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph9][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-65
[ceph9][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph9][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph9][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph9][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph9][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-65-generic
[ceph9][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph9][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph9][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph9][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph9][DEBUG ] The following packages will be REMOVED:
[ceph9][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph9][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 60 not upgraded.
[ceph9][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph9][DEBUG ] (Reading database ... 293720 files and directories currently installed.)
[ceph9][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph9][DEBUG ] ceph-mds-all stop/waiting
[ceph9][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph9][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph9][DEBUG ] ceph-all stop/waiting
[ceph9][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph9][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph9][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph9][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph9][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph9][WARNING] stop: Unknown instance: 
[ceph9][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph9][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph9][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph9][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph9][DEBUG ] Removing ceph-fs-common (0.94.2-1trusty) ...
[ceph9][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph9][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph10 ...
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][INFO  ] purging host ... ceph10
[ceph10][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph10][DEBUG ] Reading package lists...
[ceph10][DEBUG ] Building dependency tree...
[ceph10][DEBUG ] Reading state information...
[ceph10][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph10][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph10][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph10][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph10][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph10][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph10][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph10][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph10][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph10][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph10][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph10][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph10][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph10][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph10][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph10][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph10][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph10][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph10][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph10][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph10][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph10][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph10][DEBUG ] The following packages will be REMOVED:
[ceph10][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph10][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 47 not upgraded.
[ceph10][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph10][DEBUG ] (Reading database ... 323441 files and directories currently installed.)
[ceph10][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph10][DEBUG ] ceph-mds-all stop/waiting
[ceph10][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph10][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph10][DEBUG ] ceph-all stop/waiting
[ceph10][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph10][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph10][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph10][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph10][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph10][WARNING] stop: Unknown instance: 
[ceph10][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph10][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph10][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph10][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph10][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph10][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph10][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph11 ...
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][INFO  ] purging host ... ceph11
[ceph11][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph11][DEBUG ] Reading package lists...
[ceph11][DEBUG ] Building dependency tree...
[ceph11][DEBUG ] Reading state information...
[ceph11][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph11][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph11][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph11][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph11][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph11][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph11][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph11][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph11][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph11][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph11][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-65
[ceph11][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph11][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph11][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph11][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph11][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-65-generic
[ceph11][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph11][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph11][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph11][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph11][DEBUG ] The following packages will be REMOVED:
[ceph11][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph11][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 63 not upgraded.
[ceph11][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph11][DEBUG ] (Reading database ... 293732 files and directories currently installed.)
[ceph11][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph11][DEBUG ] ceph-mds-all stop/waiting
[ceph11][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph11][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph11][DEBUG ] ceph-all stop/waiting
[ceph11][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph11][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph11][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph11][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph11][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph11][WARNING] stop: Unknown instance: 
[ceph11][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph11][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph11][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph11][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph11][DEBUG ] Removing ceph-fs-common (0.94.2-1trusty) ...
[ceph11][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph11][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph12 ...
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][INFO  ] purging host ... ceph12
[ceph12][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph12][DEBUG ] Reading package lists...
[ceph12][DEBUG ] Building dependency tree...
[ceph12][DEBUG ] Reading state information...
[ceph12][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph12][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph12][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph12][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph12][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph12][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph12][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph12][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph12][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph12][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph12][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph12][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph12][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph12][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph12][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph12][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph12][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph12][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph12][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph12][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph12][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph12][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph12][DEBUG ] The following packages will be REMOVED:
[ceph12][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph12][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 47 not upgraded.
[ceph12][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph12][DEBUG ] (Reading database ... 323373 files and directories currently installed.)
[ceph12][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph12][DEBUG ] ceph-mds-all stop/waiting
[ceph12][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph12][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph12][DEBUG ] ceph-all stop/waiting
[ceph12][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph12][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph12][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph12][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph12][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph12][WARNING] stop: Unknown instance: 
[ceph12][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph12][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph12][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph12][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph12][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph12][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph12][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph15 ...
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][INFO  ] purging host ... ceph15
[ceph15][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph15][DEBUG ] Reading package lists...
[ceph15][DEBUG ] Building dependency tree...
[ceph15][DEBUG ] Reading state information...
[ceph15][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph15][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph15][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph15][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph15][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph15][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph15][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph15][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph15][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph15][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph15][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph15][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph15][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph15][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph15][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph15][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph15][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph15][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph15][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph15][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph15][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph15][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph15][DEBUG ] The following packages will be REMOVED:
[ceph15][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph15][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 47 not upgraded.
[ceph15][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph15][DEBUG ] (Reading database ... 323373 files and directories currently installed.)
[ceph15][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph15][DEBUG ] ceph-mds-all stop/waiting
[ceph15][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph15][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph15][DEBUG ] ceph-all stop/waiting
[ceph15][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph15][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph15][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph15][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph15][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph15][WARNING] stop: Unknown instance: 
[ceph15][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph15][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph15][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph15][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph15][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph15][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph15][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph16 ...
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][INFO  ] purging host ... ceph16
[ceph16][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph16][DEBUG ] Reading package lists...
[ceph16][DEBUG ] Building dependency tree...
[ceph16][DEBUG ] Reading state information...
[ceph16][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph16][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph16][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph16][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph16][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph16][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph16][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph16][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph16][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph16][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph16][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph16][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph16][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph16][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph16][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph16][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph16][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph16][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph16][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph16][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph16][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph16][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph16][DEBUG ] The following packages will be REMOVED:
[ceph16][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph16][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 47 not upgraded.
[ceph16][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph16][DEBUG ] (Reading database ... 323373 files and directories currently installed.)
[ceph16][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph16][DEBUG ] ceph-mds-all stop/waiting
[ceph16][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph16][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph16][DEBUG ] ceph-all stop/waiting
[ceph16][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph16][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph16][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph16][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph16][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph16][WARNING] stop: Unknown instance: 
[ceph16][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph16][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph16][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph16][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph16][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph16][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph16][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph17 ...
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][INFO  ] purging host ... ceph17
[ceph17][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[ceph17][DEBUG ] Reading package lists...
[ceph17][DEBUG ] Building dependency tree...
[ceph17][DEBUG ] Reading state information...
[ceph17][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph17][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph17][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph17][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph17][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph17][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph17][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph17][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph17][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph17][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph17][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph17][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph17][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph17][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph17][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph17][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph17][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph17][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph17][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph17][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph17][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph17][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph17][DEBUG ] The following packages will be REMOVED:
[ceph17][DEBUG ]   ceph* ceph-common* ceph-fs-common* ceph-mds* radosgw*
[ceph17][DEBUG ] 0 upgraded, 0 newly installed, 5 to remove and 47 not upgraded.
[ceph17][DEBUG ] After this operation, 127 MB disk space will be freed.
[ceph17][DEBUG ] (Reading database ... 323373 files and directories currently installed.)
[ceph17][DEBUG ] Removing ceph-mds (0.94.5-1trusty) ...
[ceph17][DEBUG ] ceph-mds-all stop/waiting
[ceph17][DEBUG ] Purging configuration files for ceph-mds (0.94.5-1trusty) ...
[ceph17][DEBUG ] Removing ceph (0.94.5-1trusty) ...
[ceph17][DEBUG ] ceph-all stop/waiting
[ceph17][DEBUG ] Purging configuration files for ceph (0.94.5-1trusty) ...
[ceph17][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/tmp' not empty so not removed
[ceph17][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/osd' not empty so not removed
[ceph17][WARNING] dpkg: warning: while removing ceph, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[ceph17][DEBUG ] Removing radosgw (0.94.5-1trusty) ...
[ceph17][WARNING] stop: Unknown instance: 
[ceph17][DEBUG ] Purging configuration files for radosgw (0.94.5-1trusty) ...
[ceph17][WARNING] dpkg: warning: while removing radosgw, directory '/var/lib/ceph' not empty so not removed
[ceph17][DEBUG ] Removing ceph-common (0.94.5-1trusty) ...
[ceph17][DEBUG ] Purging configuration files for ceph-common (0.94.5-1trusty) ...
[ceph17][DEBUG ] Removing ceph-fs-common (0.94.3-1trusty) ...
[ceph17][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph17][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy install ceph2 ceph4 ceph5 ceph6 ceph8 ceph9 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb7353031b8>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7fb735bc8de8>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['ceph2', 'ceph4', 'ceph5', 'ceph6', 'ceph8', 'ceph9', 'ceph10', 'ceph11', 'ceph12', 'ceph15', 'ceph16', 'ceph17']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version hammer on cluster ceph hosts ceph2 ceph4 ceph5 ceph6 ceph8 ceph9 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph2 ...
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][INFO  ] installing Ceph on ceph2
[ceph2][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph2][DEBUG ] Reading package lists...
[ceph2][DEBUG ] Building dependency tree...
[ceph2][DEBUG ] Reading state information...
[ceph2][DEBUG ] ca-certificates is already the newest version.
[ceph2][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph2][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin libaio1 libboost-program-options1.54.0
[ceph2][DEBUG ]   libboost-system1.54.0 libboost-thread1.54.0 libcephfs1 libcryptsetup4
[ceph2][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph2][DEBUG ]   liblttng-ust0 liblzo2-2 libnspr4 libnss3 libnss3-nssdb librados2
[ceph2][DEBUG ]   libradosstriper1 librbd1 libsnappy1 libtcmalloc-minimal4 libunwind8 liburcu1
[ceph2][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph2][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph2][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph2][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph2][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-58-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph2][DEBUG ]   python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
[ceph2][DEBUG ]   python-markupsafe python-pyinotify python-rados python-rbd python-werkzeug
[ceph2][DEBUG ]   xfsprogs
[ceph2][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.
[ceph2][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph2][WARNING] --2015-11-20 16:24:23--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph2][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph2][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph2][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph2][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph2][WARNING] Saving to: ‘release.asc’
[ceph2][WARNING] 
[ceph2][WARNING]      0K .                                                     100% 33.4M=0s
[ceph2][WARNING] 
[ceph2][WARNING] 2015-11-20 16:25:10 (33.4 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph2][WARNING] 
[ceph2][INFO  ] Running command: apt-key add release.asc
[ceph2][DEBUG ] OK
[ceph2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph2][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph2][DEBUG ] Ign http://repo trusty InRelease
[ceph2][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph2][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph2][DEBUG ] Hit http://repo trusty Release.gpg
[ceph2][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph2][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph2][DEBUG ] Hit http://repo trusty Release
[ceph2][DEBUG ] Hit http://repo trusty-updates Release
[ceph2][DEBUG ] Hit http://repo trusty-backports Release
[ceph2][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph2][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph2][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph2][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph2][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph2][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph2][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph2][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph2][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph2][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph2][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph2][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph2][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph2][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph2][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph2][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph2][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph2][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph2][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph2][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph2][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph2][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph2][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph2][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph2][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph2][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph2][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph2][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph2][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph2][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph2][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph2][DEBUG ] Fetched 1,059 kB in 11s (90.1 kB/s)
[ceph2][DEBUG ] Reading package lists...
[ceph2][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph2][DEBUG ] Reading package lists...
[ceph2][DEBUG ] Building dependency tree...
[ceph2][DEBUG ] Reading state information...
[ceph2][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph2][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph2][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph2][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph2][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph2][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph2][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-58-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph2][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph2][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph2][DEBUG ] The following extra packages will be installed:
[ceph2][DEBUG ]   ceph-common librados2 libradosstriper1 librbd1 python-cephfs python-rados
[ceph2][DEBUG ]   python-rbd
[ceph2][DEBUG ] Recommended packages:
[ceph2][DEBUG ]   ceph-fs-common
[ceph2][DEBUG ] The following NEW packages will be installed:
[ceph2][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph2][DEBUG ] The following packages will be upgraded:
[ceph2][DEBUG ]   librados2 libradosstriper1 librbd1 python-cephfs python-rados python-rbd
[ceph2][DEBUG ] 6 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph2][DEBUG ] Need to get 36.2 MB of archives.
[ceph2][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph2][DEBUG ] Get:1 http://ceph.com/debian-hammer/ trusty/main libradosstriper1 amd64 0.94.5-1trusty [2,448 kB]
[ceph2][DEBUG ] Get:2 http://ceph.com/debian-hammer/ trusty/main librbd1 amd64 0.94.5-1trusty [2,420 kB]
[ceph2][DEBUG ] Get:3 http://ceph.com/debian-hammer/ trusty/main librados2 amd64 0.94.5-1trusty [2,313 kB]
[ceph2][DEBUG ] Get:4 http://ceph.com/debian-hammer/ trusty/main python-rados amd64 0.94.5-1trusty [804 kB]
[ceph2][DEBUG ] Get:5 http://ceph.com/debian-hammer/ trusty/main python-cephfs amd64 0.94.5-1trusty [795 kB]
[ceph2][DEBUG ] Get:6 http://ceph.com/debian-hammer/ trusty/main python-rbd amd64 0.94.5-1trusty [800 kB]
[ceph2][DEBUG ] Get:7 http://ceph.com/debian-hammer/ trusty/main ceph-common amd64 0.94.5-1trusty [5,845 kB]
[ceph2][DEBUG ] Get:8 http://ceph.com/debian-hammer/ trusty/main ceph amd64 0.94.5-1trusty [11.1 MB]
[ceph2][DEBUG ] Get:9 http://ceph.com/debian-hammer/ trusty/main ceph-mds amd64 0.94.5-1trusty [7,275 kB]
[ceph2][DEBUG ] Get:10 http://ceph.com/debian-hammer/ trusty/main radosgw amd64 0.94.5-1trusty [2,382 kB]
[ceph2][DEBUG ] Fetched 36.2 MB in 5min 25s (111 kB/s)
[ceph2][DEBUG ] (Reading database ... 293557 files and directories currently installed.)
[ceph2][DEBUG ] Preparing to unpack .../libradosstriper1_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking libradosstriper1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph2][DEBUG ] Preparing to unpack .../librbd1_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking librbd1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph2][DEBUG ] Preparing to unpack .../librados2_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking librados2 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph2][DEBUG ] Preparing to unpack .../python-rados_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking python-rados (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph2][DEBUG ] Preparing to unpack .../python-cephfs_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking python-cephfs (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph2][DEBUG ] Preparing to unpack .../python-rbd_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking python-rbd (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph2][DEBUG ] Selecting previously unselected package ceph-common.
[ceph2][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph2][DEBUG ] Selecting previously unselected package ceph.
[ceph2][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph2][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph2][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph2][DEBUG ] Selecting previously unselected package radosgw.
[ceph2][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph2][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph2][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph2][DEBUG ] Setting up librados2 (0.94.5-1trusty) ...
[ceph2][DEBUG ] Setting up libradosstriper1 (0.94.5-1trusty) ...
[ceph2][DEBUG ] Setting up librbd1 (0.94.5-1trusty) ...
[ceph2][DEBUG ] Setting up python-rados (0.94.5-1trusty) ...
[ceph2][DEBUG ] Setting up python-cephfs (0.94.5-1trusty) ...
[ceph2][DEBUG ] Setting up python-rbd (0.94.5-1trusty) ...
[ceph2][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph2][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph2][DEBUG ] radosgw-all start/running
[ceph2][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph2][DEBUG ] ceph-all start/running
[ceph2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph2][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph2][DEBUG ] ceph-mds-all start/running
[ceph2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph2][INFO  ] Running command: ceph --version
[ceph2][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph4 ...
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][INFO  ] installing Ceph on ceph4
[ceph4][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph4][DEBUG ] Reading package lists...
[ceph4][DEBUG ] Building dependency tree...
[ceph4][DEBUG ] Reading state information...
[ceph4][DEBUG ] ca-certificates is already the newest version.
[ceph4][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph4][DEBUG ]   cryptsetup-bin libaio1 libboost-program-options1.54.0 libboost-system1.54.0
[ceph4][DEBUG ]   libboost-thread1.54.0 libcephfs1 libcryptsetup4 libfcgi0ldbl
[ceph4][DEBUG ]   libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph4][DEBUG ]   liblttng-ust0 libnspr4 libnss3 libnss3-nssdb librados2 librbd1 libsnappy1
[ceph4][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-58
[ceph4][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph4][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph4][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph4][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph4][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph4][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph4][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-58-generic
[ceph4][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph4][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph4][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-67-generic python-cephfs python-flask
[ceph4][DEBUG ]   python-itsdangerous python-jinja2 python-markupsafe python-rados python-rbd
[ceph4][DEBUG ]   python-werkzeug xfsprogs
[ceph4][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph4][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.
[ceph4][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph4][WARNING] --2015-11-20 16:31:40--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph4][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph4][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph4][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph4][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph4][WARNING] Saving to: ‘release.asc’
[ceph4][WARNING] 
[ceph4][WARNING]      0K .                                                     100% 33.9M=0s
[ceph4][WARNING] 
[ceph4][WARNING] 2015-11-20 16:31:57 (33.9 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph4][WARNING] 
[ceph4][INFO  ] Running command: apt-key add release.asc
[ceph4][DEBUG ] OK
[ceph4][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph4][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph4][DEBUG ] Ign http://repo trusty InRelease
[ceph4][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph4][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph4][DEBUG ] Hit http://repo trusty Release.gpg
[ceph4][DEBUG ] Get:1 http://repo trusty-updates Release.gpg [933 B]
[ceph4][DEBUG ] Get:2 http://repo trusty-backports Release.gpg [933 B]
[ceph4][DEBUG ] Hit http://repo trusty Release
[ceph4][DEBUG ] Get:3 http://repo trusty-updates Release [63.5 kB]
[ceph4][DEBUG ] Get:4 http://repo trusty-backports Release [63.5 kB]
[ceph4][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph4][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph4][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph4][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph4][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph4][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph4][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph4][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph4][DEBUG ] Get:5 http://repo trusty-updates/main amd64 Packages [653 kB]
[ceph4][DEBUG ] Get:6 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph4][DEBUG ] Get:7 http://repo trusty-updates/restricted amd64 Packages [15.9 kB]
[ceph4][DEBUG ] Get:8 http://repo trusty-updates/universe amd64 Packages [327 kB]
[ceph4][DEBUG ] Get:9 http://repo trusty-updates/multiverse amd64 Packages [13.0 kB]
[ceph4][DEBUG ] Get:10 http://repo trusty-updates/main i386 Packages [632 kB]
[ceph4][DEBUG ] Get:11 http://repo trusty-updates/restricted i386 Packages [15.6 kB]
[ceph4][DEBUG ] Get:12 http://repo trusty-updates/universe i386 Packages [328 kB]
[ceph4][DEBUG ] Get:13 http://repo trusty-updates/multiverse i386 Packages [13.1 kB]
[ceph4][DEBUG ] Get:14 http://repo trusty-backports/main amd64 Packages [9,211 B]
[ceph4][DEBUG ] Get:15 http://repo trusty-backports/restricted amd64 Packages [28 B]
[ceph4][DEBUG ] Get:16 http://repo trusty-backports/universe amd64 Packages [38.5 kB]
[ceph4][DEBUG ] Get:17 http://repo trusty-backports/multiverse amd64 Packages [1,571 B]
[ceph4][DEBUG ] Get:18 http://repo trusty-backports/main i386 Packages [9,206 B]
[ceph4][DEBUG ] Get:19 http://repo trusty-backports/restricted i386 Packages [28 B]
[ceph4][DEBUG ] Get:20 http://repo trusty-backports/universe i386 Packages [38.5 kB]
[ceph4][DEBUG ] Get:21 http://repo trusty-backports/multiverse i386 Packages [1,552 B]
[ceph4][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph4][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph4][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph4][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph4][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph4][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph4][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph4][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph4][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph4][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph4][DEBUG ] Get:22 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph4][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph4][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph4][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph4][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph4][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph4][DEBUG ] Get:23 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph4][DEBUG ] Get:24 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph4][DEBUG ] Get:25 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph4][DEBUG ] Get:26 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph4][DEBUG ] Get:27 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph4][DEBUG ] Get:28 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph4][DEBUG ] Get:29 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph4][DEBUG ] Get:30 http://security.ubuntu.com trusty-security/main Translation-en [201 kB]
[ceph4][DEBUG ] Get:31 http://security.ubuntu.com trusty-security/multiverse Translation-en [2,335 B]
[ceph4][DEBUG ] Get:32 http://security.ubuntu.com trusty-security/restricted Translation-en [3,206 B]
[ceph4][DEBUG ] Get:33 http://security.ubuntu.com trusty-security/universe Translation-en [69.7 kB]
[ceph4][DEBUG ] Fetched 3,560 kB in 14s (251 kB/s)
[ceph4][DEBUG ] Reading package lists...
[ceph4][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph4][DEBUG ] Reading package lists...
[ceph4][DEBUG ] Building dependency tree...
[ceph4][DEBUG ] Reading state information...
[ceph4][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph4][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph4][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph4][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph4][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph4][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph4][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph4][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph4][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph4][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph4][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph4][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-58-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph4][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph4][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph4][DEBUG ] The following extra packages will be installed:
[ceph4][DEBUG ]   ceph-common librados2 librbd1 python-cephfs python-rados python-rbd
[ceph4][DEBUG ] Recommended packages:
[ceph4][DEBUG ]   btrfs-tools libradosstriper1 ceph-fs-common ceph-fuse
[ceph4][DEBUG ] The following NEW packages will be installed:
[ceph4][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph4][DEBUG ] The following packages will be upgraded:
[ceph4][DEBUG ]   librados2 librbd1 python-cephfs python-rados python-rbd
[ceph4][DEBUG ] 5 upgraded, 4 newly installed, 0 to remove and 49 not upgraded.
[ceph4][DEBUG ] Need to get 33.8 MB of archives.
[ceph4][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph4][DEBUG ] Get:1 http://ceph.com/debian-hammer/ trusty/main librbd1 amd64 0.94.5-1trusty [2,420 kB]
[ceph4][DEBUG ] Get:2 http://ceph.com/debian-hammer/ trusty/main librados2 amd64 0.94.5-1trusty [2,313 kB]
[ceph4][DEBUG ] Get:3 http://ceph.com/debian-hammer/ trusty/main python-rados amd64 0.94.5-1trusty [804 kB]
[ceph4][DEBUG ] Get:4 http://ceph.com/debian-hammer/ trusty/main python-cephfs amd64 0.94.5-1trusty [795 kB]
[ceph4][DEBUG ] Get:5 http://ceph.com/debian-hammer/ trusty/main python-rbd amd64 0.94.5-1trusty [800 kB]
[ceph4][DEBUG ] Get:6 http://ceph.com/debian-hammer/ trusty/main ceph-common amd64 0.94.5-1trusty [5,845 kB]
[ceph4][DEBUG ] Get:7 http://ceph.com/debian-hammer/ trusty/main ceph amd64 0.94.5-1trusty [11.1 MB]
[ceph4][DEBUG ] Get:8 http://ceph.com/debian-hammer/ trusty/main ceph-mds amd64 0.94.5-1trusty [7,275 kB]
[ceph4][DEBUG ] Get:9 http://ceph.com/debian-hammer/ trusty/main radosgw amd64 0.94.5-1trusty [2,382 kB]
[ceph4][DEBUG ] Fetched 33.8 MB in 5min 17s (106 kB/s)
[ceph4][DEBUG ] (Reading database ... 293357 files and directories currently installed.)
[ceph4][DEBUG ] Preparing to unpack .../librbd1_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking librbd1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph4][DEBUG ] Preparing to unpack .../librados2_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking librados2 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph4][DEBUG ] Preparing to unpack .../python-rados_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking python-rados (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph4][DEBUG ] Preparing to unpack .../python-cephfs_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking python-cephfs (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph4][DEBUG ] Preparing to unpack .../python-rbd_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking python-rbd (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph4][DEBUG ] Selecting previously unselected package ceph-common.
[ceph4][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph4][DEBUG ] Selecting previously unselected package ceph.
[ceph4][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph4][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph4][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph4][DEBUG ] Selecting previously unselected package radosgw.
[ceph4][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph4][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph4][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph4][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph4][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph4][DEBUG ] Setting up librados2 (0.94.5-1trusty) ...
[ceph4][DEBUG ] Setting up librbd1 (0.94.5-1trusty) ...
[ceph4][DEBUG ] Setting up python-rados (0.94.5-1trusty) ...
[ceph4][DEBUG ] Setting up python-cephfs (0.94.5-1trusty) ...
[ceph4][DEBUG ] Setting up python-rbd (0.94.5-1trusty) ...
[ceph4][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph4][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph4][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph4][DEBUG ] radosgw-all start/running
[ceph4][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph4][DEBUG ] ceph-all start/running
[ceph4][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph4][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph4][DEBUG ] ceph-mds-all start/running
[ceph4][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph4][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph4][INFO  ] Running command: ceph --version
[ceph4][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph5 ...
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][INFO  ] installing Ceph on ceph5
[ceph5][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph5][DEBUG ] Reading package lists...
[ceph5][DEBUG ] Building dependency tree...
[ceph5][DEBUG ] Reading state information...
[ceph5][DEBUG ] ca-certificates is already the newest version.
[ceph5][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph5][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin libaio1 libboost-program-options1.54.0
[ceph5][DEBUG ]   libboost-system1.54.0 libboost-thread1.54.0 libcephfs1 libcryptsetup4
[ceph5][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph5][DEBUG ]   liblttng-ust0 liblzo2-2 libnspr4 libnss3 libnss3-nssdb librados2
[ceph5][DEBUG ]   libradosstriper1 librbd1 libsnappy1 libtcmalloc-minimal4 libunwind8 liburcu1
[ceph5][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph5][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph5][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph5][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph5][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph5][DEBUG ]   python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
[ceph5][DEBUG ]   python-markupsafe python-pyinotify python-rados python-rbd python-werkzeug
[ceph5][DEBUG ]   xfsprogs
[ceph5][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph5][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.
[ceph5][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph5][WARNING] --2015-11-20 16:38:16--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph5][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph5][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph5][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph5][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph5][WARNING] Saving to: ‘release.asc’
[ceph5][WARNING] 
[ceph5][WARNING]      0K .                                                     100% 39.5M=0s
[ceph5][WARNING] 
[ceph5][WARNING] 2015-11-20 16:38:26 (39.5 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph5][WARNING] 
[ceph5][INFO  ] Running command: apt-key add release.asc
[ceph5][DEBUG ] OK
[ceph5][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph5][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph5][DEBUG ] Ign http://repo trusty InRelease
[ceph5][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph5][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph5][DEBUG ] Hit http://repo trusty Release.gpg
[ceph5][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph5][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph5][DEBUG ] Hit http://repo trusty Release
[ceph5][DEBUG ] Hit http://repo trusty-updates Release
[ceph5][DEBUG ] Hit http://repo trusty-backports Release
[ceph5][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph5][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph5][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph5][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph5][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph5][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph5][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph5][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph5][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph5][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph5][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph5][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph5][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph5][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph5][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph5][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph5][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph5][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph5][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph5][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph5][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph5][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph5][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph5][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph5][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph5][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph5][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph5][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph5][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph5][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph5][DEBUG ] Fetched 1,059 kB in 2s (355 kB/s)
[ceph5][DEBUG ] Reading package lists...
[ceph5][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph5][DEBUG ] Reading package lists...
[ceph5][DEBUG ] Building dependency tree...
[ceph5][DEBUG ] Reading state information...
[ceph5][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph5][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph5][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph5][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph5][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph5][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph5][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph5][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph5][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph5][DEBUG ] The following extra packages will be installed:
[ceph5][DEBUG ]   ceph-common librados2 libradosstriper1 librbd1 python-cephfs python-rados
[ceph5][DEBUG ]   python-rbd
[ceph5][DEBUG ] Recommended packages:
[ceph5][DEBUG ]   ceph-fs-common
[ceph5][DEBUG ] The following NEW packages will be installed:
[ceph5][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph5][DEBUG ] The following packages will be upgraded:
[ceph5][DEBUG ]   librados2 libradosstriper1 librbd1 python-cephfs python-rados python-rbd
[ceph5][DEBUG ] 6 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph5][DEBUG ] Need to get 36.2 MB of archives.
[ceph5][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph5][DEBUG ] Get:1 http://ceph.com/debian-hammer/ trusty/main libradosstriper1 amd64 0.94.5-1trusty [2,448 kB]
[ceph5][DEBUG ] Get:2 http://ceph.com/debian-hammer/ trusty/main librbd1 amd64 0.94.5-1trusty [2,420 kB]
[ceph5][DEBUG ] Get:3 http://ceph.com/debian-hammer/ trusty/main librados2 amd64 0.94.5-1trusty [2,313 kB]
[ceph5][DEBUG ] Get:4 http://ceph.com/debian-hammer/ trusty/main python-rados amd64 0.94.5-1trusty [804 kB]
[ceph5][DEBUG ] Get:5 http://ceph.com/debian-hammer/ trusty/main python-cephfs amd64 0.94.5-1trusty [795 kB]
[ceph5][DEBUG ] Get:6 http://ceph.com/debian-hammer/ trusty/main python-rbd amd64 0.94.5-1trusty [800 kB]
[ceph5][DEBUG ] Get:7 http://ceph.com/debian-hammer/ trusty/main ceph-common amd64 0.94.5-1trusty [5,845 kB]
[ceph5][DEBUG ] Get:8 http://ceph.com/debian-hammer/ trusty/main ceph amd64 0.94.5-1trusty [11.1 MB]
[ceph5][DEBUG ] Get:9 http://ceph.com/debian-hammer/ trusty/main ceph-mds amd64 0.94.5-1trusty [7,275 kB]
[ceph5][DEBUG ] Get:10 http://ceph.com/debian-hammer/ trusty/main radosgw amd64 0.94.5-1trusty [2,382 kB]
[ceph5][DEBUG ] Fetched 36.2 MB in 5s (6,256 kB/s)
[ceph5][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph5][DEBUG ] Preparing to unpack .../libradosstriper1_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking libradosstriper1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph5][DEBUG ] Preparing to unpack .../librbd1_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking librbd1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph5][DEBUG ] Preparing to unpack .../librados2_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking librados2 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph5][DEBUG ] Preparing to unpack .../python-rados_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking python-rados (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph5][DEBUG ] Preparing to unpack .../python-cephfs_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking python-cephfs (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph5][DEBUG ] Preparing to unpack .../python-rbd_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking python-rbd (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph5][DEBUG ] Selecting previously unselected package ceph-common.
[ceph5][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph5][DEBUG ] Selecting previously unselected package ceph.
[ceph5][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph5][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph5][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph5][DEBUG ] Selecting previously unselected package radosgw.
[ceph5][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph5][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph5][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph5][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph5][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph5][DEBUG ] Setting up librados2 (0.94.5-1trusty) ...
[ceph5][DEBUG ] Setting up libradosstriper1 (0.94.5-1trusty) ...
[ceph5][DEBUG ] Setting up librbd1 (0.94.5-1trusty) ...
[ceph5][DEBUG ] Setting up python-rados (0.94.5-1trusty) ...
[ceph5][DEBUG ] Setting up python-cephfs (0.94.5-1trusty) ...
[ceph5][DEBUG ] Setting up python-rbd (0.94.5-1trusty) ...
[ceph5][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph5][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph5][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph5][DEBUG ] radosgw-all start/running
[ceph5][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph5][DEBUG ] ceph-all start/running
[ceph5][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph5][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph5][DEBUG ] ceph-mds-all start/running
[ceph5][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph5][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph5][INFO  ] Running command: ceph --version
[ceph5][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph6 ...
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][INFO  ] installing Ceph on ceph6
[ceph6][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph6][DEBUG ] Reading package lists...
[ceph6][DEBUG ] Building dependency tree...
[ceph6][DEBUG ] Reading state information...
[ceph6][DEBUG ] ca-certificates is already the newest version.
[ceph6][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph6][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin libaio1 libboost-program-options1.54.0
[ceph6][DEBUG ]   libboost-system1.54.0 libboost-thread1.54.0 libcephfs1 libcryptsetup4
[ceph6][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 liblttng-ust-ctl2
[ceph6][DEBUG ]   liblttng-ust0 liblzo2-2 libnspr4 libnss3 libnss3-nssdb librados2
[ceph6][DEBUG ]   libradosstriper1 librbd1 libsnappy1 libtcmalloc-minimal4 libunwind8 liburcu1
[ceph6][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph6][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph6][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph6][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph6][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph6][DEBUG ]   python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
[ceph6][DEBUG ]   python-markupsafe python-pyinotify python-rados python-rbd python-werkzeug
[ceph6][DEBUG ]   xfsprogs
[ceph6][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph6][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.
[ceph6][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph6][WARNING] --2015-11-20 16:39:30--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph6][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph6][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph6][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph6][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph6][WARNING] Saving to: ‘release.asc’
[ceph6][WARNING] 
[ceph6][WARNING]      0K .                                                     100% 41.4M=0s
[ceph6][WARNING] 
[ceph6][WARNING] 2015-11-20 16:39:32 (41.4 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph6][WARNING] 
[ceph6][INFO  ] Running command: apt-key add release.asc
[ceph6][DEBUG ] OK
[ceph6][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph6][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph6][DEBUG ] Ign http://repo trusty InRelease
[ceph6][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph6][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph6][DEBUG ] Hit http://repo trusty Release.gpg
[ceph6][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph6][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph6][DEBUG ] Hit http://repo trusty Release
[ceph6][DEBUG ] Hit http://repo trusty-updates Release
[ceph6][DEBUG ] Hit http://repo trusty-backports Release
[ceph6][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph6][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph6][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph6][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph6][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph6][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph6][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph6][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph6][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph6][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph6][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph6][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph6][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph6][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph6][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph6][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph6][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph6][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph6][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph6][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph6][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph6][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph6][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph6][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph6][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph6][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph6][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph6][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph6][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph6][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph6][DEBUG ] Fetched 1,059 kB in 10s (105 kB/s)
[ceph6][DEBUG ] Reading package lists...
[ceph6][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph6][DEBUG ] Reading package lists...
[ceph6][DEBUG ] Building dependency tree...
[ceph6][DEBUG ] Reading state information...
[ceph6][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph6][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph6][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph6][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph6][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph6][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph6][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph6][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph6][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph6][DEBUG ] The following extra packages will be installed:
[ceph6][DEBUG ]   ceph-common librados2 libradosstriper1 librbd1 python-cephfs python-rados
[ceph6][DEBUG ]   python-rbd
[ceph6][DEBUG ] Recommended packages:
[ceph6][DEBUG ]   ceph-fs-common
[ceph6][DEBUG ] The following NEW packages will be installed:
[ceph6][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph6][DEBUG ] The following packages will be upgraded:
[ceph6][DEBUG ]   librados2 libradosstriper1 librbd1 python-cephfs python-rados python-rbd
[ceph6][DEBUG ] 6 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph6][DEBUG ] Need to get 36.2 MB of archives.
[ceph6][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph6][DEBUG ] Get:1 http://ceph.com/debian-hammer/ trusty/main libradosstriper1 amd64 0.94.5-1trusty [2,448 kB]
[ceph6][DEBUG ] Get:2 http://ceph.com/debian-hammer/ trusty/main librbd1 amd64 0.94.5-1trusty [2,420 kB]
[ceph6][DEBUG ] Get:3 http://ceph.com/debian-hammer/ trusty/main librados2 amd64 0.94.5-1trusty [2,313 kB]
[ceph6][DEBUG ] Get:4 http://ceph.com/debian-hammer/ trusty/main python-rados amd64 0.94.5-1trusty [804 kB]
[ceph6][DEBUG ] Get:5 http://ceph.com/debian-hammer/ trusty/main python-cephfs amd64 0.94.5-1trusty [795 kB]
[ceph6][DEBUG ] Get:6 http://ceph.com/debian-hammer/ trusty/main python-rbd amd64 0.94.5-1trusty [800 kB]
[ceph6][DEBUG ] Get:7 http://ceph.com/debian-hammer/ trusty/main ceph-common amd64 0.94.5-1trusty [5,845 kB]
[ceph6][DEBUG ] Get:8 http://ceph.com/debian-hammer/ trusty/main ceph amd64 0.94.5-1trusty [11.1 MB]
[ceph6][DEBUG ] Get:9 http://ceph.com/debian-hammer/ trusty/main ceph-mds amd64 0.94.5-1trusty [7,275 kB]
[ceph6][DEBUG ] Get:10 http://ceph.com/debian-hammer/ trusty/main radosgw amd64 0.94.5-1trusty [2,382 kB]
[ceph6][DEBUG ] Fetched 36.2 MB in 6s (5,757 kB/s)
[ceph6][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph6][DEBUG ] Preparing to unpack .../libradosstriper1_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking libradosstriper1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph6][DEBUG ] Preparing to unpack .../librbd1_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking librbd1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph6][DEBUG ] Preparing to unpack .../librados2_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking librados2 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph6][DEBUG ] Preparing to unpack .../python-rados_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking python-rados (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph6][DEBUG ] Preparing to unpack .../python-cephfs_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking python-cephfs (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph6][DEBUG ] Preparing to unpack .../python-rbd_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking python-rbd (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph6][DEBUG ] Selecting previously unselected package ceph-common.
[ceph6][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph6][DEBUG ] Selecting previously unselected package ceph.
[ceph6][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph6][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph6][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph6][DEBUG ] Selecting previously unselected package radosgw.
[ceph6][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph6][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph6][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph6][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph6][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph6][DEBUG ] Setting up librados2 (0.94.5-1trusty) ...
[ceph6][DEBUG ] Setting up libradosstriper1 (0.94.5-1trusty) ...
[ceph6][DEBUG ] Setting up librbd1 (0.94.5-1trusty) ...
[ceph6][DEBUG ] Setting up python-rados (0.94.5-1trusty) ...
[ceph6][DEBUG ] Setting up python-cephfs (0.94.5-1trusty) ...
[ceph6][DEBUG ] Setting up python-rbd (0.94.5-1trusty) ...
[ceph6][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph6][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph6][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph6][DEBUG ] radosgw-all start/running
[ceph6][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph6][DEBUG ] ceph-all start/running
[ceph6][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph6][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph6][DEBUG ] ceph-mds-all start/running
[ceph6][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph6][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph6][INFO  ] Running command: ceph --version
[ceph6][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph8 ...
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][INFO  ] installing Ceph on ceph8
[ceph8][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph8][DEBUG ] Reading package lists...
[ceph8][DEBUG ] Building dependency tree...
[ceph8][DEBUG ] Reading state information...
[ceph8][DEBUG ] ca-certificates is already the newest version.
[ceph8][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph8][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph8][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph8][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph8][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph8][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph8][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph8][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph8][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph8][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph8][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph8][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph8][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph8][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph8][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph8][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph8][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph8][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph8][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph8][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph8][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph8][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph8][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.
[ceph8][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph8][WARNING] --2015-11-20 16:40:38--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph8][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph8][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph8][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph8][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph8][WARNING] Saving to: ‘release.asc’
[ceph8][WARNING] 
[ceph8][WARNING]      0K .                                                     100% 41.4M=0s
[ceph8][WARNING] 
[ceph8][WARNING] 2015-11-20 16:41:31 (41.4 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph8][WARNING] 
[ceph8][INFO  ] Running command: apt-key add release.asc
[ceph8][DEBUG ] OK
[ceph8][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph8][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph8][DEBUG ] Ign http://repo trusty InRelease
[ceph8][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph8][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph8][DEBUG ] Hit http://repo trusty Release.gpg
[ceph8][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph8][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph8][DEBUG ] Hit http://repo trusty Release
[ceph8][DEBUG ] Hit http://repo trusty-updates Release
[ceph8][DEBUG ] Hit http://repo trusty-backports Release
[ceph8][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph8][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph8][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph8][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph8][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph8][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph8][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph8][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph8][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph8][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph8][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph8][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph8][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph8][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph8][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph8][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph8][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph8][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph8][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph8][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph8][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph8][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph8][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph8][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph8][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph8][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph8][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph8][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph8][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph8][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph8][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph8][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph8][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph8][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph8][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph8][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph8][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph8][DEBUG ] Fetched 1,059 kB in 6s (157 kB/s)
[ceph8][DEBUG ] Reading package lists...
[ceph8][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph8][DEBUG ] Reading package lists...
[ceph8][DEBUG ] Building dependency tree...
[ceph8][DEBUG ] Reading state information...
[ceph8][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph8][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph8][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph8][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph8][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph8][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph8][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph8][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph8][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph8][DEBUG ] The following extra packages will be installed:
[ceph8][DEBUG ]   ceph-common librados2 libradosstriper1 librbd1 python-cephfs python-rados
[ceph8][DEBUG ]   python-rbd
[ceph8][DEBUG ] Recommended packages:
[ceph8][DEBUG ]   ceph-fs-common
[ceph8][DEBUG ] The following NEW packages will be installed:
[ceph8][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph8][DEBUG ] The following packages will be upgraded:
[ceph8][DEBUG ]   librados2 libradosstriper1 librbd1 python-cephfs python-rados python-rbd
[ceph8][DEBUG ] 6 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph8][DEBUG ] Need to get 36.2 MB of archives.
[ceph8][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph8][DEBUG ] Get:1 http://ceph.com/debian-hammer/ trusty/main libradosstriper1 amd64 0.94.5-1trusty [2,448 kB]
[ceph8][DEBUG ] Get:2 http://ceph.com/debian-hammer/ trusty/main librbd1 amd64 0.94.5-1trusty [2,420 kB]
[ceph8][DEBUG ] Get:3 http://ceph.com/debian-hammer/ trusty/main librados2 amd64 0.94.5-1trusty [2,313 kB]
[ceph8][DEBUG ] Get:4 http://ceph.com/debian-hammer/ trusty/main python-rados amd64 0.94.5-1trusty [804 kB]
[ceph8][DEBUG ] Get:5 http://ceph.com/debian-hammer/ trusty/main python-cephfs amd64 0.94.5-1trusty [795 kB]
[ceph8][DEBUG ] Get:6 http://ceph.com/debian-hammer/ trusty/main python-rbd amd64 0.94.5-1trusty [800 kB]
[ceph8][DEBUG ] Get:7 http://ceph.com/debian-hammer/ trusty/main ceph-common amd64 0.94.5-1trusty [5,845 kB]
[ceph8][DEBUG ] Get:8 http://ceph.com/debian-hammer/ trusty/main ceph amd64 0.94.5-1trusty [11.1 MB]
[ceph8][DEBUG ] Get:9 http://ceph.com/debian-hammer/ trusty/main ceph-mds amd64 0.94.5-1trusty [7,275 kB]
[ceph8][DEBUG ] Get:10 http://ceph.com/debian-hammer/ trusty/main radosgw amd64 0.94.5-1trusty [2,382 kB]
[ceph8][DEBUG ] Fetched 36.2 MB in 5min 35s (108 kB/s)
[ceph8][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph8][DEBUG ] Preparing to unpack .../libradosstriper1_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking libradosstriper1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph8][DEBUG ] Preparing to unpack .../librbd1_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking librbd1 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph8][DEBUG ] Preparing to unpack .../librados2_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking librados2 (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph8][DEBUG ] Preparing to unpack .../python-rados_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking python-rados (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph8][DEBUG ] Preparing to unpack .../python-cephfs_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking python-cephfs (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph8][DEBUG ] Preparing to unpack .../python-rbd_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking python-rbd (0.94.5-1trusty) over (0.94.3-1trusty) ...
[ceph8][DEBUG ] Selecting previously unselected package ceph-common.
[ceph8][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph8][DEBUG ] Selecting previously unselected package ceph.
[ceph8][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph8][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph8][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph8][DEBUG ] Selecting previously unselected package radosgw.
[ceph8][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph8][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph8][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph8][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph8][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph8][DEBUG ] Setting up librados2 (0.94.5-1trusty) ...
[ceph8][DEBUG ] Setting up libradosstriper1 (0.94.5-1trusty) ...
[ceph8][DEBUG ] Setting up librbd1 (0.94.5-1trusty) ...
[ceph8][DEBUG ] Setting up python-rados (0.94.5-1trusty) ...
[ceph8][DEBUG ] Setting up python-cephfs (0.94.5-1trusty) ...
[ceph8][DEBUG ] Setting up python-rbd (0.94.5-1trusty) ...
[ceph8][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph8][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph8][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph8][DEBUG ] radosgw-all start/running
[ceph8][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph8][DEBUG ] ceph-all start/running
[ceph8][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph8][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph8][DEBUG ] ceph-mds-all start/running
[ceph8][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph8][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph8][INFO  ] Running command: ceph --version
[ceph8][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph9 ...
[ceph9][DEBUG ] connected to host: ceph9 
[ceph9][DEBUG ] detect platform information from remote host
[ceph9][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph9][INFO  ] installing Ceph on ceph9
[ceph9][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph9][DEBUG ] Reading package lists...
[ceph9][DEBUG ] Building dependency tree...
[ceph9][DEBUG ] Reading state information...
[ceph9][DEBUG ] ca-certificates is already the newest version.
[ceph9][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph9][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph9][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph9][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph9][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph9][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph9][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph9][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph9][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph9][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph9][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-65
[ceph9][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph9][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph9][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph9][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph9][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-65-generic
[ceph9][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph9][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph9][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph9][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph9][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.
[ceph9][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph9][WARNING] --2015-11-20 16:48:01--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph9][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph9][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph9][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph9][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph9][WARNING] Saving to: ‘release.asc’
[ceph9][WARNING] 
[ceph9][WARNING]      0K .                                                     100% 42.0M=0s
[ceph9][WARNING] 
[ceph9][WARNING] 2015-11-20 16:48:07 (42.0 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph9][WARNING] 
[ceph9][INFO  ] Running command: apt-key add release.asc
[ceph9][DEBUG ] OK
[ceph9][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph9][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph9][DEBUG ] Ign http://repo trusty InRelease
[ceph9][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph9][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph9][DEBUG ] Hit http://repo trusty Release.gpg
[ceph9][DEBUG ] Get:1 http://repo trusty-updates Release.gpg [933 B]
[ceph9][DEBUG ] Get:2 http://repo trusty-backports Release.gpg [933 B]
[ceph9][DEBUG ] Hit http://repo trusty Release
[ceph9][DEBUG ] Get:3 http://repo trusty-updates Release [63.5 kB]
[ceph9][DEBUG ] Get:4 http://repo trusty-backports Release [63.5 kB]
[ceph9][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph9][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph9][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph9][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph9][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph9][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph9][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph9][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph9][DEBUG ] Get:5 http://repo trusty-updates/main amd64 Packages [653 kB]
[ceph9][DEBUG ] Get:6 http://repo trusty-updates/restricted amd64 Packages [15.9 kB]
[ceph9][DEBUG ] Get:7 http://repo trusty-updates/universe amd64 Packages [327 kB]
[ceph9][DEBUG ] Get:8 http://repo trusty-updates/multiverse amd64 Packages [13.0 kB]
[ceph9][DEBUG ] Get:9 http://repo trusty-updates/main i386 Packages [632 kB]
[ceph9][DEBUG ] Get:10 http://repo trusty-updates/restricted i386 Packages [15.6 kB]
[ceph9][DEBUG ] Get:11 http://repo trusty-updates/universe i386 Packages [328 kB]
[ceph9][DEBUG ] Get:12 http://repo trusty-updates/multiverse i386 Packages [13.1 kB]
[ceph9][DEBUG ] Get:13 http://repo trusty-backports/main amd64 Packages [9,211 B]
[ceph9][DEBUG ] Get:14 http://repo trusty-backports/restricted amd64 Packages [28 B]
[ceph9][DEBUG ] Get:15 http://repo trusty-backports/universe amd64 Packages [38.5 kB]
[ceph9][DEBUG ] Get:16 http://repo trusty-backports/multiverse amd64 Packages [1,571 B]
[ceph9][DEBUG ] Get:17 http://repo trusty-backports/main i386 Packages [9,206 B]
[ceph9][DEBUG ] Get:18 http://repo trusty-backports/restricted i386 Packages [28 B]
[ceph9][DEBUG ] Get:19 http://repo trusty-backports/universe i386 Packages [38.5 kB]
[ceph9][DEBUG ] Get:20 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph9][DEBUG ] Get:21 http://repo trusty-backports/multiverse i386 Packages [1,552 B]
[ceph9][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph9][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph9][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph9][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph9][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph9][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph9][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph9][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph9][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph9][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph9][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph9][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph9][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph9][DEBUG ] Get:22 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph9][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph9][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph9][DEBUG ] Get:23 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph9][DEBUG ] Get:24 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph9][DEBUG ] Get:25 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph9][DEBUG ] Get:26 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph9][DEBUG ] Get:27 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph9][DEBUG ] Get:28 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph9][DEBUG ] Get:29 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph9][DEBUG ] Get:30 http://security.ubuntu.com trusty-security/main Translation-en [201 kB]
[ceph9][DEBUG ] Get:31 http://security.ubuntu.com trusty-security/multiverse Translation-en [2,335 B]
[ceph9][DEBUG ] Get:32 http://security.ubuntu.com trusty-security/restricted Translation-en [3,206 B]
[ceph9][DEBUG ] Get:33 http://security.ubuntu.com trusty-security/universe Translation-en [69.7 kB]
[ceph9][DEBUG ] Fetched 3,560 kB in 12s (279 kB/s)
[ceph9][DEBUG ] Reading package lists...
[ceph9][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph9][DEBUG ] Reading package lists...
[ceph9][DEBUG ] Building dependency tree...
[ceph9][DEBUG ] Reading state information...
[ceph9][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph9][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph9][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph9][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph9][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph9][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph9][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph9][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph9][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph9][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph9][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph9][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-65-generic
[ceph9][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph9][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph9][DEBUG ] The following extra packages will be installed:
[ceph9][DEBUG ]   ceph-common
[ceph9][DEBUG ] Recommended packages:
[ceph9][DEBUG ]   ceph-fs-common
[ceph9][DEBUG ] The following NEW packages will be installed:
[ceph9][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph9][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 66 not upgraded.
[ceph9][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph9][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph9][DEBUG ] Selecting previously unselected package ceph-common.
[ceph9][DEBUG ] (Reading database ... 293476 files and directories currently installed.)
[ceph9][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph9][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph9][DEBUG ] Selecting previously unselected package ceph.
[ceph9][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph9][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph9][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph9][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph9][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph9][DEBUG ] Selecting previously unselected package radosgw.
[ceph9][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph9][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph9][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph9][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph9][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph9][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph9][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph9][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph9][DEBUG ] radosgw-all start/running
[ceph9][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph9][DEBUG ] ceph-all start/running
[ceph9][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph9][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph9][DEBUG ] ceph-mds-all start/running
[ceph9][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph9][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph9][INFO  ] Running command: ceph --version
[ceph9][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph10 ...
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][INFO  ] installing Ceph on ceph10
[ceph10][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph10][DEBUG ] Reading package lists...
[ceph10][DEBUG ] Building dependency tree...
[ceph10][DEBUG ] Reading state information...
[ceph10][DEBUG ] ca-certificates is already the newest version.
[ceph10][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph10][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph10][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph10][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph10][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph10][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph10][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph10][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph10][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph10][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph10][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph10][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph10][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph10][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph10][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph10][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph10][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph10][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph10][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph10][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph10][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph10][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph10][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
[ceph10][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph10][WARNING] --2015-11-20 16:49:07--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph10][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph10][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph10][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph10][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph10][WARNING] Saving to: ‘release.asc’
[ceph10][WARNING] 
[ceph10][WARNING]      0K .                                                     100% 24.0M=0s
[ceph10][WARNING] 
[ceph10][WARNING] 2015-11-20 16:49:10 (24.0 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph10][WARNING] 
[ceph10][INFO  ] Running command: apt-key add release.asc
[ceph10][DEBUG ] OK
[ceph10][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph10][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph10][DEBUG ] Ign http://repo trusty InRelease
[ceph10][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph10][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph10][DEBUG ] Hit http://repo trusty Release.gpg
[ceph10][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph10][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph10][DEBUG ] Hit http://repo trusty Release
[ceph10][DEBUG ] Hit http://repo trusty-updates Release
[ceph10][DEBUG ] Hit http://repo trusty-backports Release
[ceph10][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph10][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph10][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph10][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph10][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph10][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph10][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph10][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph10][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph10][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph10][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph10][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph10][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph10][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph10][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph10][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph10][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph10][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph10][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph10][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph10][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph10][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph10][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph10][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph10][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph10][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph10][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph10][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph10][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph10][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph10][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph10][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph10][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph10][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph10][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph10][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph10][DEBUG ] Fetched 1,059 kB in 9s (110 kB/s)
[ceph10][DEBUG ] Reading package lists...
[ceph10][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph10][DEBUG ] Reading package lists...
[ceph10][DEBUG ] Building dependency tree...
[ceph10][DEBUG ] Reading state information...
[ceph10][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph10][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph10][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph10][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph10][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph10][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph10][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph10][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph10][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph10][DEBUG ] The following extra packages will be installed:
[ceph10][DEBUG ]   ceph-common
[ceph10][DEBUG ] Recommended packages:
[ceph10][DEBUG ]   ceph-fs-common
[ceph10][DEBUG ] The following NEW packages will be installed:
[ceph10][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph10][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph10][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph10][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph10][DEBUG ] Selecting previously unselected package ceph-common.
[ceph10][DEBUG ] (Reading database ... 323197 files and directories currently installed.)
[ceph10][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph10][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph10][DEBUG ] Selecting previously unselected package ceph.
[ceph10][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph10][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph10][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph10][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph10][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph10][DEBUG ] Selecting previously unselected package radosgw.
[ceph10][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph10][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph10][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph10][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph10][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph10][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph10][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph10][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph10][DEBUG ] radosgw-all start/running
[ceph10][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph10][DEBUG ] ceph-all start/running
[ceph10][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph10][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph10][DEBUG ] ceph-mds-all start/running
[ceph10][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph10][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph10][INFO  ] Running command: ceph --version
[ceph10][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph11 ...
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][INFO  ] installing Ceph on ceph11
[ceph11][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph11][DEBUG ] Reading package lists...
[ceph11][DEBUG ] Building dependency tree...
[ceph11][DEBUG ] Reading state information...
[ceph11][DEBUG ] ca-certificates is already the newest version.
[ceph11][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph11][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph11][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph11][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph11][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph11][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph11][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph11][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph11][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph11][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph11][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-65
[ceph11][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph11][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph11][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph11][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph11][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-65-generic
[ceph11][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph11][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph11][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph11][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph11][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.
[ceph11][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph11][WARNING] --2015-11-20 16:50:01--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph11][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph11][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph11][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph11][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph11][WARNING] Saving to: ‘release.asc’
[ceph11][WARNING] 
[ceph11][WARNING]      0K .                                                     100% 29.9M=0s
[ceph11][WARNING] 
[ceph11][WARNING] 2015-11-20 16:50:10 (29.9 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph11][WARNING] 
[ceph11][INFO  ] Running command: apt-key add release.asc
[ceph11][DEBUG ] OK
[ceph11][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph11][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph11][DEBUG ] Ign http://repo trusty InRelease
[ceph11][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph11][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph11][DEBUG ] Hit http://repo trusty Release.gpg
[ceph11][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph11][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph11][DEBUG ] Hit http://repo trusty Release
[ceph11][DEBUG ] Hit http://repo trusty-updates Release
[ceph11][DEBUG ] Hit http://repo trusty-backports Release
[ceph11][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph11][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph11][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph11][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph11][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph11][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph11][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph11][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph11][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph11][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph11][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph11][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph11][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph11][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph11][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph11][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph11][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph11][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph11][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph11][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph11][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph11][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph11][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph11][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph11][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph11][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph11][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph11][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph11][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph11][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph11][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph11][DEBUG ] Fetched 1,059 kB in 3s (344 kB/s)
[ceph11][DEBUG ] Reading package lists...
[ceph11][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph11][DEBUG ] Reading package lists...
[ceph11][DEBUG ] Building dependency tree...
[ceph11][DEBUG ] Reading state information...
[ceph11][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph11][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph11][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph11][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph11][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph11][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph11][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph11][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph11][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph11][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph11][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph11][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-65-generic
[ceph11][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph11][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph11][DEBUG ] The following extra packages will be installed:
[ceph11][DEBUG ]   ceph-common
[ceph11][DEBUG ] Recommended packages:
[ceph11][DEBUG ]   ceph-fs-common
[ceph11][DEBUG ] The following NEW packages will be installed:
[ceph11][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph11][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 63 not upgraded.
[ceph11][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph11][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph11][DEBUG ] Selecting previously unselected package ceph-common.
[ceph11][DEBUG ] (Reading database ... 293488 files and directories currently installed.)
[ceph11][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph11][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph11][DEBUG ] Selecting previously unselected package ceph.
[ceph11][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph11][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph11][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph11][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph11][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph11][DEBUG ] Selecting previously unselected package radosgw.
[ceph11][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph11][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph11][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph11][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph11][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph11][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph11][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph11][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph11][DEBUG ] radosgw-all start/running
[ceph11][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph11][DEBUG ] ceph-all start/running
[ceph11][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph11][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph11][DEBUG ] ceph-mds-all start/running
[ceph11][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph11][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph11][INFO  ] Running command: ceph --version
[ceph11][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph12 ...
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][INFO  ] installing Ceph on ceph12
[ceph12][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph12][DEBUG ] Reading package lists...
[ceph12][DEBUG ] Building dependency tree...
[ceph12][DEBUG ] Reading state information...
[ceph12][DEBUG ] ca-certificates is already the newest version.
[ceph12][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph12][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph12][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph12][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph12][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph12][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph12][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph12][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph12][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph12][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph12][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph12][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph12][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph12][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph12][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph12][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph12][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph12][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph12][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph12][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph12][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph12][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph12][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
[ceph12][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph12][WARNING] --2015-11-20 16:50:55--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph12][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph12][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph12][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph12][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph12][WARNING] Saving to: ‘release.asc’
[ceph12][WARNING] 
[ceph12][WARNING]      0K .                                                     100% 40.9M=0s
[ceph12][WARNING] 
[ceph12][WARNING] 2015-11-20 16:50:59 (40.9 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph12][WARNING] 
[ceph12][INFO  ] Running command: apt-key add release.asc
[ceph12][DEBUG ] OK
[ceph12][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph12][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph12][DEBUG ] Ign http://repo trusty InRelease
[ceph12][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph12][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph12][DEBUG ] Hit http://repo trusty Release.gpg
[ceph12][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph12][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph12][DEBUG ] Hit http://repo trusty Release
[ceph12][DEBUG ] Hit http://repo trusty-updates Release
[ceph12][DEBUG ] Hit http://repo trusty-backports Release
[ceph12][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph12][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph12][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph12][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph12][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph12][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph12][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph12][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph12][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph12][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph12][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph12][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph12][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph12][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph12][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph12][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph12][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph12][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph12][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph12][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph12][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph12][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph12][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph12][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph12][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph12][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph12][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph12][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph12][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph12][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph12][DEBUG ] Fetched 1,059 kB in 10s (98.0 kB/s)
[ceph12][DEBUG ] Reading package lists...
[ceph12][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph12][DEBUG ] Reading package lists...
[ceph12][DEBUG ] Building dependency tree...
[ceph12][DEBUG ] Reading state information...
[ceph12][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph12][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph12][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph12][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph12][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph12][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph12][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph12][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph12][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph12][DEBUG ] The following extra packages will be installed:
[ceph12][DEBUG ]   ceph-common
[ceph12][DEBUG ] Recommended packages:
[ceph12][DEBUG ]   ceph-fs-common
[ceph12][DEBUG ] The following NEW packages will be installed:
[ceph12][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph12][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph12][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph12][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph12][DEBUG ] Selecting previously unselected package ceph-common.
[ceph12][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph12][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph12][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph12][DEBUG ] Selecting previously unselected package ceph.
[ceph12][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph12][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph12][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph12][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph12][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph12][DEBUG ] Selecting previously unselected package radosgw.
[ceph12][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph12][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph12][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph12][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph12][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph12][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph12][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph12][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph12][DEBUG ] radosgw-all start/running
[ceph12][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph12][DEBUG ] ceph-all start/running
[ceph12][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph12][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph12][DEBUG ] ceph-mds-all start/running
[ceph12][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph12][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph12][INFO  ] Running command: ceph --version
[ceph12][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph15 ...
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][INFO  ] installing Ceph on ceph15
[ceph15][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph15][DEBUG ] Reading package lists...
[ceph15][DEBUG ] Building dependency tree...
[ceph15][DEBUG ] Reading state information...
[ceph15][DEBUG ] ca-certificates is already the newest version.
[ceph15][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph15][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph15][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph15][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph15][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph15][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph15][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph15][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph15][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph15][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph15][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph15][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph15][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph15][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph15][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph15][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph15][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph15][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph15][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph15][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph15][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph15][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph15][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
[ceph15][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph15][WARNING] --2015-11-20 16:51:55--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph15][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph15][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph15][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph15][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph15][WARNING] Saving to: ‘release.asc’
[ceph15][WARNING] 
[ceph15][WARNING]      0K .                                                     100% 32.9M=0s
[ceph15][WARNING] 
[ceph15][WARNING] 2015-11-20 16:51:59 (32.9 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph15][WARNING] 
[ceph15][INFO  ] Running command: apt-key add release.asc
[ceph15][DEBUG ] OK
[ceph15][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph15][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph15][DEBUG ] Ign http://repo trusty InRelease
[ceph15][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph15][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph15][DEBUG ] Hit http://repo trusty Release.gpg
[ceph15][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph15][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph15][DEBUG ] Hit http://repo trusty Release
[ceph15][DEBUG ] Hit http://repo trusty-updates Release
[ceph15][DEBUG ] Hit http://repo trusty-backports Release
[ceph15][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph15][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph15][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph15][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph15][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph15][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph15][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph15][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph15][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph15][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph15][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph15][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph15][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph15][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph15][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph15][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph15][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph15][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph15][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph15][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph15][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph15][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph15][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph15][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph15][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph15][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph15][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph15][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph15][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph15][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph15][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph15][DEBUG ] Fetched 1,059 kB in 3s (349 kB/s)
[ceph15][DEBUG ] Reading package lists...
[ceph15][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph15][DEBUG ] Reading package lists...
[ceph15][DEBUG ] Building dependency tree...
[ceph15][DEBUG ] Reading state information...
[ceph15][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph15][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph15][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph15][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph15][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph15][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph15][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph15][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph15][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph15][DEBUG ] The following extra packages will be installed:
[ceph15][DEBUG ]   ceph-common
[ceph15][DEBUG ] Recommended packages:
[ceph15][DEBUG ]   ceph-fs-common
[ceph15][DEBUG ] The following NEW packages will be installed:
[ceph15][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph15][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph15][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph15][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph15][DEBUG ] Selecting previously unselected package ceph-common.
[ceph15][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph15][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph15][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph15][DEBUG ] Selecting previously unselected package ceph.
[ceph15][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph15][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph15][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph15][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph15][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph15][DEBUG ] Selecting previously unselected package radosgw.
[ceph15][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph15][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph15][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph15][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph15][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph15][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph15][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph15][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph15][DEBUG ] radosgw-all start/running
[ceph15][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph15][DEBUG ] ceph-all start/running
[ceph15][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph15][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph15][DEBUG ] ceph-mds-all start/running
[ceph15][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph15][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph15][INFO  ] Running command: ceph --version
[ceph15][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph16 ...
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][INFO  ] installing Ceph on ceph16
[ceph16][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph16][DEBUG ] Reading package lists...
[ceph16][DEBUG ] Building dependency tree...
[ceph16][DEBUG ] Reading state information...
[ceph16][DEBUG ] ca-certificates is already the newest version.
[ceph16][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph16][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph16][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph16][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph16][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph16][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph16][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph16][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph16][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph16][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph16][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph16][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph16][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph16][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph16][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph16][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph16][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph16][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph16][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph16][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph16][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph16][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph16][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
[ceph16][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph16][WARNING] --2015-11-20 16:52:43--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph16][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph16][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph16][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph16][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph16][WARNING] Saving to: ‘release.asc’
[ceph16][WARNING] 
[ceph16][WARNING]      0K .                                                     100% 40.6M=0s
[ceph16][WARNING] 
[ceph16][WARNING] 2015-11-20 16:52:45 (40.6 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph16][WARNING] 
[ceph16][INFO  ] Running command: apt-key add release.asc
[ceph16][DEBUG ] OK
[ceph16][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph16][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph16][DEBUG ] Ign http://repo trusty InRelease
[ceph16][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph16][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph16][DEBUG ] Hit http://repo trusty Release.gpg
[ceph16][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph16][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph16][DEBUG ] Hit http://repo trusty Release
[ceph16][DEBUG ] Hit http://repo trusty-updates Release
[ceph16][DEBUG ] Hit http://repo trusty-backports Release
[ceph16][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph16][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph16][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph16][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph16][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph16][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph16][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph16][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph16][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph16][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph16][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph16][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph16][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph16][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph16][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph16][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph16][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph16][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph16][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph16][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph16][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph16][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph16][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph16][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph16][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph16][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph16][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph16][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph16][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph16][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph16][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph16][DEBUG ] Fetched 1,059 kB in 12s (87.6 kB/s)
[ceph16][DEBUG ] Reading package lists...
[ceph16][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph16][DEBUG ] Reading package lists...
[ceph16][DEBUG ] Building dependency tree...
[ceph16][DEBUG ] Reading state information...
[ceph16][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph16][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph16][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph16][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph16][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph16][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph16][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph16][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph16][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph16][DEBUG ] The following extra packages will be installed:
[ceph16][DEBUG ]   ceph-common
[ceph16][DEBUG ] Recommended packages:
[ceph16][DEBUG ]   ceph-fs-common
[ceph16][DEBUG ] The following NEW packages will be installed:
[ceph16][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph16][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph16][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph16][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph16][DEBUG ] Selecting previously unselected package ceph-common.
[ceph16][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph16][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph16][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph16][DEBUG ] Selecting previously unselected package ceph.
[ceph16][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph16][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph16][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph16][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph16][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph16][DEBUG ] Selecting previously unselected package radosgw.
[ceph16][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph16][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph16][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph16][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph16][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph16][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph16][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph16][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph16][DEBUG ] radosgw-all start/running
[ceph16][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph16][DEBUG ] ceph-all start/running
[ceph16][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph16][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph16][DEBUG ] ceph-mds-all start/running
[ceph16][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph16][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph16][INFO  ] Running command: ceph --version
[ceph16][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph17 ...
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][INFO  ] installing Ceph on ceph17
[ceph17][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates
[ceph17][DEBUG ] Reading package lists...
[ceph17][DEBUG ] Building dependency tree...
[ceph17][DEBUG ] Reading state information...
[ceph17][DEBUG ] ca-certificates is already the newest version.
[ceph17][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph17][DEBUG ]   btrfs-tools ceph-fuse cryptsetup-bin gdisk libaio1
[ceph17][DEBUG ]   libboost-program-options1.54.0 libboost-system1.54.0 libboost-thread1.54.0
[ceph17][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libicu52
[ceph17][DEBUG ]   libjs-jquery libleveldb1 liblttng-ust-ctl2 liblttng-ust0 liblzo2-2 libnspr4
[ceph17][DEBUG ]   libnss3 libnss3-nssdb librados2 libradosstriper1 librbd1 libsnappy1
[ceph17][DEBUG ]   libtcmalloc-minimal4 libunwind8 liburcu1 linux-headers-3.13.0-57
[ceph17][DEBUG ]   linux-headers-3.13.0-57-generic linux-headers-3.13.0-58
[ceph17][DEBUG ]   linux-headers-3.13.0-58-generic linux-headers-3.13.0-59
[ceph17][DEBUG ]   linux-headers-3.13.0-59-generic linux-headers-3.13.0-61
[ceph17][DEBUG ]   linux-headers-3.13.0-61-generic linux-headers-3.13.0-62
[ceph17][DEBUG ]   linux-headers-3.13.0-62-generic linux-headers-3.13.0-65
[ceph17][DEBUG ]   linux-headers-3.13.0-65-generic linux-headers-3.13.0-66
[ceph17][DEBUG ]   linux-headers-3.13.0-66-generic linux-headers-3.13.0-67
[ceph17][DEBUG ]   linux-headers-3.13.0-67-generic linux-image-3.13.0-57-generic
[ceph17][DEBUG ]   linux-image-3.13.0-58-generic linux-image-3.13.0-59-generic
[ceph17][DEBUG ]   linux-image-3.13.0-61-generic linux-image-3.13.0-62-generic
[ceph17][DEBUG ]   linux-image-3.13.0-65-generic linux-image-3.13.0-66-generic
[ceph17][DEBUG ]   linux-image-3.13.0-67-generic linux-image-extra-3.13.0-57-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-58-generic linux-image-extra-3.13.0-59-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-61-generic linux-image-extra-3.13.0-62-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-65-generic linux-image-extra-3.13.0-66-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-67-generic python-blinker python-cephfs
[ceph17][DEBUG ]   python-flask python-itsdangerous python-jinja2 python-markupsafe
[ceph17][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug xfsprogs
[ceph17][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph17][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
[ceph17][INFO  ] Running command: wget -O release.asc https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph17][WARNING] --2015-11-20 16:53:48--  https://git.ceph.com/?p=ceph.git;a=blob_plain;f=keys/release.asc
[ceph17][WARNING] Resolving git.ceph.com (git.ceph.com)... 67.205.20.229
[ceph17][WARNING] Connecting to git.ceph.com (git.ceph.com)|67.205.20.229|:443... connected.
[ceph17][WARNING] HTTP request sent, awaiting response... 200 OK
[ceph17][WARNING] Length: 1645 (1.6K) [text/plain]
[ceph17][WARNING] Saving to: ‘release.asc’
[ceph17][WARNING] 
[ceph17][WARNING]      0K .                                                     100% 40.4M=0s
[ceph17][WARNING] 
[ceph17][WARNING] 2015-11-20 16:53:51 (40.4 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph17][WARNING] 
[ceph17][INFO  ] Running command: apt-key add release.asc
[ceph17][DEBUG ] OK
[ceph17][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph17][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph17][DEBUG ] Ign http://repo trusty InRelease
[ceph17][DEBUG ] Ign http://repo trusty-updates InRelease
[ceph17][DEBUG ] Ign http://repo trusty-backports InRelease
[ceph17][DEBUG ] Hit http://repo trusty Release.gpg
[ceph17][DEBUG ] Hit http://repo trusty-updates Release.gpg
[ceph17][DEBUG ] Hit http://repo trusty-backports Release.gpg
[ceph17][DEBUG ] Hit http://repo trusty Release
[ceph17][DEBUG ] Hit http://repo trusty-updates Release
[ceph17][DEBUG ] Hit http://repo trusty-backports Release
[ceph17][DEBUG ] Hit http://repo trusty/main amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty/restricted amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty/universe amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty/multiverse amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty/main i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty/restricted i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty/universe i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty/multiverse i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/main amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/restricted amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/universe amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/multiverse amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/main i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/restricted i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/universe i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-updates/multiverse i386 Packages
[ceph17][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
[ceph17][DEBUG ] Hit http://repo trusty-backports/main amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/restricted amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/universe amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/multiverse amd64 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/main i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/restricted i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/universe i386 Packages
[ceph17][DEBUG ] Hit http://repo trusty-backports/multiverse i386 Packages
[ceph17][DEBUG ] Ign http://repo trusty-updates/main Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-updates/multiverse Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-updates/restricted Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-updates/universe Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-backports/main Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-backports/multiverse Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-backports/restricted Translation-en
[ceph17][DEBUG ] Ign http://repo trusty-backports/universe Translation-en
[ceph17][DEBUG ] Ign http://repo trusty/main Translation-en_US
[ceph17][DEBUG ] Ign http://repo trusty/main Translation-en
[ceph17][DEBUG ] Ign http://repo trusty/multiverse Translation-en_US
[ceph17][DEBUG ] Ign http://repo trusty/multiverse Translation-en
[ceph17][DEBUG ] Ign http://repo trusty/restricted Translation-en_US
[ceph17][DEBUG ] Ign http://repo trusty/restricted Translation-en
[ceph17][DEBUG ] Ign http://repo trusty/universe Translation-en_US
[ceph17][DEBUG ] Ign http://repo trusty/universe Translation-en
[ceph17][DEBUG ] Hit http://ceph.com trusty InRelease
[ceph17][DEBUG ] Hit http://ceph.com trusty/main amd64 Packages
[ceph17][DEBUG ] Hit http://ceph.com trusty/main i386 Packages
[ceph17][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main amd64 Packages [370 kB]
[ceph17][DEBUG ] Ign http://ceph.com trusty/main Translation-en_US
[ceph17][DEBUG ] Ign http://ceph.com trusty/main Translation-en
[ceph17][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[ceph17][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe amd64 Packages [120 kB]
[ceph17][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,740 B]
[ceph17][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main i386 Packages [350 kB]
[ceph17][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[ceph17][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe i386 Packages [119 kB]
[ceph17][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse i386 Packages [4,921 B]
[ceph17][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Translation-en
[ceph17][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Translation-en
[ceph17][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Translation-en
[ceph17][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Translation-en
[ceph17][DEBUG ] Fetched 1,059 kB in 10s (102 kB/s)
[ceph17][DEBUG ] Reading package lists...
[ceph17][INFO  ] Running command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[ceph17][DEBUG ] Reading package lists...
[ceph17][DEBUG ] Building dependency tree...
[ceph17][DEBUG ] Reading state information...
[ceph17][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph17][DEBUG ]   linux-headers-3.13.0-57 linux-headers-3.13.0-57-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-58 linux-headers-3.13.0-58-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-59 linux-headers-3.13.0-59-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-61 linux-headers-3.13.0-61-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-62 linux-headers-3.13.0-62-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-65 linux-headers-3.13.0-65-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-66 linux-headers-3.13.0-66-generic
[ceph17][DEBUG ]   linux-headers-3.13.0-67 linux-headers-3.13.0-67-generic
[ceph17][DEBUG ]   linux-image-3.13.0-57-generic linux-image-3.13.0-58-generic
[ceph17][DEBUG ]   linux-image-3.13.0-59-generic linux-image-3.13.0-61-generic
[ceph17][DEBUG ]   linux-image-3.13.0-62-generic linux-image-3.13.0-65-generic
[ceph17][DEBUG ]   linux-image-3.13.0-66-generic linux-image-3.13.0-67-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-57-generic linux-image-extra-3.13.0-58-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-59-generic linux-image-extra-3.13.0-61-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-62-generic linux-image-extra-3.13.0-65-generic
[ceph17][DEBUG ]   linux-image-extra-3.13.0-66-generic linux-image-extra-3.13.0-67-generic
[ceph17][DEBUG ] Use 'apt-get autoremove' to remove them.
[ceph17][DEBUG ] The following extra packages will be installed:
[ceph17][DEBUG ]   ceph-common
[ceph17][DEBUG ] Recommended packages:
[ceph17][DEBUG ]   ceph-fs-common
[ceph17][DEBUG ] The following NEW packages will be installed:
[ceph17][DEBUG ]   ceph ceph-common ceph-mds radosgw
[ceph17][DEBUG ] 0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.
[ceph17][DEBUG ] Need to get 0 B/26.6 MB of archives.
[ceph17][DEBUG ] After this operation, 126 MB of additional disk space will be used.
[ceph17][DEBUG ] Selecting previously unselected package ceph-common.
[ceph17][DEBUG ] (Reading database ... 323129 files and directories currently installed.)
[ceph17][DEBUG ] Preparing to unpack .../ceph-common_0.94.5-1trusty_amd64.deb ...
[ceph17][DEBUG ] Unpacking ceph-common (0.94.5-1trusty) ...
[ceph17][DEBUG ] Selecting previously unselected package ceph.
[ceph17][DEBUG ] Preparing to unpack .../ceph_0.94.5-1trusty_amd64.deb ...
[ceph17][DEBUG ] Unpacking ceph (0.94.5-1trusty) ...
[ceph17][DEBUG ] Selecting previously unselected package ceph-mds.
[ceph17][DEBUG ] Preparing to unpack .../ceph-mds_0.94.5-1trusty_amd64.deb ...
[ceph17][DEBUG ] Unpacking ceph-mds (0.94.5-1trusty) ...
[ceph17][DEBUG ] Selecting previously unselected package radosgw.
[ceph17][DEBUG ] Preparing to unpack .../radosgw_0.94.5-1trusty_amd64.deb ...
[ceph17][DEBUG ] Unpacking radosgw (0.94.5-1trusty) ...
[ceph17][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph17][DEBUG ] ureadahead will be reprofiled on next reboot
[ceph17][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[ceph17][DEBUG ] Setting up ceph-common (0.94.5-1trusty) ...
[ceph17][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph17][DEBUG ] Setting up radosgw (0.94.5-1trusty) ...
[ceph17][DEBUG ] radosgw-all start/running
[ceph17][DEBUG ] Setting up ceph (0.94.5-1trusty) ...
[ceph17][DEBUG ] ceph-all start/running
[ceph17][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph17][DEBUG ] Setting up ceph-mds (0.94.5-1trusty) ...
[ceph17][DEBUG ] ceph-mds-all start/running
[ceph17][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
[ceph17][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[ceph17][INFO  ] Running command: ceph --version
[ceph17][DEBUG ] ceph version 0.94.5 (9764da52395923e0b32908d83a9f7304401fee43)
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph17:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f403bed5710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f403c33f7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph17][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph17:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa342907710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fa342d717d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph17][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph17:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f59a66d1710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f59a6b3b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph17][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph17:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3ebf754710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f3ebfbbe7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph17][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f910525b710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f91056c57d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph16][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f66454e8710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f66459527d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph16][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b5dce9710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f3b5e1537d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph16][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6f43134710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6f4359e7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph16][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe2614ad710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe2619177d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph15][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f18f3cfc710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f18f41667d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph15][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f61a4ddd710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f61a52477d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph15][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f17df990710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f17dfdfa7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph15][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f078b1b6710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f078b6207d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph12][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efc1043d710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7efc108a77d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph12][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1445511710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f144597b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph12][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe05c604710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe05ca6e7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph12][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph11:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7b17bdf710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f7b180497d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph11][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph11:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f998b975710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f998bddf7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph11][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph11:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fad523d5710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fad5283f7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph11][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph11:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f65c8922710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f65c8d8c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph11][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbf7a287710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fbf7a6f17d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph10][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf18569710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcf189d37d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph10][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcd3c435710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcd3c89f7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph10][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f71ed7ec710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f71edc567d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph10][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph9:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda6270e710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fda62b787d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph9:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fce8f420710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fce8f88a7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph9:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fca6081b710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fca60c857d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph9:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcba297a710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcba2de47d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph9
[ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph9 resulted in errors: HostNotFound ceph9

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd1e1cd1710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd1e213b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph8][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe9d6559710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe9d69c37d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph8][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efd22e7f710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7efd232e97d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph8][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb98b0bc710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fb98b5267d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph8][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feeba1ae710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7feeba6187d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph6][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f574f4b3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f574f91d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph6][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd57b39b710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd57b8057d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph6][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f34dd199710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f34dd6037d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph6][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5840802710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f5840c6c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph5][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f67f10e4710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f67f154e7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph5][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4c1a083710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4c1a4ed7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph5][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd2b1675710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd2b1adf7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph5][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc8c6ae1710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fc8c6f4b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph4][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f755fbbd710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f75600277d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph4][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f532e6ef710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f532eb597d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph4][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbf3463d710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fbf34aa77d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph4][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5431f69710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f54323d37d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph2][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0df8fc3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0df942d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph2][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc7816ca710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fc781b347d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph2][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1f55a9e710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f1f55f087d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph2][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9a9a88e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9a9acec758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sda:/dev/sda
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sda journal /dev/sda activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:eecd39f6-bbc4-42a7-9bb2-3c240c9d9d2e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/eecd39f6-bbc4-42a7-9bb2-3c240c9d9d2e
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/eecd39f6-bbc4-42a7-9bb2-3c240c9d9d2e
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:76931c2d-c6b7-4f4d-8b2f-10ceeda9cdab --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph2][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.8V3uBP with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.8V3uBP
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.8V3uBP
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.8V3uBP/journal -> /dev/disk/by-partuuid/eecd39f6-bbc4-42a7-9bb2-3c240c9d9d2e
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.8V3uBP
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.8V3uBP
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fed2578e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fed25bec758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdc:/dev/sdc
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdc journal /dev/sdc activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:367829a7-13e4-4350-a109-69d820065f66 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/367829a7-13e4-4350-a109-69d820065f66
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/367829a7-13e4-4350-a109-69d820065f66
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e4badb29-a946-44b7-bb68-c74340290cff --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdc
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdc1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[ceph2][DEBUG ] meta-data=/dev/sdc1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.r3eLzc with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.r3eLzc
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.r3eLzc
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.r3eLzc/journal -> /dev/disk/by-partuuid/367829a7-13e4-4350-a109-69d820065f66
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.r3eLzc
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.r3eLzc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2331fe3128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2332441758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdd:/dev/sdd
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdd journal /dev/sdd activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5959b459-1eb7-4b9a-a42d-ff2b94594aef --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5959b459-1eb7-4b9a-a42d-ff2b94594aef
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5959b459-1eb7-4b9a-a42d-ff2b94594aef
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:6331104d-14df-40b1-b890-f8788deb0c78 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph2][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.q3S7NY with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.q3S7NY
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.q3S7NY
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.q3S7NY/journal -> /dev/disk/by-partuuid/5959b459-1eb7-4b9a-a42d-ff2b94594aef
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.q3S7NY
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.q3S7NY
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5956f0c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f595736a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sde:/dev/sde
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sde journal /dev/sde activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:8801c483-0620-4113-9687-50367b1ae639 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8801c483-0620-4113-9687-50367b1ae639
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8801c483-0620-4113-9687-50367b1ae639
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:872071f2-6fab-494a-b2ff-930f766e870a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph2][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.QTi2gV with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.QTi2gV
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.QTi2gV
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.QTi2gV/journal -> /dev/disk/by-partuuid/8801c483-0620-4113-9687-50367b1ae639
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.QTi2gV
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.QTi2gV
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd309a99128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd309ef7758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sda:/dev/sda
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sda journal /dev/sda activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ca5503af-b1ae-4e2a-8ba0-b5cc8ea3f8e3 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ca5503af-b1ae-4e2a-8ba0-b5cc8ea3f8e3
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ca5503af-b1ae-4e2a-8ba0-b5cc8ea3f8e3
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:8d42e171-15dd-4d3a-a373-e430027fa7cb --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph4][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.pIqmZ4 with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.pIqmZ4
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.pIqmZ4
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.pIqmZ4/journal -> /dev/disk/by-partuuid/ca5503af-b1ae-4e2a-8ba0-b5cc8ea3f8e3
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.pIqmZ4
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.pIqmZ4
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6222414128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6222872758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdb:/dev/sdb
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdb journal /dev/sdb activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5e551ed6-24f7-4629-89ac-e016dd0e6e54 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5e551ed6-24f7-4629-89ac-e016dd0e6e54
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5e551ed6-24f7-4629-89ac-e016dd0e6e54
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f82dd8e7-2e69-4d90-ae48-47526ddfca6c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph4][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.Be0mip with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.Be0mip
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Be0mip
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Be0mip/journal -> /dev/disk/by-partuuid/5e551ed6-24f7-4629-89ac-e016dd0e6e54
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Be0mip
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Be0mip
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdbb3a85128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdbb3ee3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdc:/dev/sdc
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdc journal /dev/sdc activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a66fa488-41bd-477b-ac3c-dbe62e29bb8d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a66fa488-41bd-477b-ac3c-dbe62e29bb8d
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a66fa488-41bd-477b-ac3c-dbe62e29bb8d
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:889fe752-ea19-47cd-8506-ce6fe5c191ab --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdc
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdc1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[ceph4][DEBUG ] meta-data=/dev/sdc1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.OoAr2s with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.OoAr2s
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.OoAr2s
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.OoAr2s/journal -> /dev/disk/by-partuuid/a66fa488-41bd-477b-ac3c-dbe62e29bb8d
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.OoAr2s
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.OoAr2s
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f92f7cb9128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f92f8117758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdd:/dev/sdd
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdd journal /dev/sdd activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:78b20666-b057-47cc-bdcb-0b5acc2c7346 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/78b20666-b057-47cc-bdcb-0b5acc2c7346
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/78b20666-b057-47cc-bdcb-0b5acc2c7346
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:fd9c991c-285e-41d1-8094-be95c4f682ee --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph4][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.xsjy5J with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.xsjy5J
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.xsjy5J
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.xsjy5J/journal -> /dev/disk/by-partuuid/78b20666-b057-47cc-bdcb-0b5acc2c7346
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.xsjy5J
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.xsjy5J
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fef412ab128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fef41709758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sda:/dev/sda
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sda journal /dev/sda activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:59233b16-578c-4e56-9c6d-1ff50ff909d7 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/59233b16-578c-4e56-9c6d-1ff50ff909d7
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/59233b16-578c-4e56-9c6d-1ff50ff909d7
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ccca366d-2481-4ab9-8a0b-392cb3298a71 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph5][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.9rhBq_ with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.9rhBq_
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.9rhBq_
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.9rhBq_/journal -> /dev/disk/by-partuuid/59233b16-578c-4e56-9c6d-1ff50ff909d7
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.9rhBq_
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.9rhBq_
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f20be50c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f20be96a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdb:/dev/sdb
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdb journal /dev/sdb activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:8fb12903-7471-4e9d-be92-4252bf26d8c9 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8fb12903-7471-4e9d-be92-4252bf26d8c9
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8fb12903-7471-4e9d-be92-4252bf26d8c9
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:83c77905-6536-4825-a7fa-11aae52e34b1 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph5][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.8khAaE with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.8khAaE
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.8khAaE
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.8khAaE/journal -> /dev/disk/by-partuuid/8fb12903-7471-4e9d-be92-4252bf26d8c9
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.8khAaE
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.8khAaE
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f834e9fc128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f834ee5a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdd:/dev/sdd
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdd journal /dev/sdd activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:328b83e7-9330-4bc7-bce5-e2dad7855212 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/328b83e7-9330-4bc7-bce5-e2dad7855212
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/328b83e7-9330-4bc7-bce5-e2dad7855212
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d05af375-2c86-4caa-bdd5-4971728a7956 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph5][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.pqlzon with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.pqlzon
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.pqlzon
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.pqlzon/journal -> /dev/disk/by-partuuid/328b83e7-9330-4bc7-bce5-e2dad7855212
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.pqlzon
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.pqlzon
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1284eeb128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1285349758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sde:/dev/sde
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sde journal /dev/sde activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:cfc438a9-b756-4649-96c2-da093a61d666 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cfc438a9-b756-4649-96c2-da093a61d666
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cfc438a9-b756-4649-96c2-da093a61d666
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:866a200c-944d-4eb5-8742-fbc2c2d1ca95 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph5][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.xwjcB6 with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.xwjcB6
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.xwjcB6
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.xwjcB6/journal -> /dev/disk/by-partuuid/cfc438a9-b756-4649-96c2-da093a61d666
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.xwjcB6
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.xwjcB6
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa80d7d6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa80dc34758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sda:/dev/sda
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sda journal /dev/sda activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a618e5af-aa2f-4b0c-8fc5-b2fd7b7977ff --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a618e5af-aa2f-4b0c-8fc5-b2fd7b7977ff
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a618e5af-aa2f-4b0c-8fc5-b2fd7b7977ff
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:837a16a6-b669-4577-b3fe-ba84f263cc30 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph6][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.7Kqzyl with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.7Kqzyl
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.7Kqzyl
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.7Kqzyl/journal -> /dev/disk/by-partuuid/a618e5af-aa2f-4b0c-8fc5-b2fd7b7977ff
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.7Kqzyl
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.7Kqzyl
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc9db978128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc9dbdd6758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdb:/dev/sdb
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdb journal /dev/sdb activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:f4a27c04-3515-4ea2-90b1-d087daa59d51 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/f4a27c04-3515-4ea2-90b1-d087daa59d51
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/f4a27c04-3515-4ea2-90b1-d087daa59d51
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:786326a3-4557-4a86-b0f0-6ea0a1d2c6ca --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph6][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.LMd1KV with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.LMd1KV
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.LMd1KV
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.LMd1KV/journal -> /dev/disk/by-partuuid/f4a27c04-3515-4ea2-90b1-d087daa59d51
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.LMd1KV
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.LMd1KV
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f82270bd128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f822751b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdd:/dev/sdd
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdd journal /dev/sdd activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a5d7b64c-b329-4914-bd08-a012f28b5216 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a5d7b64c-b329-4914-bd08-a012f28b5216
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a5d7b64c-b329-4914-bd08-a012f28b5216
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:865ab54a-519c-43e1-acda-ec0480967c31 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph6][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.IWWQei with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.IWWQei
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.IWWQei
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.IWWQei/journal -> /dev/disk/by-partuuid/a5d7b64c-b329-4914-bd08-a012f28b5216
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.IWWQei
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.IWWQei
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8410da6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8411204758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sde:/dev/sde
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sde journal /dev/sde activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:f4eacfdf-ec5a-436b-b172-b853bac7601b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/f4eacfdf-ec5a-436b-b172-b853bac7601b
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/f4eacfdf-ec5a-436b-b172-b853bac7601b
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d64bdcf1-691d-4e77-b1ac-bc51999ce0ec --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph6][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.TaUM_d with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.TaUM_d
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.TaUM_d
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.TaUM_d/journal -> /dev/disk/by-partuuid/f4eacfdf-ec5a-436b-b172-b853bac7601b
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.TaUM_d
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.TaUM_d
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbf55b4c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fbf55faa758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sda:/dev/sda
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sda journal /dev/sda activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:0282b2a9-6119-4e3e-b8d7-1df8ae6bbf29 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/0282b2a9-6119-4e3e-b8d7-1df8ae6bbf29
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/0282b2a9-6119-4e3e-b8d7-1df8ae6bbf29
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d6a9f9b9-863d-4046-987a-c106f1a9dc52 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph8][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.Lb6dqy with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.Lb6dqy
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Lb6dqy
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Lb6dqy/journal -> /dev/disk/by-partuuid/0282b2a9-6119-4e3e-b8d7-1df8ae6bbf29
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Lb6dqy
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Lb6dqy
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f37bf923128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f37bfd81758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdc:/dev/sdc
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdc journal /dev/sdc activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:362242c5-4a6c-480d-b1db-2426a1ccf00d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/362242c5-4a6c-480d-b1db-2426a1ccf00d
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/362242c5-4a6c-480d-b1db-2426a1ccf00d
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a7d5b28b-aa88-4bbc-8980-c072610f34c3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdc1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[ceph8][DEBUG ] meta-data=/dev/sdc1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.PwNKib with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.PwNKib
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.PwNKib
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.PwNKib/journal -> /dev/disk/by-partuuid/362242c5-4a6c-480d-b1db-2426a1ccf00d
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.PwNKib
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.PwNKib
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa5b78f5128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa5b7d53758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdd:/dev/sdd
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdd journal /dev/sdd activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:8ebe1de8-060b-44b0-a036-cb4a6729781c --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8ebe1de8-060b-44b0-a036-cb4a6729781c
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8ebe1de8-060b-44b0-a036-cb4a6729781c
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0b2b2c06-1e04-43df-9dba-af5d9d604146 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph8][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.Lhd21P with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.Lhd21P
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Lhd21P
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Lhd21P/journal -> /dev/disk/by-partuuid/8ebe1de8-060b-44b0-a036-cb4a6729781c
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Lhd21P
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Lhd21P
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fddb9cef128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fddba14d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sde:/dev/sde
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sde journal /dev/sde activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:e2294c44-d492-4bde-be89-f0042c341464 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/e2294c44-d492-4bde-be89-f0042c341464
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/e2294c44-d492-4bde-be89-f0042c341464
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:72bd0201-2463-4234-aa95-8685aa4105e8 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph8][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.7zgZir with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.7zgZir
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.7zgZir
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.7zgZir/journal -> /dev/disk/by-partuuid/e2294c44-d492-4bde-be89-f0042c341464
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.7zgZir
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.7zgZir
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph9:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f962f02b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f962f489758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph9:/dev/sda:/dev/sda
[ceph_deploy.osd][ERROR ] connecting to host: ceph9 resulted in errors: HostNotFound ceph9
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph9:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd7fb6d6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd7fbb34758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph9:/dev/sdb:/dev/sdb
[ceph_deploy.osd][ERROR ] connecting to host: ceph9 resulted in errors: HostNotFound ceph9
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph9:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2d44f9b128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2d453f9758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph9:/dev/sdc:/dev/sdc
[ceph_deploy.osd][ERROR ] connecting to host: ceph9 resulted in errors: HostNotFound ceph9
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph9:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph9', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9e58fe8128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9e59446758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph9:/dev/sdd:/dev/sdd
[ceph_deploy.osd][ERROR ] connecting to host: ceph9 resulted in errors: HostNotFound ceph9
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd70de80128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd70e2de758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sda:/dev/sda
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sda journal /dev/sda activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:74d42b35-4fc4-4314-a7f0-df9620417409 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/74d42b35-4fc4-4314-a7f0-df9620417409
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/74d42b35-4fc4-4314-a7f0-df9620417409
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:33d4aa73-4eb3-4449-8ac7-f84acaba18f3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph10][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.Ya0y4a with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.Ya0y4a
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.Ya0y4a
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.Ya0y4a/journal -> /dev/disk/by-partuuid/74d42b35-4fc4-4314-a7f0-df9620417409
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.Ya0y4a
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Ya0y4a
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb06bd09128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb06c167758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdb:/dev/sdb
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdb journal /dev/sdb activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:337706a7-c510-4111-99ac-44a890a436b6 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/337706a7-c510-4111-99ac-44a890a436b6
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/337706a7-c510-4111-99ac-44a890a436b6
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c9245f6a-c54f-4adf-a426-88422df873a9 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph10][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.9ZAtWJ with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.9ZAtWJ
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.9ZAtWJ
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.9ZAtWJ/journal -> /dev/disk/by-partuuid/337706a7-c510-4111-99ac-44a890a436b6
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.9ZAtWJ
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.9ZAtWJ
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9893561128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f98939bf758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdd:/dev/sdd
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdd journal /dev/sdd activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:8788bc54-49a9-49be-884f-aa8fa9079c2e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8788bc54-49a9-49be-884f-aa8fa9079c2e
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/8788bc54-49a9-49be-884f-aa8fa9079c2e
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3d912cbb-2883-4538-968e-c438b767e5eb --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph10][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.PenLCq with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.PenLCq
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.PenLCq
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.PenLCq/journal -> /dev/disk/by-partuuid/8788bc54-49a9-49be-884f-aa8fa9079c2e
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.PenLCq
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.PenLCq
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f634a953128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f634adb1758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sde:/dev/sde
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sde journal /dev/sde activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:3f9a09d7-b1e2-4f91-9570-6ba92947e44f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3f9a09d7-b1e2-4f91-9570-6ba92947e44f
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3f9a09d7-b1e2-4f91-9570-6ba92947e44f
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3e5837cf-6905-4d3e-bbca-8d332fc5cf28 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph10][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.HDcIUu with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.HDcIUu
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.HDcIUu
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.HDcIUu/journal -> /dev/disk/by-partuuid/3f9a09d7-b1e2-4f91-9570-6ba92947e44f
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.HDcIUu
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.HDcIUu
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd12b985128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd12bde3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sda:/dev/sda
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sda journal /dev/sda activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:cfe45ad2-32e9-4b69-910a-8a173ee7dde4 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cfe45ad2-32e9-4b69-910a-8a173ee7dde4
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cfe45ad2-32e9-4b69-910a-8a173ee7dde4
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0bb120e9-05de-463b-b9cb-2bd87783a17a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph11][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.xfKicU with options noatime,inode64
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.xfKicU
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.xfKicU
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.xfKicU/journal -> /dev/disk/by-partuuid/cfe45ad2-32e9-4b69-910a-8a173ee7dde4
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.xfKicU
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.xfKicU
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1a4e8fb128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1a4ed59758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdb:/dev/sdb
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdb journal /dev/sdb activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:37ecd7ca-ee0c-48e7-91d5-1c3279d5be70 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/37ecd7ca-ee0c-48e7-91d5-1c3279d5be70
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/37ecd7ca-ee0c-48e7-91d5-1c3279d5be70
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:8947470c-07b9-4612-b57a-60a957360bf3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph11][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.G29Pvr with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.G29Pvr
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.G29Pvr
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.G29Pvr/journal -> /dev/disk/by-partuuid/37ecd7ca-ee0c-48e7-91d5-1c3279d5be70
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.G29Pvr
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.G29Pvr
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f842082c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8420c8a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdd:/dev/sdd
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdd journal /dev/sdd activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:31496909-64e8-44cc-a851-bad1f1855bd3 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/31496909-64e8-44cc-a851-bad1f1855bd3
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/31496909-64e8-44cc-a851-bad1f1855bd3
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:00b6ec6a-48f2-4c82-ba7b-091ed055873a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph11][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt._1hnsa with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt._1hnsa
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt._1hnsa
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt._1hnsa/journal -> /dev/disk/by-partuuid/31496909-64e8-44cc-a851-bad1f1855bd3
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt._1hnsa
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt._1hnsa
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f81990e1128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f819953f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sde:/dev/sde
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sde journal /dev/sde activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:2e59cd5f-167c-4c6c-b9e4-f554c654b90c --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/2e59cd5f-167c-4c6c-b9e4-f554c654b90c
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/2e59cd5f-167c-4c6c-b9e4-f554c654b90c
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:86323646-4095-418a-ade4-c8e227c2972c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph11][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.xXy4uQ with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.xXy4uQ
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.xXy4uQ
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.xXy4uQ/journal -> /dev/disk/by-partuuid/2e59cd5f-167c-4c6c-b9e4-f554c654b90c
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.xXy4uQ
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.xXy4uQ
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8d5f09d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8d5f4fb758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sda:/dev/sda
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sda journal /dev/sda activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a32c48ae-54fc-4462-842c-6404512e7dbb --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a32c48ae-54fc-4462-842c-6404512e7dbb
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a32c48ae-54fc-4462-842c-6404512e7dbb
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:cca8b08b-b808-4900-afee-51f023e6a9cd --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph12][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.yLdcdZ with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.yLdcdZ
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.yLdcdZ
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.yLdcdZ/journal -> /dev/disk/by-partuuid/a32c48ae-54fc-4462-842c-6404512e7dbb
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.yLdcdZ
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.yLdcdZ
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f054c74f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f054cbad758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdb journal /dev/sdb activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:eea48a5d-d377-4cb8-a197-5d0f82e8dc06 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/eea48a5d-d377-4cb8-a197-5d0f82e8dc06
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/eea48a5d-d377-4cb8-a197-5d0f82e8dc06
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:6f54737f-e877-4e85-a3f9-a6707a1717a8 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph12][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.b3R4ZS with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.b3R4ZS
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.b3R4ZS
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.b3R4ZS/journal -> /dev/disk/by-partuuid/eea48a5d-d377-4cb8-a197-5d0f82e8dc06
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.b3R4ZS
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.b3R4ZS
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd8a2d3c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd8a319a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdd:/dev/sdd
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdd journal /dev/sdd activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:3a6dee9f-9cbb-427b-bd42-80e502502706 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3a6dee9f-9cbb-427b-bd42-80e502502706
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3a6dee9f-9cbb-427b-bd42-80e502502706
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e24a5e94-8fe2-42df-9b6b-ec3b5870a86c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph12][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.6y0ajC with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.6y0ajC
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.6y0ajC
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.6y0ajC/journal -> /dev/disk/by-partuuid/3a6dee9f-9cbb-427b-bd42-80e502502706
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.6y0ajC
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.6y0ajC
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1d3a787128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1d3abe5758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sde:/dev/sde
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sde journal /dev/sde activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:191b6922-1bb4-4b69-9264-67080204052b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/191b6922-1bb4-4b69-9264-67080204052b
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/191b6922-1bb4-4b69-9264-67080204052b
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0236a2e3-3522-4491-8283-46218e3659e5 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph12][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.a4SRbu with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.a4SRbu
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.a4SRbu
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.a4SRbu/journal -> /dev/disk/by-partuuid/191b6922-1bb4-4b69-9264-67080204052b
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.a4SRbu
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.a4SRbu
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f11607e8128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1160c46758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sda journal /dev/sda activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:9b1690fa-e98c-4119-ba58-6af64ebaed7d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9b1690fa-e98c-4119-ba58-6af64ebaed7d
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/9b1690fa-e98c-4119-ba58-6af64ebaed7d
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7b17cc51-743e-4ff7-99bb-80862539103b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph15][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt._Yu_1b with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt._Yu_1b
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt._Yu_1b
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt._Yu_1b/journal -> /dev/disk/by-partuuid/9b1690fa-e98c-4119-ba58-6af64ebaed7d
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt._Yu_1b
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt._Yu_1b
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f078a480128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f078a8de758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdb:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdb journal /dev/sdb activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:cfa1daa9-b47b-480c-9f04-97ede05cee0d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cfa1daa9-b47b-480c-9f04-97ede05cee0d
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cfa1daa9-b47b-480c-9f04-97ede05cee0d
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:608333fa-dab6-486c-9ef6-9f09867988ad --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph15][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.2E04qi with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.2E04qi
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.2E04qi
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.2E04qi/journal -> /dev/disk/by-partuuid/cfa1daa9-b47b-480c-9f04-97ede05cee0d
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.2E04qi
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.2E04qi
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb249c1e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb24a07c758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdd:/dev/sdd
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdd journal /dev/sdd activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b6b02aaf-2da5-4a76-9234-d1bb0b802629 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b6b02aaf-2da5-4a76-9234-d1bb0b802629
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b6b02aaf-2da5-4a76-9234-d1bb0b802629
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f40ef5ce-9f04-47ff-8852-4ba593443ddf --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph15][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.hQBosi with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.hQBosi
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.hQBosi
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.hQBosi/journal -> /dev/disk/by-partuuid/b6b02aaf-2da5-4a76-9234-d1bb0b802629
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.hQBosi
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.hQBosi
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f47de585128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f47de9e3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sde:/dev/sde
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sde journal /dev/sde activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:90564d6b-241b-4398-89fe-ac84cd0b24f7 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/90564d6b-241b-4398-89fe-ac84cd0b24f7
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/90564d6b-241b-4398-89fe-ac84cd0b24f7
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e9e20858-703f-4e67-b145-f0fcd19f6bb0 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph15][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.MWKlkb with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.MWKlkb
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.MWKlkb
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.MWKlkb/journal -> /dev/disk/by-partuuid/90564d6b-241b-4398-89fe-ac84cd0b24f7
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.MWKlkb
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.MWKlkb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8d934a2128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8d93900758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sda:/dev/sda
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sda journal /dev/sda activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:107554c8-95d6-4e31-98ab-d7bcefb136f8 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/107554c8-95d6-4e31-98ab-d7bcefb136f8
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/107554c8-95d6-4e31-98ab-d7bcefb136f8
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:9578418e-9026-4d14-b12c-2377dd3c9ae5 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph16][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.8adTu5 with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.8adTu5
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.8adTu5
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.8adTu5/journal -> /dev/disk/by-partuuid/107554c8-95d6-4e31-98ab-d7bcefb136f8
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.8adTu5
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.8adTu5
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ae7a20128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0ae7e7e758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdb:/dev/sdb
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdb journal /dev/sdb activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph16][WARNING] ceph-disk: Error: Device is mounted: /dev/sdb1
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f992d8fb128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f992dd59758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdd:/dev/sdd
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdd journal /dev/sdd activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:25084a74-0001-4898-b249-cdc4fd988223 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/25084a74-0001-4898-b249-cdc4fd988223
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/25084a74-0001-4898-b249-cdc4fd988223
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a84021b8-f2ff-4926-9cbe-524f7203ca3b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph16][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.xSXK7A with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.xSXK7A
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.xSXK7A
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.xSXK7A/journal -> /dev/disk/by-partuuid/25084a74-0001-4898-b249-cdc4fd988223
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.xSXK7A
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.xSXK7A
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbef5e16128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fbef6274758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sde:/dev/sde
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sde journal /dev/sde activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:65e19223-374c-417a-824f-856f19c06d45 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/65e19223-374c-417a-824f-856f19c06d45
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/65e19223-374c-417a-824f-856f19c06d45
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:6ba831a1-e462-4dd8-b6fc-ce37e638d1bc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph16][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.hGWRHt with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.hGWRHt
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.hGWRHt
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.hGWRHt/journal -> /dev/disk/by-partuuid/65e19223-374c-417a-824f-856f19c06d45
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.hGWRHt
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.hGWRHt
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f745f8aa128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f745fd08758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sda:/dev/sda
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sda journal /dev/sda activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:273128b2-0583-47b8-acd4-3074c26a9f0a --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/273128b2-0583-47b8-acd4-3074c26a9f0a
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/273128b2-0583-47b8-acd4-3074c26a9f0a
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:eaf37559-0246-497c-bf2c-b41f18c6b123 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph17][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.lVb5RC with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.lVb5RC
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.lVb5RC
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.lVb5RC/journal -> /dev/disk/by-partuuid/273128b2-0583-47b8-acd4-3074c26a9f0a
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.lVb5RC
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.lVb5RC
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc433465128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc4338c3758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdb:/dev/sdb
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdb journal /dev/sdb activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4c19f529-2857-4667-b9f7-449976325b8c --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4c19f529-2857-4667-b9f7-449976325b8c
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4c19f529-2857-4667-b9f7-449976325b8c
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4b79ebf4-dcd8-467e-b90e-2222d26af31a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph17][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.8BJFrn with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.8BJFrn
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.8BJFrn
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.8BJFrn/journal -> /dev/disk/by-partuuid/4c19f529-2857-4667-b9f7-449976325b8c
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.8BJFrn
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.8BJFrn
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa576bc8128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa577026758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdd:/dev/sdd
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdd journal /dev/sdd activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:3e801f03-6517-4c0a-bb86-8504f4266b98 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3e801f03-6517-4c0a-bb86-8504f4266b98
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/3e801f03-6517-4c0a-bb86-8504f4266b98
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:05d9818c-fddb-41ac-a56b-baa83a8830bd --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph17][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.zK22fD with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.zK22fD
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.zK22fD
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.zK22fD/journal -> /dev/disk/by-partuuid/3e801f03-6517-4c0a-bb86-8504f4266b98
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.zK22fD
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.zK22fD
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe571bc7128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe572025758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sde:/dev/sde
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sde journal /dev/sde activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4de0b2f5-6a0a-44bb-ab69-695aea36ce9b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4de0b2f5-6a0a-44bb-ab69-695aea36ce9b
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4de0b2f5-6a0a-44bb-ab69-695aea36ce9b
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f20d863c-42a4-4d4d-9800-35cf0fcaa907 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph17][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.AtnyBf with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.AtnyBf
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.AtnyBf
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.AtnyBf/journal -> /dev/disk/by-partuuid/4de0b2f5-6a0a-44bb-ab69-695aea36ce9b
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.AtnyBf
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.AtnyBf
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph4 ceph5 ceph6 ceph8 ceph9 ceph10 ceph11 ceph12 ceph15 ceph16 ceph17
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff9f83bfdd0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['ceph1', 'ceph2', 'ceph4', 'ceph5', 'ceph6', 'ceph8', 'ceph9', 'ceph10', 'ceph11', 'ceph12', 'ceph15', 'ceph16', 'ceph17']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff9f8c556e0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1
[ceph1][DEBUG ] connected to host: ceph1 
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph9
[ceph_deploy.admin][ERROR ] connecting to host: ceph9 resulted in errors: HostNotFound ceph9
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbf0eeb6710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fbf0f3207d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph6][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3696f0d128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f369736b758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdb:/dev/sdb
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdb journal /dev/sdb activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:d8419614-6a90-4714-933a-9af21bd82d7e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d8419614-6a90-4714-933a-9af21bd82d7e
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d8419614-6a90-4714-933a-9af21bd82d7e
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4dfd477c-236e-4aa7-be91-bc9a65a00852 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph6][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.IQKIl_ with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.IQKIl_
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.IQKIl_
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.IQKIl_/journal -> /dev/disk/by-partuuid/d8419614-6a90-4714-933a-9af21bd82d7e
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.IQKIl_
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.IQKIl_
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd24c991710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd24cdfb7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph6][DEBUG ] The new table will be used at the next reboot.
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph6][DEBUG ] The new table will be used at the next reboot.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] Error: Partition(s) 1, 2 on /dev/sde have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph6][INFO  ] Running command: partprobe /dev/sde
[ceph6][WARNING] Error: Partition(s) 1, 2 on /dev/sde have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sde

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f184fc80128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f18500de758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sde:/dev/sde
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sde journal /dev/sde activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph6][WARNING] ceph-disk: Error: Device is mounted: /dev/sde1
[ceph6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0f0c78a710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0f0cbf47d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph12][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f154f7ef128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f154fc4d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdb journal /dev/sdb activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5603f8ab-297a-49cb-aa94-2b2b3d5ee4c8 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5603f8ab-297a-49cb-aa94-2b2b3d5ee4c8
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5603f8ab-297a-49cb-aa94-2b2b3d5ee4c8
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:53351329-bc3c-44e9-a2e2-804a37c453dc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph12][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.PT4wWg with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.PT4wWg
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.PT4wWg
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.PT4wWg/journal -> /dev/disk/by-partuuid/5603f8ab-297a-49cb-aa94-2b2b3d5ee4c8
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.PT4wWg
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.PT4wWg
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4a7dcb3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4a7e11d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph2][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5a2e9be128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5a2ee1c758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sde:/dev/sde
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sde journal /dev/sde activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b9b93767-2ff2-4863-9054-81cceb5d948c --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b9b93767-2ff2-4863-9054-81cceb5d948c
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b9b93767-2ff2-4863-9054-81cceb5d948c
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3bf06fd0-79c4-42b3-a116-1659ed45a1fc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph2][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.THfTmS with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.THfTmS
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.THfTmS
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.THfTmS/journal -> /dev/disk/by-partuuid/b9b93767-2ff2-4863-9054-81cceb5d948c
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.THfTmS
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.THfTmS
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f13bfbce710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f13c00387d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph16][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f11301fa128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1130658758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sdc:/dev/sdc
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sdc journal /dev/sdc activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6445a583-f095-485b-8f80-1892c8d6eded --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6445a583-f095-485b-8f80-1892c8d6eded
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6445a583-f095-485b-8f80-1892c8d6eded
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c43105c4-5513-47be-9209-0ebfa30a91ce --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdc
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdc1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[ceph16][DEBUG ] meta-data=/dev/sdc1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.aJbhWN with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.aJbhWN
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.aJbhWN
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.aJbhWN/journal -> /dev/disk/by-partuuid/6445a583-f095-485b-8f80-1892c8d6eded
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.aJbhWN
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.aJbhWN
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f002977a710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0029be47d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph8][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ad5d86128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7ad61e4758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sda:/dev/sda
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sda journal /dev/sda activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:50ad2070-7106-484e-8076-acadb006256a --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/50ad2070-7106-484e-8076-acadb006256a
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/50ad2070-7106-484e-8076-acadb006256a
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e04d09e9-73de-486c-925f-ad59dc77750b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph8][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.XRdx_4 with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.XRdx_4
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.XRdx_4
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.XRdx_4/journal -> /dev/disk/by-partuuid/50ad2070-7106-484e-8076-acadb006256a
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.XRdx_4
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.XRdx_4
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5536308710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f55367727d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph10][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5d8a7db128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5d8ac39758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sda:/dev/sda
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sda journal /dev/sda activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4cd7d72b-6f70-40a8-a859-3de5fcc61651 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4cd7d72b-6f70-40a8-a859-3de5fcc61651
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4cd7d72b-6f70-40a8-a859-3de5fcc61651
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:59eaf77d-f522-4695-820b-8a2628378915 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph10][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.ASt_nj with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.ASt_nj
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.ASt_nj
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.ASt_nj/journal -> /dev/disk/by-partuuid/4cd7d72b-6f70-40a8-a859-3de5fcc61651
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.ASt_nj
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.ASt_nj
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedc3f83710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fedc43ed7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph15][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f56bdbfb128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f56be059758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sda journal /dev/sda activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4165cef7-23a6-48a8-9224-c4ad958108a3 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4165cef7-23a6-48a8-9224-c4ad958108a3
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4165cef7-23a6-48a8-9224-c4ad958108a3
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f76673ee-80fc-489f-a711-103e33b344ba --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph15][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.OGVvsV with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.OGVvsV
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.OGVvsV
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.OGVvsV/journal -> /dev/disk/by-partuuid/4165cef7-23a6-48a8-9224-c4ad958108a3
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.OGVvsV
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.OGVvsV
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f070cbc5710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f070d02f7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph4][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph4][DEBUG ] The new table will be used at the next reboot.
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph4][DEBUG ] The new table will be used at the next reboot.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] Error: Partition(s) 1, 2 on /dev/sdd have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph4][INFO  ] Running command: partprobe /dev/sdd
[ceph4][WARNING] Error: Partition(s) 1, 2 on /dev/sdd have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph4][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sdd

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f73a8b36128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f73a8f94758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdd:/dev/sdd
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdd journal /dev/sdd activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ee7c8eb8-48dc-4a4f-87e7-b6b21ab4dc3d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph4][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph4][DEBUG ] The new table will be used at the next reboot.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] Error: Partition(s) 1 on /dev/sdd have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ee7c8eb8-48dc-4a4f-87e7-b6b21ab4dc3d
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ee7c8eb8-48dc-4a4f-87e7-b6b21ab4dc3d
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:1519d6ae-3b88-436f-a3a1-03ab677a4ecc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph4][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph4][DEBUG ] The new table will be used at the next reboot.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph4][WARNING] mkfs.xfs: /dev/sdd1 contains a mounted filesystem
[ceph4][WARNING] Usage: mkfs.xfs
[ceph4][WARNING] /* blocksize */		[-b log=n|size=num]
[ceph4][WARNING] /* data subvol */	[-d agcount=n,agsize=n,file,name=xxx,size=num,
[ceph4][WARNING] 			    (sunit=value,swidth=value|su=num,sw=num),
[ceph4][WARNING] 			    sectlog=n|sectsize=num
[ceph4][WARNING] /* inode size */	[-i log=n|perblock=n|size=num,maxpct=n,attr=0|1|2,
[ceph4][WARNING] 			    projid32bit=0|1]
[ceph4][WARNING] /* log subvol */	[-l agnum=n,internal,size=num,logdev=xxx,version=n
[ceph4][WARNING] 			    sunit=value|su=num,sectlog=n|sectsize=num,
[ceph4][WARNING] 			    lazy-count=0|1]
[ceph4][WARNING] /* label */		[-L label (maximum 12 characters)]
[ceph4][WARNING] /* naming */		[-n log=n|size=num,version=2|ci]
[ceph4][WARNING] /* prototype file */	[-p fname]
[ceph4][WARNING] /* quiet */		[-q]
[ceph4][WARNING] /* realtime subvol */	[-r extsize=num,size=num,rtdev=xxx]
[ceph4][WARNING] /* sectorsize */	[-s log=n|size=num]
[ceph4][WARNING] /* version */		[-V]
[ceph4][WARNING] 			devicename
[ceph4][WARNING] <devicename> is required unless -d name=xxx is given.
[ceph4][WARNING] <num> is xxx (bytes), xxxs (sectors), xxxb (fs blocks), xxxk (xxx KiB),
[ceph4][WARNING]       xxxm (xxx MiB), xxxg (xxx GiB), xxxt (xxx TiB) or xxxp (xxx PiB).
[ceph4][WARNING] <value> is xxx (512 byte blocks).
[ceph4][WARNING] ceph-disk: Error: Command '['/sbin/mkfs', '-t', 'xfs', '-f', '-i', 'size=2048', '--', '/dev/sdd1']' returned non-zero exit status 1
[ceph4][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f35a0b61710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f35a0fcb7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph4][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph4][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fee30284128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fee306e2758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdd:/dev/sdd
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdd journal /dev/sdd activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:a146c72e-bb35-4990-9440-b32f5af2f571 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a146c72e-bb35-4990-9440-b32f5af2f571
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/a146c72e-bb35-4990-9440-b32f5af2f571
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:07ddf2f3-1706-4188-9d38-1a95dac40de4 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph4][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.tPKGCx with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.tPKGCx
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.tPKGCx
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.tPKGCx/journal -> /dev/disk/by-partuuid/a146c72e-bb35-4990-9440-b32f5af2f571
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.tPKGCx
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.tPKGCx
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph4][WARNING] there is 1 OSD down
[ceph4][WARNING] there is 1 OSD out
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe21acb3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe21b11d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph15][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3576b89128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3576fe7758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdd:/dev/sdd
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdd journal /dev/sdd activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:775ff760-0d5b-4866-a5f7-df4bfc0fa494 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/775ff760-0d5b-4866-a5f7-df4bfc0fa494
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/775ff760-0d5b-4866-a5f7-df4bfc0fa494
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:9f9c0bb2-5a19-4421-829e-f42696c44ab9 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph15][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.sATeGK with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.sATeGK
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.sATeGK
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.sATeGK/journal -> /dev/disk/by-partuuid/775ff760-0d5b-4866-a5f7-df4bfc0fa494
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.sATeGK
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.sATeGK
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph11:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd24a3f1710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd24a85b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph11
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph11][DEBUG ] zeroing last few blocks of device
[ceph11][DEBUG ] find the location of an executable
[ceph11][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph11][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph11][WARNING] backup header from main header.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph11][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph11][WARNING] 
[ceph11][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph11][WARNING] 
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph11][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph11][DEBUG ] ****************************************************************************
[ceph11][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph11][DEBUG ] other utilities.
[ceph11][DEBUG ] Creating new GPT entries.
[ceph11][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph11][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph11:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph11', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa02d4ca128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa02d928758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph11:/dev/sdb:/dev/sdb
[ceph11][DEBUG ] connected to host: ceph11 
[ceph11][DEBUG ] detect platform information from remote host
[ceph11][DEBUG ] detect machine type
[ceph11][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph11
[ceph11][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph11 disk /dev/sdb journal /dev/sdb activate True
[ceph11][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph11][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph11][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:12e5c346-af44-47f5-a922-5ba6b2e49919 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/12e5c346-af44-47f5-a922-5ba6b2e49919
[ceph11][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/12e5c346-af44-47f5-a922-5ba6b2e49919
[ceph11][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0e7030dc-562b-4c7e-a637-963e6ee8c1f3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph11][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph11][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph11][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph11][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph11][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph11][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph11][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph11][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph11][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph11][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.UxWP5l with options noatime,inode64
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.UxWP5l
[ceph11][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.UxWP5l
[ceph11][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.UxWP5l/journal -> /dev/disk/by-partuuid/12e5c346-af44-47f5-a922-5ba6b2e49919
[ceph11][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.UxWP5l
[ceph11][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.UxWP5l
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph11][DEBUG ] The operation has completed successfully.
[ceph11][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph11][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph11][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph11][INFO  ] checking OSD status...
[ceph11][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph11 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f73b96c6710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f73b9b307d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph5][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4ee46d4128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4ee4b32758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdd:/dev/sdd
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdd journal /dev/sdd activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4a9941e6-899c-43ca-9a14-d891eeecc2ac --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4a9941e6-899c-43ca-9a14-d891eeecc2ac
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4a9941e6-899c-43ca-9a14-d891eeecc2ac
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:04d1e327-490a-48dc-8ee9-fbb08aec62db --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph5][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.VzlY7V with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.VzlY7V
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.VzlY7V
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.VzlY7V/journal -> /dev/disk/by-partuuid/4a9941e6-899c-43ca-9a14-d891eeecc2ac
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.VzlY7V
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.VzlY7V
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb04f452710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fb04f8bc7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph5][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f09dcba6128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f09dd004758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sdd:/dev/sdd
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sdd journal /dev/sdd activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:4accee92-105a-40a4-a26f-f6ca808828d5 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4accee92-105a-40a4-a26f-f6ca808828d5
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/4accee92-105a-40a4-a26f-f6ca808828d5
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ce300044-26fc-46e0-861c-2fba7382f906 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph5][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.exOKPb with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.exOKPb
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.exOKPb
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.exOKPb/journal -> /dev/disk/by-partuuid/4accee92-105a-40a4-a26f-f6ca808828d5
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.exOKPb
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.exOKPb
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f371e2b9710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f371e7237d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph12][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5c34b5f128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5c34fbd758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdb journal /dev/sdb activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph12][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph12][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:f6ba910e-a873-4cde-9e89-7b9e87a71159 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/f6ba910e-a873-4cde-9e89-7b9e87a71159
[ceph12][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/f6ba910e-a873-4cde-9e89-7b9e87a71159
[ceph12][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:58c5555a-c2e1-4cf4-b192-423444be335b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph12][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph12][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph12][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph12][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph12][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph12][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph12][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph12][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph12][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph12][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.i77rnC with options noatime,inode64
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.i77rnC
[ceph12][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.i77rnC
[ceph12][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.i77rnC/journal -> /dev/disk/by-partuuid/f6ba910e-a873-4cde-9e89-7b9e87a71159
[ceph12][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.i77rnC
[ceph12][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.i77rnC
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph12][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph12][INFO  ] checking OSD status...
[ceph12][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph12 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6cddac9710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6cddf337d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph16][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3113819128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3113c77758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sde:/dev/sde
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sde journal /dev/sde activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:1f5c0efa-0952-4673-a789-e19c9fb6d513 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/1f5c0efa-0952-4673-a789-e19c9fb6d513
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/1f5c0efa-0952-4673-a789-e19c9fb6d513
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7ee285ed-e7da-4202-9590-bcef278de710 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph16][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.1zKLR3 with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.1zKLR3
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.1zKLR3
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.1zKLR3/journal -> /dev/disk/by-partuuid/1f5c0efa-0952-4673-a789-e19c9fb6d513
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.1zKLR3
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.1zKLR3
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph12:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efe210cd710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7efe215377d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph12
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph12][DEBUG ] zeroing last few blocks of device
[ceph12][DEBUG ] find the location of an executable
[ceph12][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph12][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph12][WARNING] backup header from main header.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph12][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph12][WARNING] 
[ceph12][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph12][WARNING] 
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph12][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph12][DEBUG ] ****************************************************************************
[ceph12][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph12][DEBUG ] The new table will be used at the next reboot.
[ceph12][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph12][DEBUG ] other utilities.
[ceph12][DEBUG ] Creating new GPT entries.
[ceph12][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph12][DEBUG ] The new table will be used at the next reboot.
[ceph12][DEBUG ] The operation has completed successfully.
[ceph12][WARNING] Error: Partition(s) 1, 2 on /dev/sdb have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph12][INFO  ] Running command: partprobe /dev/sdb
[ceph12][WARNING] Error: Partition(s) 1, 2 on /dev/sdb have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph12][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sdb

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph12:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph12', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f52bf093128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f52bf4f1758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph12:/dev/sdb:/dev/sdb
[ceph12][DEBUG ] connected to host: ceph12 
[ceph12][DEBUG ] detect platform information from remote host
[ceph12][DEBUG ] detect machine type
[ceph12][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph12
[ceph12][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph12][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph12 disk /dev/sdb journal /dev/sdb activate True
[ceph12][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph12][WARNING] ceph-disk: Error: Device is mounted: /dev/sdb1
[ceph12][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2937f28710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f29383927d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph16][DEBUG ] The new table will be used at the next reboot.
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph16][DEBUG ] The new table will be used at the next reboot.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] Error: Partition(s) 1, 2 on /dev/sde have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph16][INFO  ] Running command: partprobe /dev/sde
[ceph16][WARNING] Error: Partition(s) 1, 2 on /dev/sde have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sde

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcb76428128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcb76886758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sde:/dev/sde
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sde journal /dev/sde activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph16][WARNING] ceph-disk: Error: Device is mounted: /dev/sde1
[ceph16][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcb4902e710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcb494987d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph2][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fab15181128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fab155df758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sda:/dev/sda
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sda journal /dev/sda activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:7db407a2-1e20-43e4-b175-43673537d6c7 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/7db407a2-1e20-43e4-b175-43673537d6c7
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/7db407a2-1e20-43e4-b175-43673537d6c7
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:37851540-de96-44f7-b150-32e351628978 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph2][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.0wCoFR with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.0wCoFR
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.0wCoFR
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.0wCoFR/journal -> /dev/disk/by-partuuid/7db407a2-1e20-43e4-b175-43673537d6c7
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.0wCoFR
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.0wCoFR
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f010e445710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f010e8af7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph10][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9045d98128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f90461f6758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sde:/dev/sde
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sde journal /dev/sde activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5335780a-8ee4-4c01-bf59-f9d1688d41c5 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5335780a-8ee4-4c01-bf59-f9d1688d41c5
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5335780a-8ee4-4c01-bf59-f9d1688d41c5
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7e5777ad-31b3-4334-b413-8a90edaebd1d --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph10][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.13EVug with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.13EVug
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.13EVug
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.13EVug/journal -> /dev/disk/by-partuuid/5335780a-8ee4-4c01-bf59-f9d1688d41c5
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.13EVug
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.13EVug
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph6:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb086c2710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7ffb08b2c7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph6
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph6][DEBUG ] zeroing last few blocks of device
[ceph6][DEBUG ] find the location of an executable
[ceph6][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph6][WARNING] backup header from main header.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph6][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph6][WARNING] 
[ceph6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph6][WARNING] 
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph6][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph6][DEBUG ] ****************************************************************************
[ceph6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph6][DEBUG ] other utilities.
[ceph6][DEBUG ] Creating new GPT entries.
[ceph6][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph6][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph6:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph6', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd6a150c128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd6a196a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph6:/dev/sdd:/dev/sdd
[ceph6][DEBUG ] connected to host: ceph6 
[ceph6][DEBUG ] detect platform information from remote host
[ceph6][DEBUG ] detect machine type
[ceph6][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph6
[ceph6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph6 disk /dev/sdd journal /dev/sdd activate True
[ceph6][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph6][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph6][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:18c79c9d-b0e0-4ee9-8dde-315bfa524098 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/18c79c9d-b0e0-4ee9-8dde-315bfa524098
[ceph6][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/18c79c9d-b0e0-4ee9-8dde-315bfa524098
[ceph6][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:80bd8588-c155-488c-a606-a82dbfea45f0 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph6][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph6][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph6][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph6][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph6][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph6][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.6Cdeq1 with options noatime,inode64
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.6Cdeq1
[ceph6][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.6Cdeq1
[ceph6][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.6Cdeq1/journal -> /dev/disk/by-partuuid/18c79c9d-b0e0-4ee9-8dde-315bfa524098
[ceph6][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.6Cdeq1
[ceph6][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.6Cdeq1
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph6][DEBUG ] The operation has completed successfully.
[ceph6][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph6][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph6][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph6][INFO  ] checking OSD status...
[ceph6][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph6 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f85941e0710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f859464a7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph10][DEBUG ] The new table will be used at the next reboot.
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph10][DEBUG ] The new table will be used at the next reboot.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] Error: Partition(s) 1 on /dev/sdd have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph10][INFO  ] Running command: partprobe /dev/sdd
[ceph10][WARNING] Error: Partition(s) 1 on /dev/sdd have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph10][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sdd

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3ae9711128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3ae9b6f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdd:/dev/sdd
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdd journal /dev/sdd activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph10][WARNING] ceph-disk: Error: Device is mounted: /dev/sdd1
[ceph10][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f47d01fa710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f47d06647d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph15][DEBUG ] The new table will be used at the next reboot.
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph15][DEBUG ] The new table will be used at the next reboot.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] Error: Partition(s) 1 on /dev/sdb have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph15][INFO  ] Running command: partprobe /dev/sdb
[ceph15][WARNING] Error: Partition(s) 1 on /dev/sdb have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sdb

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc5bdaba128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc5bdf18758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdb:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdb journal /dev/sdb activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph15][WARNING] ceph-disk: Error: Device is mounted: /dev/sdb1
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f154050e710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f15409787d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph10][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f75e71b1128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f75e760f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdd:/dev/sdd
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdd journal /dev/sdd activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:54b6c104-1f4d-4eef-acd6-d9a7f65cd918 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/54b6c104-1f4d-4eef-acd6-d9a7f65cd918
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/54b6c104-1f4d-4eef-acd6-d9a7f65cd918
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3447bb0a-99a6-4e0b-92b1-c3be747cdc91 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph10][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.6jM5gu with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.6jM5gu
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.6jM5gu
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.6jM5gu/journal -> /dev/disk/by-partuuid/54b6c104-1f4d-4eef-acd6-d9a7f65cd918
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.6jM5gu
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.6jM5gu
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f26cdb6d710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f26cdfd77d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph15][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fec9fdac128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7feca020a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdb:/dev/sdb
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdb journal /dev/sdb activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:c5cc21f8-7358-437e-83fa-086ad08af1ed --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/c5cc21f8-7358-437e-83fa-086ad08af1ed
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/c5cc21f8-7358-437e-83fa-086ad08af1ed
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:402459ec-400a-4a73-871a-9fc2b2d45cfe --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph15][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt._PbtD6 with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt._PbtD6
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt._PbtD6
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt._PbtD6/journal -> /dev/disk/by-partuuid/c5cc21f8-7358-437e-83fa-086ad08af1ed
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt._PbtD6
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt._PbtD6
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff75dcb3710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7ff75e11d7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph2][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f72c39dc128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f72c3e3a758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sda:/dev/sda
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sda journal /dev/sda activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:75f7a5e5-4166-4991-a974-d9d81fd0aae5 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/75f7a5e5-4166-4991-a974-d9d81fd0aae5
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/75f7a5e5-4166-4991-a974-d9d81fd0aae5
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e6fc89bc-ae25-4a18-aa82-a0a60fb8de0b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph2][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.OEIuQo with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.OEIuQo
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.OEIuQo
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.OEIuQo/journal -> /dev/disk/by-partuuid/75f7a5e5-4166-4991-a974-d9d81fd0aae5
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.OEIuQo
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.OEIuQo
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5085ab1710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f5085f1b7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph4][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph4][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f883d3d0128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f883d82e758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdd:/dev/sdd
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdd journal /dev/sdd activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:96c80a52-9cfa-4624-a0ec-fbf2003c19fb --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/96c80a52-9cfa-4624-a0ec-fbf2003c19fb
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/96c80a52-9cfa-4624-a0ec-fbf2003c19fb
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:42ee1e9c-9e02-4f01-a734-f26a90b42f2f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph4][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.3kdMQa with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.3kdMQa
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.3kdMQa
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.3kdMQa/journal -> /dev/disk/by-partuuid/96c80a52-9cfa-4624-a0ec-fbf2003c19fb
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.3kdMQa
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.3kdMQa
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f418c4dd710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f418c9477d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph8][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbc801d9128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fbc80637758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sda:/dev/sda
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sda journal /dev/sda activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:1b3d51d3-4448-40da-9edd-edbbee1d1970 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/1b3d51d3-4448-40da-9edd-edbbee1d1970
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/1b3d51d3-4448-40da-9edd-edbbee1d1970
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:2db7e70d-fd68-48bf-bdc5-7416e3cd7e92 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph8][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.NK4RLy with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.NK4RLy
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.NK4RLy
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.NK4RLy/journal -> /dev/disk/by-partuuid/1b3d51d3-4448-40da-9edd-edbbee1d1970
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.NK4RLy
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.NK4RLy
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph10:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7f71d9c710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f7f722067d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph10
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph10][DEBUG ] zeroing last few blocks of device
[ceph10][DEBUG ] find the location of an executable
[ceph10][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph10][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph10][WARNING] backup header from main header.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph10][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph10][WARNING] 
[ceph10][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph10][WARNING] 
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph10][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph10][DEBUG ] ****************************************************************************
[ceph10][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph10][DEBUG ] other utilities.
[ceph10][DEBUG ] Creating new GPT entries.
[ceph10][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph10][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph10:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph10', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f759a634128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f759aa92758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph10:/dev/sdb:/dev/sdb
[ceph10][DEBUG ] connected to host: ceph10 
[ceph10][DEBUG ] detect platform information from remote host
[ceph10][DEBUG ] detect machine type
[ceph10][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph10
[ceph10][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph10 disk /dev/sdb journal /dev/sdb activate True
[ceph10][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph10][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph10][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:5b0e8ea3-b24b-4f86-bdf6-3117b50f704b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5b0e8ea3-b24b-4f86-bdf6-3117b50f704b
[ceph10][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/5b0e8ea3-b24b-4f86-bdf6-3117b50f704b
[ceph10][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:61ef3170-d515-4d51-a600-6ff269ec77b8 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph10][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph10][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph10][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph10][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph10][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph10][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph10][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph10][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph10][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph10][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.N1GZSw with options noatime,inode64
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.N1GZSw
[ceph10][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.N1GZSw
[ceph10][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.N1GZSw/journal -> /dev/disk/by-partuuid/5b0e8ea3-b24b-4f86-bdf6-3117b50f704b
[ceph10][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.N1GZSw
[ceph10][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.N1GZSw
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph10][DEBUG ] The operation has completed successfully.
[ceph10][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph10][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph10][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph10][INFO  ] checking OSD status...
[ceph10][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph10 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f23680bd710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f23685277d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph15][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7e95d13128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7e96171758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sdd:/dev/sdd
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sdd journal /dev/sdd activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:7cc3d015-7458-4081-9407-67b2fcd3c784 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/7cc3d015-7458-4081-9407-67b2fcd3c784
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/7cc3d015-7458-4081-9407-67b2fcd3c784
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7da701cc-00db-465f-a941-96675533e461 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph15][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.TFIbdE with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.TFIbdE
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.TFIbdE
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.TFIbdE/journal -> /dev/disk/by-partuuid/7cc3d015-7458-4081-9407-67b2fcd3c784
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.TFIbdE
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.TFIbdE
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph17:sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda663aa710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fda668147d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph17
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph17][DEBUG ] zeroing last few blocks of device
[ceph17][DEBUG ] find the location of an executable
[ceph17][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdb
[ceph17][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph17][WARNING] backup header from main header.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph17][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph17][WARNING] 
[ceph17][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph17][WARNING] 
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph17][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph17][DEBUG ] ****************************************************************************
[ceph17][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph17][DEBUG ] other utilities.
[ceph17][DEBUG ] Creating new GPT entries.
[ceph17][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdb
[ceph17][INFO  ] Running command: partprobe /dev/sdb
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph17:sdb:/dev/sdb
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph17', '/dev/sdb', '/dev/sdb')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd7e4689128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd7e4ae7758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph17:/dev/sdb:/dev/sdb
[ceph17][DEBUG ] connected to host: ceph17 
[ceph17][DEBUG ] detect platform information from remote host
[ceph17][DEBUG ] detect machine type
[ceph17][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph17
[ceph17][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph17 disk /dev/sdb journal /dev/sdb activate True
[ceph17][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph17][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph17][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:245dafd7-34e1-4439-9ba3-7f50d8348e4f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/245dafd7-34e1-4439-9ba3-7f50d8348e4f
[ceph17][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/245dafd7-34e1-4439-9ba3-7f50d8348e4f
[ceph17][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e75614e1-1511-46b6-8569-67b634458429 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph17][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[ceph17][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=60719917 blks
[ceph17][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph17][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph17][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph17][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph17][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph17][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph17][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph17][WARNING] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.ruRsGf with options noatime,inode64
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.ruRsGf
[ceph17][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.ruRsGf
[ceph17][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.ruRsGf/journal -> /dev/disk/by-partuuid/245dafd7-34e1-4439-9ba3-7f50d8348e4f
[ceph17][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.ruRsGf
[ceph17][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.ruRsGf
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[ceph17][DEBUG ] The operation has completed successfully.
[ceph17][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdb
[ceph17][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdb
[ceph17][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph17][INFO  ] checking OSD status...
[ceph17][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph17 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f075e79c710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f075ec067d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdc on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdc
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdc
[ceph8][INFO  ] Running command: partprobe /dev/sdc
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdc:/dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdc', '/dev/sdc')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff1e16a7128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff1e1b05758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdc:/dev/sdc
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdc journal /dev/sdc activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:56bb9b2a-1a22-4696-aad8-6c6ce6a699d6 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/56bb9b2a-1a22-4696-aad8-6c6ce6a699d6
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/56bb9b2a-1a22-4696-aad8-6c6ce6a699d6
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:cf729db3-89b7-41fe-b047-1286664164c0 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdc1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[ceph8][DEBUG ] meta-data=/dev/sdc1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.zRvyee with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.zRvyee
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.zRvyee
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.zRvyee/journal -> /dev/disk/by-partuuid/56bb9b2a-1a22-4696-aad8-6c6ce6a699d6
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.zRvyee
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.zRvyee
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdc
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdc
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph5:sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff7a7e62710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7ff7a82cc7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sde on ceph5
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph5][DEBUG ] zeroing last few blocks of device
[ceph5][DEBUG ] find the location of an executable
[ceph5][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sde
[ceph5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph5][WARNING] backup header from main header.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph5][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph5][WARNING] 
[ceph5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph5][WARNING] 
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph5][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph5][DEBUG ] ****************************************************************************
[ceph5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph5][DEBUG ] other utilities.
[ceph5][DEBUG ] Creating new GPT entries.
[ceph5][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sde
[ceph5][INFO  ] Running command: partprobe /dev/sde
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph5:sde:/dev/sde
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph5', '/dev/sde', '/dev/sde')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fefbd73e128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fefbdb9c758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph5:/dev/sde:/dev/sde
[ceph5][DEBUG ] connected to host: ceph5 
[ceph5][DEBUG ] detect platform information from remote host
[ceph5][DEBUG ] detect machine type
[ceph5][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph5
[ceph5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph5 disk /dev/sde journal /dev/sde activate True
[ceph5][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sde /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph5][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph5][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:d3380d76-582a-4ecc-b8b6-8abbda47480f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d3380d76-582a-4ecc-b8b6-8abbda47480f
[ceph5][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/d3380d76-582a-4ecc-b8b6-8abbda47480f
[ceph5][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:57b5af08-6144-469f-85c9-8e3356697b0c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph5][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sde1
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sde1
[ceph5][DEBUG ] meta-data=/dev/sde1              isize=2048   agcount=4, agsize=60719917 blks
[ceph5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph5][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph5][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph5][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph5][WARNING] DEBUG:ceph-disk:Mounting /dev/sde1 on /var/lib/ceph/tmp/mnt.4YUwHE with options noatime,inode64
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sde1 /var/lib/ceph/tmp/mnt.4YUwHE
[ceph5][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.4YUwHE
[ceph5][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.4YUwHE/journal -> /dev/disk/by-partuuid/d3380d76-582a-4ecc-b8b6-8abbda47480f
[ceph5][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.4YUwHE
[ceph5][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.4YUwHE
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde
[ceph5][DEBUG ] The operation has completed successfully.
[ceph5][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sde
[ceph5][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sde
[ceph5][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph5][INFO  ] checking OSD status...
[ceph5][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph5 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph16:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb332caa710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fb3331147d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph16
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph16][DEBUG ] zeroing last few blocks of device
[ceph16][DEBUG ] find the location of an executable
[ceph16][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph16][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph16][WARNING] backup header from main header.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph16][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph16][WARNING] 
[ceph16][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph16][WARNING] 
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph16][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph16][DEBUG ] ****************************************************************************
[ceph16][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph16][DEBUG ] other utilities.
[ceph16][DEBUG ] Creating new GPT entries.
[ceph16][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph16][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph16:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph16', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa9b6cd1128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa9b712f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph16:/dev/sda:/dev/sda
[ceph16][DEBUG ] connected to host: ceph16 
[ceph16][DEBUG ] detect platform information from remote host
[ceph16][DEBUG ] detect machine type
[ceph16][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph16
[ceph16][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph16 disk /dev/sda journal /dev/sda activate True
[ceph16][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph16][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph16][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:73de623a-5618-4344-929e-fdabcad4bed6 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/73de623a-5618-4344-929e-fdabcad4bed6
[ceph16][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/73de623a-5618-4344-929e-fdabcad4bed6
[ceph16][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:255dbc86-985f-43ef-8d7d-a935fb50b90c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph16][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph16][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph16][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph16][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph16][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph16][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph16][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph16][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph16][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph16][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.gYRHqz with options noatime,inode64
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.gYRHqz
[ceph16][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.gYRHqz
[ceph16][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.gYRHqz/journal -> /dev/disk/by-partuuid/73de623a-5618-4344-929e-fdabcad4bed6
[ceph16][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.gYRHqz
[ceph16][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.gYRHqz
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph16][DEBUG ] The operation has completed successfully.
[ceph16][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph16][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph16][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph16][INFO  ] checking OSD status...
[ceph16][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph16 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph2:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda46279710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fda466e37d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph2
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph2][DEBUG ] zeroing last few blocks of device
[ceph2][DEBUG ] find the location of an executable
[ceph2][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph2][WARNING] backup header from main header.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph2][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph2][WARNING] 
[ceph2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph2][WARNING] 
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph2][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph2][DEBUG ] ****************************************************************************
[ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph2][DEBUG ] other utilities.
[ceph2][DEBUG ] Creating new GPT entries.
[ceph2][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph2][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph2:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph2', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f551c770128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f551cbce758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph2:/dev/sdd:/dev/sdd
[ceph2][DEBUG ] connected to host: ceph2 
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph2
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph2 disk /dev/sdd journal /dev/sdd activate True
[ceph2][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph2][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:cea72f37-257c-45c4-9fc9-1272c70394ef --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cea72f37-257c-45c4-9fc9-1272c70394ef
[ceph2][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/cea72f37-257c-45c4-9fc9-1272c70394ef
[ceph2][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0d80f9e3-56b7-411f-a3f0-4a8e65e6d71a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph2][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph2][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph2][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph2][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph2][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph2][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.66fmvT with options noatime,inode64
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.66fmvT
[ceph2][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.66fmvT
[ceph2][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.66fmvT/journal -> /dev/disk/by-partuuid/cea72f37-257c-45c4-9fc9-1272c70394ef
[ceph2][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.66fmvT
[ceph2][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.66fmvT
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph2][DEBUG ] The operation has completed successfully.
[ceph2][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph2][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph2][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph2][INFO  ] checking OSD status...
[ceph2][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph2 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph8:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcaf16b0710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcaf1b1a7d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph8
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph8][DEBUG ] zeroing last few blocks of device
[ceph8][DEBUG ] find the location of an executable
[ceph8][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph8][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph8][WARNING] backup header from main header.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph8][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph8][WARNING] 
[ceph8][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph8][WARNING] 
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph8][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph8][DEBUG ] ****************************************************************************
[ceph8][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph8][DEBUG ] other utilities.
[ceph8][DEBUG ] Creating new GPT entries.
[ceph8][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph8][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph8:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph8', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f84d8e01128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f84d925f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph8:/dev/sdd:/dev/sdd
[ceph8][DEBUG ] connected to host: ceph8 
[ceph8][DEBUG ] detect platform information from remote host
[ceph8][DEBUG ] detect machine type
[ceph8][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph8
[ceph8][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph8 disk /dev/sdd journal /dev/sdd activate True
[ceph8][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph8][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph8][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:34fb6bba-24f1-40a2-afb8-a669d76df17a --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/34fb6bba-24f1-40a2-afb8-a669d76df17a
[ceph8][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/34fb6bba-24f1-40a2-afb8-a669d76df17a
[ceph8][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:af7997cb-5679-46c8-b7ec-4c9954630286 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph8][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph8][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph8][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph8][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph8][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph8][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph8][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph8][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph8][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph8][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.dH29z6 with options noatime,inode64
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.dH29z6
[ceph8][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.dH29z6
[ceph8][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.dH29z6/journal -> /dev/disk/by-partuuid/34fb6bba-24f1-40a2-afb8-a669d76df17a
[ceph8][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.dH29z6
[ceph8][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.dH29z6
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph8][DEBUG ] The operation has completed successfully.
[ceph8][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph8][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph8][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph8][INFO  ] checking OSD status...
[ceph8][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph8 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph4:sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3aeb75a710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f3aebbc47d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sdd on ceph4
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph4][DEBUG ] zeroing last few blocks of device
[ceph4][DEBUG ] find the location of an executable
[ceph4][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sdd
[ceph4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph4][WARNING] backup header from main header.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph4][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph4][WARNING] 
[ceph4][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph4][WARNING] 
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph4][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph4][DEBUG ] ****************************************************************************
[ceph4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph4][DEBUG ] other utilities.
[ceph4][DEBUG ] Creating new GPT entries.
[ceph4][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sdd
[ceph4][INFO  ] Running command: partprobe /dev/sdd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph4:sdd:/dev/sdd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph4', '/dev/sdd', '/dev/sdd')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f367c8ef128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f367cd4d758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph4:/dev/sdd:/dev/sdd
[ceph4][DEBUG ] connected to host: ceph4 
[ceph4][DEBUG ] detect platform information from remote host
[ceph4][DEBUG ] detect machine type
[ceph4][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph4
[ceph4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph4 disk /dev/sdd journal /dev/sdd activate True
[ceph4][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph4][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph4][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:b071a033-eb78-4643-95ad-7e73d348a83d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b071a033-eb78-4643-95ad-7e73d348a83d
[ceph4][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/b071a033-eb78-4643-95ad-7e73d348a83d
[ceph4][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f85e7108-9aca-4749-b920-1ac35b776f09 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph4][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sdd1
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[ceph4][DEBUG ] meta-data=/dev/sdd1              isize=2048   agcount=4, agsize=60719917 blks
[ceph4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph4][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph4][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph4][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph4][WARNING] DEBUG:ceph-disk:Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.v59knk with options noatime,inode64
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.v59knk
[ceph4][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.v59knk
[ceph4][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.v59knk/journal -> /dev/disk/by-partuuid/b071a033-eb78-4643-95ad-7e73d348a83d
[ceph4][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.v59knk
[ceph4][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.v59knk
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[ceph4][DEBUG ] The operation has completed successfully.
[ceph4][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sdd
[ceph4][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
[ceph4][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph4][INFO  ] checking OSD status...
[ceph4][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph4 is now ready for osd use.
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff97005f710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7ff9704c97d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph15][DEBUG ] The new table will be used at the next reboot.
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph15][DEBUG ] The new table will be used at the next reboot.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] Error: Partition(s) 1, 2 on /dev/sda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph15][INFO  ] Running command: partprobe /dev/sda
[ceph15][WARNING] Error: Partition(s) 1, 2 on /dev/sda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: partprobe /dev/sda

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0b175c7128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0b17a25758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sda journal /dev/sda activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:6d773d86-e63c-4bfc-8820-917e9c07e460 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph15][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph15][DEBUG ] The new table will be used at the next reboot.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] Error: Partition(s) 1 on /dev/sda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6d773d86-e63c-4bfc-8820-917e9c07e460
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/6d773d86-e63c-4bfc-8820-917e9c07e460
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a9b2b0a1-b199-48d3-95d3-c3df3ff2e5c7 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph15][DEBUG ] Warning: The kernel is still using the old partition table.
[ceph15][DEBUG ] The new table will be used at the next reboot.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph15][WARNING] mkfs.xfs: /dev/sda1 contains a mounted filesystem
[ceph15][WARNING] Usage: mkfs.xfs
[ceph15][WARNING] /* blocksize */		[-b log=n|size=num]
[ceph15][WARNING] /* data subvol */	[-d agcount=n,agsize=n,file,name=xxx,size=num,
[ceph15][WARNING] 			    (sunit=value,swidth=value|su=num,sw=num),
[ceph15][WARNING] 			    sectlog=n|sectsize=num
[ceph15][WARNING] /* inode size */	[-i log=n|perblock=n|size=num,maxpct=n,attr=0|1|2,
[ceph15][WARNING] 			    projid32bit=0|1]
[ceph15][WARNING] /* log subvol */	[-l agnum=n,internal,size=num,logdev=xxx,version=n
[ceph15][WARNING] 			    sunit=value|su=num,sectlog=n|sectsize=num,
[ceph15][WARNING] 			    lazy-count=0|1]
[ceph15][WARNING] /* label */		[-L label (maximum 12 characters)]
[ceph15][WARNING] /* naming */		[-n log=n|size=num,version=2|ci]
[ceph15][WARNING] /* prototype file */	[-p fname]
[ceph15][WARNING] /* quiet */		[-q]
[ceph15][WARNING] /* realtime subvol */	[-r extsize=num,size=num,rtdev=xxx]
[ceph15][WARNING] /* sectorsize */	[-s log=n|size=num]
[ceph15][WARNING] /* version */		[-V]
[ceph15][WARNING] 			devicename
[ceph15][WARNING] <devicename> is required unless -d name=xxx is given.
[ceph15][WARNING] <num> is xxx (bytes), xxxs (sectors), xxxb (fs blocks), xxxk (xxx KiB),
[ceph15][WARNING]       xxxm (xxx MiB), xxxg (xxx GiB), xxxt (xxx TiB) or xxxp (xxx PiB).
[ceph15][WARNING] <value> is xxx (512 byte blocks).
[ceph15][WARNING] ceph-disk: Error: Command '['/sbin/mkfs', '-t', 'xfs', '-f', '-i', 'size=2048', '--', '/dev/sda1']' returned non-zero exit status 1
[ceph15][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf disk zap ceph15:sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : zap
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4cc84e6710>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4cc89507d0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', None)]
[ceph_deploy.osd][DEBUG ] zapping /dev/sda on ceph15
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph15][DEBUG ] zeroing last few blocks of device
[ceph15][DEBUG ] find the location of an executable
[ceph15][INFO  ] Running command: /usr/sbin/ceph-disk zap /dev/sda
[ceph15][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[ceph15][WARNING] backup header from main header.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[ceph15][WARNING] on the recovery & transformation menu to examine the two tables.
[ceph15][WARNING] 
[ceph15][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[ceph15][WARNING] 
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[ceph15][DEBUG ] verification and recovery are STRONGLY recommended.
[ceph15][DEBUG ] ****************************************************************************
[ceph15][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[ceph15][DEBUG ] other utilities.
[ceph15][DEBUG ] Creating new GPT entries.
[ceph15][DEBUG ] The operation has completed successfully.
[ceph_deploy.osd][DEBUG ] Calling partprobe on zapped device /dev/sda
[ceph15][INFO  ] Running command: partprobe /dev/sda
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephus/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.28): /usr/bin/ceph-deploy --overwrite-conf osd create ceph15:sda:/dev/sda
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('ceph15', '/dev/sda', '/dev/sda')]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc159709128>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc159b67758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph15:/dev/sda:/dev/sda
[ceph15][DEBUG ] connected to host: ceph15 
[ceph15][DEBUG ] detect platform information from remote host
[ceph15][DEBUG ] detect machine type
[ceph15][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph15
[ceph15][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host ceph15 disk /dev/sda journal /dev/sda activate True
[ceph15][INFO  ] Running command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sda /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[ceph15][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[ceph15][WARNING] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:31ce6f3d-a7e5-4e53-9a04-e65ba3244b0f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/31ce6f3d-a7e5-4e53-9a04-e65ba3244b0f
[ceph15][WARNING] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/31ce6f3d-a7e5-4e53-9a04-e65ba3244b0f
[ceph15][WARNING] DEBUG:ceph-disk:Creating osd partition on /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a3e4ccf3-a60a-4afa-bea8-390ca96cdde5 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on created device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/udevadm settle
[ceph15][WARNING] DEBUG:ceph-disk:Creating xfs fs on /dev/sda1
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1
[ceph15][DEBUG ] meta-data=/dev/sda1              isize=2048   agcount=4, agsize=60719917 blks
[ceph15][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=0
[ceph15][DEBUG ] data     =                       bsize=4096   blocks=242879665, imaxpct=25
[ceph15][DEBUG ]          =                       sunit=0      swidth=0 blks
[ceph15][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0
[ceph15][DEBUG ] log      =internal log           bsize=4096   blocks=118593, version=2
[ceph15][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[ceph15][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[ceph15][WARNING] DEBUG:ceph-disk:Mounting /dev/sda1 on /var/lib/ceph/tmp/mnt.nfhSr4 with options noatime,inode64
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sda1 /var/lib/ceph/tmp/mnt.nfhSr4
[ceph15][WARNING] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.nfhSr4
[ceph15][WARNING] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.nfhSr4/journal -> /dev/disk/by-partuuid/31ce6f3d-a7e5-4e53-9a04-e65ba3244b0f
[ceph15][WARNING] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.nfhSr4
[ceph15][WARNING] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.nfhSr4
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda
[ceph15][DEBUG ] The operation has completed successfully.
[ceph15][WARNING] DEBUG:ceph-disk:Calling partprobe on prepared device /dev/sda
[ceph15][WARNING] INFO:ceph-disk:Running command: /sbin/partprobe /dev/sda
[ceph15][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph15][INFO  ] checking OSD status...
[ceph15][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph15 is now ready for osd use.
